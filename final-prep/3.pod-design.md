# Labels & Find resource by labels
- kubectl run nginx --image=nginx --labels=key1=val1,key2=val2 --dry-run -o yaml `labels can parse comma, no need to --labels multiple time, that's why it's called --labels not --label`
- kubectl run anothernginx --image=nginx --labels=sex=male
- kubectl get pods --show-labels
- kubectl get pods --selector=key1=val1 --show-labels
- [VVI] when query use --selector=KEY=VALUE or -l=KEY=VALUE
    - kubectl get pods -l=sex=male --show-labels
    - kubectl get pods -l=key1=val1 --show-labels
    - kubectl get pods -l='key1 in (a,b,val1)' --show-labels
- [VVI] if the question asks for number of labels in an object, it's better to use the describe api, --show-labels flag will require manual counting.
- kubectl delete pod -l app=v1  `delete pod by labels`
- kubectl get pod -l app=v1  `get pod by labels`

# Labels CRUD
- Create  
    - kubectl run nginx --image=nginx --labels=key1=val1,key2=val2 --dry-run -o yaml `During object creation`
    - kubectl label pod nginx sex=female `After the object is created`
    - [VVI] **kubectl label api is used to create, update or delete label after a k8s object is created**
    - Add a new label tier=web to all pods having 'app=v2' or 'app=v1' labels
        - kubectl label pods -l='storage in(gp2,io3)' generation=old
        - kubectl label pods -l=generation generation-
    - kubectl label pod pod0{1..4} test=label `pod expression also works`
    - kubectl label pod pod0{1..4} test- 
    - kubectl label pod -l=slang=too-vulgar slang- `get pods that has label slang=too-vulgar and removes that label`
- Update 
    - kubectl label pod nginx sex=na (error: 'sex' already has a value (female), and --overwrite is false)
    - kubectl label pod nginx sex=na --overwrite
- Delete
    - kubectl label pod nginx sex-
    - kubectl label pod --selector=app app- `remove label from multiple pods`

- Read (not label but by label)
    - Setup lab [AWS Basic knowledge would be helpful]
        - kubectl run pod01 --image=nginx --labels=instance=m4large,storage=gp2 
        - kubectl run pod02 --image=nginx --labels=instance=m4large,storage=io3,az=ap-southeast-01
        - kubectl run pod03 --image=nginx --labels=instance=t3small,storage=gp2,az=ap-southeast-01
        - kubectl run pod04 --image=nginx --labels=instance=t3small,storage=io3 
    - Find all pods that has gp2 storage type
        - kubectl get pods -l=storage=gp2 --show-labels
    - Find all pods that has storage of type either gp2 or io3.
        - kubectl get pods -l='storage in(gp2,io3)' --show-labels
    - Find all pods that are az bound (has tag az)
        - kubectl get pods -l=az  --show-labels
    - [And query] Find all pods that are az bound *and* storage type io3 
        - kubectl get pods -l=az,storage=io3 --show-labels `verified, one row returned`
        - kubectl get pods -l=instance=t3medium,storage=gp2 --show-labels `empty result`
        - kubectl get pods -l=app=nginx,'slang in (vulgar)' --show-labels `and combining multiple operator`
    - [Or query]Find all pods that are az bound *or* storage type io3 
        - Currently, Kubernetes does not support OR in label selectors. You can only OR different values of the same label (like kubectl get pods -l 'app in (foo,bar)').
    - Get the label 'app' for the pods (show a column with APP labels)
        - [VVI] kubectl get po -L app
        - kubectl get pod -L app,slang `accepts multiple comma separated value`

# Annotation
- Create
    - kubectl annotate pod --selector='storage=gp2' owner=marketing
    - kubectl annotate pod nginx{1..3} description='my description'
- Read
    - kubectl annotate pod pod01 --list
    - kubectl annotate pod -l storage=gp2 --list [VVI - select pods and then list annotation]
- Update
    - kubectl annotate pod --selector='storage=gp2' owner=finance --overwrite
- Delete
    - kubectl annotate pod -l storage=gp2 owner-

# Deployment
- A Deployment provides declarative updates for Pods and ReplicaSets.
    - Recreate : Take down the whole app and recreate it (has downtime, not default), under the hood it scales down the existing replicaset to zero and creates a new replicaset with the desired capaticy.
    - Rolling Update: Default, Under the hood ,it scales down the existing replicaset in small chunks (e.g 2 at a time) and simultaneously scales up the new replicaset in small chunks. Let's say we have 6 pods running with app v1, we want to upgrade it to v2 and v2 image is not there in the docker repo, it can show more than 6 pods but the additional pods are not in ready state(CURRENT > DESIRED).
- Commands:
    - kubectl rollout status deploy DEPLOYMENT_NAME  
    - kubectl rollout history deploy DEPLOYMENT_NAME 
    - kubectl rollout undo deploy DEPLOYMENT_NAME --to-revision=REVISION_NUMBER `--to-revision is optional, bu default it rolls back to its previous version`
- [VVI] we can use kubectl set image deployment like we did for pod to update the image of a running deployment
- kubectl create deployment dp --image=nginx:1.22 --replicas=2
- kubectl get rs
- kubectl rollout status deploy dp
- kubectl rollout history deploy dp
- kubectl describe deploy dp | grep -i 'image'
- kubectl set image deploy dp nginx=nginx:1.23 [VVI]
- kubectl describe deploy dp | grep -i 'image'
- kubectl rollout history deploy dp
- kubectl rollout undo deploy dp --to-revision=1 `going back to 1.22 all the way back, not the last one 1.23`
- kubectl describe deploy dp | grep -i 'image'
    `Image:        nginx:1.22`
- Tracking change cause
    - kubectl create deployment dp --image=nginx:1.22 --replicas=2
    - kubectl annotate deployment dp kubernetes.io/change-cause="initial deployment"
    - kubectl rollout history deploy dp
- Manual scaling 
    - kubectl scale deployment dp --replicas=3
- Auto scaling
    - kubectl autoscale deployment dp --min=3 --max=4 --cpu-percent=80 `it should spin up one more pod : verified`
    - kubectl get hpa
- Pausing and Resuming a rollout of a Deployment
    - When you update a Deployment, or plan to, you can pause rollouts for that Deployment before you trigger one or more updates. When you're ready to apply those changes, you resume rollouts for the Deployment. This approach allows you to apply multiple fixes in between pausing and resuming without triggering unnecessary rollouts.
    - [Before]kubectl rollout pause deploy dp
    - kubectl set image deploy dp nginx=nginx:1.23
    - kubectl describe deploy dp | grep -i 'image' ` Image:        nginx:1.23`
    - kubectl describe pods dp-7b4dc4844f-frf7f | grep -i 'image' `Image:          nginx:1.22`
    - kubectl describe rs dp-7b4dc4844f | grep -i 'image' `Image:        nginx:1.22`
    - [BAM!!!]kubectl rollout resume deploy dp
    - [After]kubectl describe deploy dp | grep -i 'image' ` Image:        nginx:1.23`
    - kubectl describe pods dp-5bf47fb84b-4p9x9 | grep -i 'image' `Image: nginx:1.23`
    - kubectl describe rs dp-5bf47fb84b | grep -i 'image' `Image:        nginx:1.23` *this ine has desired=3,cirrent=3,ready=3, old one(dp-7b4dc4844f) has all zeros*
- [VVI] seting up resources on deployment
    - kubectl set resources deployment dp -c=nginx --limits=cpu=200m,memory=512Mi *triggers new replicaset based deployment*
    - [VVI] *kubectl set resources pod -h* - use this command to set resource to container inside a pod.
- `progressDeadlineSeconds` is the way to controll time limit of deployment.
- if there is a time related question, search the doc with `time,seconds` its hard to remember all the control parameters.
# Job
- `restartPolicy: Never` Job should not be restarted because the job controller automatically re-runs the pod if it fails, until it meets .spec.completions. Only a RestartPolicy equal to Never or OnFailure is allowed. If we provide `restartPolicy: Always` , the job API will reject the request.
- kubectl get pods --selector=job-name=pi `Get the pods related to job`
- delete Job `oldjob` but leave its pods running, using kubectl delete jobs/oldjob --cascade=orphan [VVI]
- Important control points for Job
    - backoffLimit
    - activeDeadlineSeconds : The job needs to be finished within this time otherwise it will be counted as a failed job.
# CronJob:
- schedule: cron expression to dictate when to kick off new job
- startingDeadlineSeconds: For example, if it is set to 200, it allows a job to be created for up to 200 seconds after the actual schedule.
- successfulJobsHistoryLimit: These fields specify how many completed jobs should be kept
- failedJobsHistoryLimit: These fields specify how many failed jobs should be kept

# Multicontainer pod
- They share the same network and volume
- They are reachable via localhost
- You can target a container by using -c flag
    - example `kubectl exec POD_NAME -c CONTAINER_NAME -it -- sh `
- kubectl top pod POD_NAME --containers