`Observability (18%)`
# Content 01
- Create an nginx pod with a liveness probe that just runs the command 'ls'. Save its YAML in pod.yaml. Run it, check its probe status, delete it.
    - `028.yml` kubectl apply -f 028.yml
    - kubectl describe pod nginx | grep -i liveness `Liveness:       exec [ls] delay=0s timeout=1s period=10s #success=1 #failure=3`

- Modify the pod.yaml file so that liveness probe starts kicking in after 5 seconds whereas the interval between probes would be 5 seconds. Run it, check the probe, delete it.
    - `029.yml` kubectl apply -f 029.yml `Liveness:       exec [ls] delay=5s timeout=1s period=5s #success=1 #failure=3`
- Create an nginx pod (that includes port 80) with an HTTP readinessProbe on path '/' on port 80. Again, run it, check the readinessProbe, delete it.
    - kubectl run nginx --image=nginx --port=80 --dry-run=client -o yaml > 030.yml
    - `030.yml` kubectl apply -f 030.yml
    - kubectl logs nginx -f `will show the ping log in the nginx container`
- Lots of pods are running in qa,alan,test,production namespaces. All of these pods are configured with liveness probe. Please list all pods whose liveness probe are failed in the format of <namespace>/<pod name> per line.
    - [VVI] kubectl get events -o json | jq -r '.items[] | select(.message | contains("failed liveness probe")).involvedObject | .namespace + "/" + .name' `more a shell command question, than kubernetes question`
    - kubectl get events -o json | grep 'failed liveness probe'
    - [VVI] Lesson : kubernetes rediness and liveness prob failures can be extracted from kubernetes events. True for any error, that's the last resort
        - To debug a pod
            - Describe command event log `kubectl describe POD_NAME | grep -i 'event'`
            - Container log `kubectl logs POD_NAME`
            - kubectl get events `kubectl get events`


- Create a busybox pod that runs i=0; while true; do echo "$i: $(date)"; i=$((i+1)); sleep 1; done. Check its logs
    - 

- Create a busybox pod that runs 'ls /notexist'. Determine if there's an error (of course there is), see it. In the end, delete the pod

- Create a busybox pod that runs 'notexist'. Determine if there's an error (of course there is), see it. In the end, delete the pod forcefully with a 0 grace period

- Get CPU/memory utilization for nodes (metrics-server must be running)
