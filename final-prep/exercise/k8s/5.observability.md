`Observability (18%)`
# Content 01
- Create an nginx pod with a liveness probe that just runs the command 'ls'. Save its YAML in pod.yaml. Run it, check its probe status, delete it.
    - `028.yml` kubectl apply -f 028.yml
    - kubectl describe pod nginx | grep -i liveness `Liveness:       exec [ls] delay=0s timeout=1s period=10s #success=1 #failure=3`

- Modify the pod.yaml file so that liveness probe starts kicking in after 5 seconds whereas the interval between probes would be 5 seconds. Run it, check the probe, delete it.
    - `029.yml` kubectl apply -f 029.yml `Liveness:       exec [ls] delay=5s timeout=1s period=5s #success=1 #failure=3`
- Create an nginx pod (that includes port 80) with an HTTP readinessProbe on path '/' on port 80. Again, run it, check the readinessProbe, delete it.
    - kubectl run nginx --image=nginx --port=80 --dry-run=client -o yaml > 030.yml
    - `030.yml` kubectl apply -f 030.yml
    - kubectl logs nginx -f `will show the ping log in the nginx container`
- Lots of pods are running in qa,alan,test,production namespaces. All of these pods are configured with liveness probe. Please list all pods whose liveness probe are failed in the format of <namespace>/<pod name> per line.
    - [VVI] kubectl get events -o json | jq -r '.items[] | select(.message | contains("failed liveness probe")).involvedObject | .namespace + "/" + .name' `more a shell command question, than kubernetes question`
    - kubectl get events -o json | grep 'failed liveness probe'
    - [VVI] Lesson : kubernetes rediness and liveness prob failures can be extracted from kubernetes events. True for any error, that's the last resort
        - To debug a pod
            - Describe command event log `kubectl describe POD_NAME | grep -i 'event'`
            - Container log `kubectl logs POD_NAME`
            - kubectl get events `kubectl get events`


- Create a busybox pod that runs i=0; while true; do echo "$i: $(date)"; i=$((i+1)); sleep 1; done. Check its logs
    - kubectl run bb --image=busybox --restart=Never --dry-run=client -o yaml -- sh -c 'i=0; while true; do echo "$i: $(date)"; i=$((i+1)); sleep 1; done' > 031.yml
    - kubectl apply -f 031.yml 
    - kubectl logs bb -f

- Create a busybox pod that runs 'ls /notexist'. Determine if there's an error (of course there is), see it. In the end, delete the pod
    - kubectl run bb --image=busybox --restart=Never --dry-run=client -o yaml -- sh -c 'ls /notexist' > 032.yml
    - kubectl apply -f 032.yml 
    - kubectl logs bb -f `CrashLoopBackOff`
- Create a busybox pod that runs 'notexist'. Determine if there's an error (of course there is), see it. In the end, delete the pod forcefully with a 0 grace period
    - kubectl run bb --image=busybox --restart=Never -- sh -c 'notexist' 
    - kubectl logs bb
    - kubectl delete pod bb --force --grace-period=0
- Get CPU/memory utilization for nodes (metrics-server must be running)
    -  kubectl top nodes

# Content 02
- kubectl apply -f 033.yml 
- Check the Pod's status. Do you see any issue?
    - `Running`
- Follow the logs of the running container and identify an issue.
    - `/bin/sh: can't create /root/tmp/curr-date.txt: nonexistent directory`
- Fix the issue by shelling into the container. After resolving the issue the current date should be written to a file. Render the output.
    - kubectl exec failing-pod -it -- sh
    - mkdir /root/tmp/

# Content 03
- Get the memory and CPU usage of all the pods and find out top 3 pods which have the highest usage 
    - [VVI] kubectl top pod --all-namespaces | sort --reverse --key 3 --numeric | head -3