# Volume
- Simplest one is node port
- hostPath: Suitable for learning, test and single node.
```
volumes:
  - name: test-volume
    hostPath:
      path: /data
      type: Directory
```
- emptyDir : Good fit for multicontainer pods!
```
  volumes:
  - name: cache-volume
    emptyDir: {}
```
```
volumes:
  - name: cache-volume
    emptyDir:
      sizeLimit: 500Mi  -----------------------------------------------------------------> limit emptyDir size
```

# Persistance Volume:
- The physcial storage created by system admins. Application developers will claim it. 
    - It can also declare hostpath
```
apiVersion: v1
kind: PersistentVolume
metadata:
  name: task-pv-volume
  labels:
    type: local
spec:
  storageClassName: manual
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: "/mnt/data"
```
- Access Modes
    - The access modes are:
        - ReadWriteOncethe volume can be mounted as read-write by a single node. ReadWriteOnce access mode still can allow multiple pods to access the volume when the pods are running on the same node.
        - ReadOnlyMany
        the volume can be mounted as read-only by many nodes.
        - ReadWriteMany
        the volume can be mounted as read-write by many nodes.
        - ReadWriteOncePod
        the volume can be mounted as read-write by a single Pod. Use ReadWriteOncePod access mode if you want to ensure that only one pod across whole cluster can read that PVC or write to it. 
# volume vs Persistent volume
- Volume decouples the storage from the Container. Its lifecycle is coupled to a pod. It enables safe container restarts and sharing data between containers in a pod.

- Persistent Volume decouples the storage from the Pod. Its lifecycle is independent. It enables safe pod restarts and sharing data between pods.

- Practical example, consided pod in aws az - 1a and "volume" created in 1a and attached to the pod. Things are fine. Now the pod crashes and needs to be rescheduled. It can get scheduled to az - 1b but the volume is in 1a and hence wont get attached. If we use a persistent volume instead, k8s will consider the az into pod scheduling and only schedule on 1a

- A volume exists in the context of a pod, that is, you can't create a volume on its own. A persistent volume on the other hand is a first class object with its own lifecycle

# Persitance volume claim:
- To claim a PV
- [VVI] The binding algorithm:
    - Sufficient capacity
    - AccessMode : ReadWriteOnce/ReadOnlyMany/ReadWriteMany/ReadWriteOncePod
    - Volume Mode : Filesystem/Block
    - Storage class: Just a string, but has a default value in a cluster if it's not set explictely.
    - Selector:
-  If a PVC is created only then PV allocates physical storage on disk. What happens when a PVC is deleted? 
    - Depends on the persitanceVolumeReclaimPolicy: 
        - Retain -- manual reclamation
        - Recycle -- basic scrub (rm -rf /thevolume/*)
        - Delete -- associated storage asset such as AWS EBS, GCE PD, Azure Disk, or OpenStack Cinder volume is deleted
- pvc is found but still the actual volume on disk is not created!
- verified: only when pod is created and the PVC is created by the pod, the actual directory was created.

# Storage Class:
- To avoid manually creating PV, we no longer need a PV
- Refer to StorageClass name from PVC using storageClassName
- A storage class always creates PV, but we don't have to manually create that PV.
- A PV can have a class, which is specified by setting the storageClassName attribute to the name of a StorageClass. A PV of a particular class can only be bound to PVCs requesting that class. A PV with no storageClassName has no class and can only be bound to PVCs that request no particular class.
- [VVI] basically, we have a storage class name, it can be used by storage class which means resources created by that storage class can be refered via that sotrage class. It can be refered by pv, a PV manually joining a storage class. It can be refered by PVC, a pvc requesting storage from that class.

# Stateful Set:
- To have a stable nodename, or they need to come up in a a particular order we can use stateful set.
- Change kind: Deployment to kind: StatefulSet and add a serviceName: `The name of a headless service`, this enables auto DNS creation for each pod in the form of `PODNAME.HEADLESS_SERVICE_NAME.NAMESPACE.SVC.cluser.local` . 
- If we create a headless service named `myhlsvc` and use it in a stateful set, each pod of that set will have a DNS entry in the form of PODNAME.myhlsvc.NAMESPACE.SVC.cluser.local. - [VVI] 
- Difference from the regular service is - `clusterIP: None`
- By default Ordered graceful deployment - node01 is ready, then node02 starts up. Scale down in reversed order. We can change the behavior using podManagementPolicy:Parallel![VVI] (default value of the field is orderedReady)
- Consider a master-slave topology: node01 is master that does both read-write, node02,node03,node04 are just readonly slaves. Traditional service will evenly distribute load evenly among all pods regardless of they are of they are of type READ or WRITE. However, since we are using a headless service with our Staeful, we have DNS entry for each pods, hence, from the client we can send write request to master DNS and read requests to node2.3.4...



# DaemonSet:
- Runs one instance in all nodes.

# Storage in stateful set:
- Persitance volume claim in stateful works like this 
    - We Have 1 SC, 1 SC creates 1 PV, 1 PVC claims that PV, that PVC is refered from the Stateful sets pod template. Hence, all pods uses the same PVC, the same PV and shares the same volume! `By default` 
    - How can we have distinct storage for each POD. (mysql master slave, each node has its own data). Each pod needs its own local storage.
        - Each POD needs it's own PVC, EACH PVC needs a PV, PVs can be created using the same Storage Class.
        - So, how do we auotomatically create PVC for each pod in a stateful set?  We can achieve that using a `volumeClaimTemplate` [VVI] , no more `volume` section in the stateful set but a `volumeClaimTemplate`.
  

  # New discovery:
  - Some non cloud simple ways to mount temporary volume in a pod
      - emptyDir
      - hostPath
      - local `node affinity is a must in this case`
    ```
    volumes:
    - name: host-data
      hostPath:
        path: /etc/hostPath
        type: Directory
    ```
    - hostPath volume types:
      - Directory – Mounts an existing directory on the host.
      - DirectoryOrCreate – Mounts a directory on the host, and creates it if it doesn’t exist.
      - File – Mounts an existing single file on the host.
      - FileOrCreate – Mounts a file on the host, and creates it if it
doesn’t exist.
    or 
    ```
    emptyDir : {} --------------> we can allocate memory limit also.
    ```
- An emptyDir volume is first created when a Pod is assigned to a node, and exists as long as that Pod is running on that node. As the name says, the emptyDir volume is initially empty. All containers in the Pod can read and write the same files in the emptyDir volume, though that volume can be mounted at the same or different paths in each container. When a Pod is removed from a node for any reason, the data in the emptyDir is deleted permanently.
- The emptyDir.medium field controls where emptyDir volumes are stored. By default emptyDir volumes are stored on whatever medium that backs the node such as disk, SSD, or network storage, depending on your environment. If you set the emptyDir.medium field to "Memory", Kubernetes mounts a tmpfs (RAM-backed filesystem) for you instead. While tmpfs is very fast, be aware that unlike disks, tmpfs is cleared on node reboot and any files you write count against your container's memory limit.
- A size limit can be specified for the default medium, which limits the capacity of the emptyDir volume. The storage is allocated from node ephemeral storage. If that is filled up from another source (for example, log files or image overlays), the emptyDir may run out of capacity before this limit.
```
emptyDir:
      medium: Memory
      sizeLimit: 500Mi
```
- subPathExpr: $(POD_NAME) - very powerfull, if we want to have pod specific hostpath.
- vvi, bind a pvc to a pv explicitely using `volumeName: foo-pv` key in `kind: PersistentVolumeClaim`. By specifying a PersistentVolume in a PersistentVolumeClaim, you declare a binding between that specific PV and PVC. If the PersistentVolume exists and has not reserved PersistentVolumeClaims through its claimRef field, then the PersistentVolume and PersistentVolumeClaim will be bound.The binding happens regardless of some volume matching criteria, including node affinity. The control plane still checks that storage class, access modes, and requested storage size are valid.

- This method does not guarantee any binding privileges to the PersistentVolume. If other PersistentVolumeClaims could use the PV that you specify, you first need to reserve that storage volume. Specify the relevant PersistentVolumeClaim in the claimRef field of the PV so that other PVCs can not bind to it.This is useful if you want to consume PersistentVolumes that have their claimPolicy set to Retain, including cases where you are reusing an existing PV.

```
claimRef:
    name: foo-pvc
    namespace: foo
```

  - subPathExpr: $(POD_NAME)

# PersistentVolumeClaim
- Selector 
Claims can specify a label selector to further filter the set of volumes. Only the volumes whose labels match the selector can be bound to the claim. The selector can consist of two fields:

matchLabels - the volume must have a label with this value
matchExpressions - a list of requirements made by specifying key, list of values, and operator that relates the key and values. Valid operators include In, NotIn, Exists, and DoesNotExist.
All of the requirements, from both matchLabels and matchExpressions, are ANDed together – they must all be satisfied in order to match.


# Little host path experiment
- Create a pod with the following yaml `hostpathvolexp.yml` , `k apply -f hostpathvolexp.yml`
- `k get pods -o wide` get the nodename. `minikube-m02`
- login to `minikube-m02` node `minikube ssh -n minikube-m02`
- ls /my/hostpath/exp/ab8c0 or cd /my/hostpath/exp/ab8c0
- k exec hostpathvol -it -- sh
- echo 'hello' > /test-pd/hello.txt
- ls -a /my/hostpath/exp/ab8c0
- Delete pod kept the data, restart pod also kept the data inside /my/hostpath/exp/ab8c0 path on the host.

# Summary on PV , PVC:
- use nodeaffinity with `local`
- PV-PVC static binding is two way using claimref from pv and storageClassName from pvc
- PV can also use selector to limit PV options.
- Delete,Retain,Recycle are three PV options when old pv-pvc binding is deleted and new pv-pvc binding request is received.