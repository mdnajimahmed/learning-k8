# Volume
- Simplest one is node port
- hostPath: Suitable for learning, test and single node.
```
volumes:
  - name: test-volume
    hostPath:
      path: /data
      type: Directory
```
- emptyDir : Good fit for multicontainer pods!
```
  volumes:
  - name: cache-volume
    emptyDir: {}
```

# Persistance Volume:
- The physcial storage created by system admins. Application developers will claim it. 
    - It can also declare hostpath
```
apiVersion: v1
kind: PersistentVolume
metadata:
  name: task-pv-volume
  labels:
    type: local
spec:
  storageClassName: manual
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: "/mnt/data"
```
- Access Modes
    - The access modes are:
        - ReadWriteOncethe volume can be mounted as read-write by a single node. ReadWriteOnce access mode still can allow multiple pods to access the volume when the pods are running on the same node.
        - ReadOnlyMany
        the volume can be mounted as read-only by many nodes.
        - ReadWriteMany
        the volume can be mounted as read-write by many nodes.
        - ReadWriteOncePod
        the volume can be mounted as read-write by a single Pod. Use ReadWriteOncePod access mode if you want to ensure that only one pod across whole cluster can read that PVC or write to it. 
# volume vs Persistent volume
- Volume decouples the storage from the Container. Its lifecycle is coupled to a pod. It enables safe container restarts and sharing data between containers in a pod.

- Persistent Volume decouples the storage from the Pod. Its lifecycle is independent. It enables safe pod restarts and sharing data between pods.

- Practical example, consided pod in aws az - 1a and "volume" created in 1a and attached to the pod. Things are fine. Now the pod crashes and needs to be rescheduled. It can get scheduled to az - 1b but the volume is in 1a and hence wont get attached. If we use a persistent volume instead, k8s will consider the az into pod scheduling and only schedule on 1a

- A volume exists in the context of a pod, that is, you can't create a volume on its own. A persistent volume on the other hand is a first class object with its own lifecycle

# Persitance volume claim:
- To claim a PV
- [VVI] The binding algorithm:
    - Sufficient capacity
    - AccessMode : ReadWriteOnce/ReadOnlyMany/ReadWriteMany/ReadWriteOncePod
    - Volume Mode : Filesystem/Block
    - Storage class: Just a string, but has a default value in a cluster if it's not set explictely.
    - Selector:
-  If a PVC is created only then PV allocates physical storage on disk. What happens when a PVC is deleted? 
    - Depends on the persitanceVolumeReclaimPolicy: 
        - Retain -- manual reclamation
        - Recycle -- basic scrub (rm -rf /thevolume/*)
        - Delete -- associated storage asset such as AWS EBS, GCE PD, Azure Disk, or OpenStack Cinder volume is deleted

# Storage Class:
- To avoid manually creating PV, we no longer need a PV
- Refer to StorageClass name from PVC using storageClassName
- A storage class always creates PV, but we don't have to manually create that PV.
- A PV can have a class, which is specified by setting the storageClassName attribute to the name of a StorageClass. A PV of a particular class can only be bound to PVCs requesting that class. A PV with no storageClassName has no class and can only be bound to PVCs that request no particular class.
- [VVI] basically, we have a storage class name, it can be used by storage class which means resources created by that storage class can be refered via that sotrage class. It can be refered by pv, a PV manually joining a storage class. It can be refered by PVC, a pvc requesting storage from that class.

# Stateful Set:
- To have a stable nodename, or they need to come up in a a particular order we can use stateful set.
- Change kind: Deployment to kind: StatefulSet and add a serviceName: `The name of a headless service`, this enables auto DNS creation for each pod in the form of `PODNAME.HEADLESS_SERVICE_NAME.NAMESPACE.SVC.cluser.local` . 
- If we create a headless service named `myhlsvc` and use it in a stateful set, each pod of that set will have a DNS entry in the form of PODNAME.myhlsvc.NAMESPACE.SVC.cluser.local. - [VVI] 
- Difference from the regular service is - `clusterIP: None`
- By default Ordered graceful deployment - node01 is ready, then node02 starts up. Scale down in reversed order. We can change the behavior using podManagementPolicy:Parallel![VVI] (default value of the field is orderedReady)
- Consider a master-slave topology: node01 is master that does both read-write, node02,node03,node04 are just readonly slaves. Traditional service will evenly distribute load evenly among all pods regardless of they are of they are of type READ or WRITE. However, since we are using a headless service with our Staeful, we have DNS entry for each pods, hence, from the client we can send write request to master DNS and read requests to node2.3.4...



# DaemonSet:
- Runs one instance in all nodes.

# Storage in stateful set:
- Persitance volume claim in stateful works like this 
    - We Have 1 SC, 1 SC creates 1 PV, 1 PVC claims that PV, that PVC is refered from the Stateful sets pod template. Hence, all pods uses the same PVC, the same PV and shares the same volume! `By default` 
    - How can we have distinct storage for each POD. (mysql master slave, each node has its own data). Each pod needs its own local storage.
        - Each POD needs it's own PVC, EACH PVC needs a PV, PVs can be created using the same Storage Class.
        - So, how do we auotomatically create PVC for each pod in a stateful set?  We can achieve that using a `volumeClaimTemplate` [VVI] , no more `volume` section in the stateful set but a `volumeClaimTemplate`.