* 
* ==> Audit <==
* |-----------|--------------------------------|----------|-------|---------|---------------------|---------------------|
|  Command  |              Args              | Profile  | User  | Version |     Start Time      |      End Time       |
|-----------|--------------------------------|----------|-------|---------|---------------------|---------------------|
| ssh       |                                | minikube | apple | v1.26.1 | 01 Sep 22 16:17 +08 |                     |
| ssh       |                                | minikube | apple | v1.26.1 | 01 Sep 22 17:24 +08 | 01 Sep 22 17:43 +08 |
| service   | postgres-readiness-test --url  | minikube | apple | v1.26.1 | 02 Sep 22 21:12 +08 | 02 Sep 22 21:13 +08 |
| ssh       |                                | minikube | apple | v1.26.1 | 03 Sep 22 10:34 +08 | 03 Sep 22 10:39 +08 |
| ssh       |                                | minikube | apple | v1.26.1 | 03 Sep 22 12:42 +08 |                     |
| ssh       |                                | minikube | apple | v1.26.1 | 03 Sep 22 12:44 +08 | 03 Sep 22 12:46 +08 |
| ssh       |                                | minikube | apple | v1.26.1 | 03 Sep 22 14:00 +08 | 03 Sep 22 14:00 +08 |
| ssh       |                                | minikube | apple | v1.26.1 | 03 Sep 22 16:57 +08 | 03 Sep 22 17:57 +08 |
| delete    |                                | minikube | apple | v1.26.1 | 04 Sep 22 16:43 +08 | 04 Sep 22 16:44 +08 |
| start     | --vm-driver=virtualbox         | minikube | apple | v1.26.1 | 04 Sep 22 16:45 +08 |                     |
|           | --network-plugin=cni           |          |       |         |                     |                     |
|           | --enable-default-cni           |          |       |         |                     |                     |
| start     | --network-plugin=cni           | minikube | apple | v1.26.1 | 04 Sep 22 16:46 +08 |                     |
|           | --enable-default-cni --nodes 2 |          |       |         |                     |                     |
| start     | --network-plugin=cni           | minikube | apple | v1.26.1 | 04 Sep 22 16:53 +08 |                     |
|           | --enable-default-cni           |          |       |         |                     |                     |
| delete    |                                | minikube | apple | v1.26.1 | 04 Sep 22 16:55 +08 | 04 Sep 22 16:55 +08 |
| start     | --network-plugin=cni           | minikube | apple | v1.26.1 | 04 Sep 22 16:55 +08 | 04 Sep 22 16:56 +08 |
|           | --enable-default-cni           |          |       |         |                     |                     |
| stop      |                                | minikube | apple | v1.26.1 | 04 Sep 22 18:11 +08 | 04 Sep 22 18:11 +08 |
| start     |                                | minikube | apple | v1.26.1 | 04 Sep 22 18:12 +08 | 04 Sep 22 18:12 +08 |
| dashboard | --url                          | minikube | apple | v1.26.1 | 04 Sep 22 21:15 +08 |                     |
| ssh       |                                | minikube | apple | v1.26.1 | 05 Sep 22 09:14 +08 |                     |
| ssh       |                                | minikube | apple | v1.26.1 | 05 Sep 22 21:51 +08 | 05 Sep 22 21:53 +08 |
| ssh       |                                | minikube | apple | v1.26.1 | 05 Sep 22 21:55 +08 |                     |
| delete    |                                | minikube | apple | v1.26.1 | 07 Sep 22 19:18 +08 | 07 Sep 22 19:19 +08 |
| start     | --network-plugin=cni           | minikube | apple | v1.26.1 | 07 Sep 22 19:19 +08 | 07 Sep 22 19:20 +08 |
|           | --enable-default-cni           |          |       |         |                     |                     |
| delete    |                                | minikube | apple | v1.26.1 | 03 Oct 22 09:38 +08 | 03 Oct 22 09:38 +08 |
| start     |                                | minikube | apple | v1.26.1 | 03 Oct 22 09:39 +08 | 03 Oct 22 09:39 +08 |
| ssh       |                                | minikube | apple | v1.26.1 | 03 Oct 22 09:42 +08 |                     |
| ssh       |                                | minikube | apple | v1.26.1 | 03 Oct 22 10:52 +08 | 03 Oct 22 10:52 +08 |
| start     |                                | minikube | apple | v1.26.1 | 08 Oct 22 08:41 +08 | 08 Oct 22 08:41 +08 |
| delete    |                                | minikube | apple | v1.26.1 | 08 Oct 22 11:32 +08 | 08 Oct 22 11:32 +08 |
| start     |                                | minikube | apple | v1.26.1 | 08 Oct 22 11:36 +08 | 08 Oct 22 11:37 +08 |
| start     |                                | minikube | apple | v1.26.1 | 08 Oct 22 15:43 +08 | 08 Oct 22 15:46 +08 |
| addons    | enable metrics-server          | minikube | apple | v1.26.1 | 08 Oct 22 18:59 +08 | 08 Oct 22 18:59 +08 |
| addons    | enable metrics-server          | minikube | apple | v1.26.1 | 08 Oct 22 19:00 +08 | 08 Oct 22 19:00 +08 |
| delete    |                                | minikube | apple | v1.26.1 | 09 Oct 22 13:37 +08 | 09 Oct 22 13:37 +08 |
| start     | --nodes 2                      | minikube | apple | v1.26.1 | 09 Oct 22 13:37 +08 | 09 Oct 22 13:38 +08 |
| delete    |                                | minikube | apple | v1.26.1 | 10 Oct 22 09:33 +08 | 10 Oct 22 09:33 +08 |
| start     |                                | minikube | apple | v1.26.1 | 10 Oct 22 09:34 +08 | 10 Oct 22 09:34 +08 |
| addons    | enable metrics-server          | minikube | apple | v1.26.1 | 16 Oct 22 11:55 +08 | 16 Oct 22 11:56 +08 |
| addons    | list                           | minikube | apple | v1.26.1 | 16 Oct 22 11:56 +08 | 16 Oct 22 11:56 +08 |
| delete    |                                | minikube | apple | v1.26.1 | 22 Oct 22 15:20 +08 | 22 Oct 22 15:21 +08 |
| start     | --network-plugin=cni           | minikube | apple | v1.26.1 | 22 Oct 22 15:23 +08 |                     |
|           | --enable-default-cni --nodes 2 |          |       |         |                     |                     |
| delete    |                                | minikube | apple | v1.26.1 | 22 Oct 22 15:37 +08 | 22 Oct 22 15:39 +08 |
| start     | --nodes 2                      | minikube | apple | v1.26.1 | 22 Oct 22 15:43 +08 | 22 Oct 22 15:44 +08 |
| addons    | enable metrics-server          | minikube | apple | v1.26.1 | 24 Oct 22 10:14 +08 | 24 Oct 22 10:14 +08 |
| delete    |                                | minikube | apple | v1.26.1 | 26 Oct 22 07:58 +08 | 26 Oct 22 07:58 +08 |
| start     | --nodes 2 --network-plugin=cni | minikube | apple | v1.26.1 | 26 Oct 22 08:00 +08 | 26 Oct 22 08:04 +08 |
|           | --cni calico                   |          |       |         |                     |                     |
| ssh       |                                | minikube | apple | v1.26.1 | 26 Oct 22 16:45 +08 |                     |
| delete    |                                | minikube | apple | v1.26.1 | 28 Oct 22 13:51 +08 | 28 Oct 22 13:52 +08 |
| delete    | --all                          | minikube | apple | v1.26.1 | 28 Oct 22 13:52 +08 | 28 Oct 22 13:56 +08 |
| start     | --nodes 2 --network-plugin=cni | minikube | apple | v1.26.1 | 28 Oct 22 14:25 +08 | 28 Oct 22 14:27 +08 |
|           | --cni calico                   |          |       |         |                     |                     |
| start     |                                | minikube | apple | v1.26.1 | 29 Oct 22 07:44 +08 | 29 Oct 22 07:45 +08 |
| ssh       |                                | minikube | apple | v1.26.1 | 03 Nov 22 09:22 +08 | 03 Nov 22 09:22 +08 |
| ssh       |                                | minikube | apple | v1.26.1 | 03 Nov 22 09:27 +08 |                     |
| ssh       |                                | minikube | apple | v1.26.1 | 03 Nov 22 09:32 +08 |                     |
| ssh       |                                | minikube | apple | v1.26.1 | 03 Nov 22 09:36 +08 |                     |
| ssh       |                                | minikube | apple | v1.26.1 | 03 Nov 22 09:38 +08 |                     |
| ssh       |                                | minikube | apple | v1.26.1 | 03 Nov 22 09:54 +08 | 03 Nov 22 09:54 +08 |
| ssh       | minikube-m02                   | minikube | apple | v1.26.1 | 03 Nov 22 09:55 +08 |                     |
| ssh       | minikube-m02                   | minikube | apple | v1.26.1 | 03 Nov 22 09:56 +08 |                     |
| ip        |                                | minikube | apple | v1.26.1 | 03 Nov 22 10:20 +08 | 03 Nov 22 10:20 +08 |
| addons    | enable ingress                 | minikube | apple | v1.26.1 | 03 Nov 22 10:21 +08 |                     |
|-----------|--------------------------------|----------|-------|---------|---------------------|---------------------|

* 
* ==> Last Start <==
* Log file created at: 2022/10/29 07:44:35
Running on machine: Apples-MacBook-Pro
Binary: Built with gc go1.18.5 for darwin/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I1029 07:44:35.562959    2106 out.go:296] Setting OutFile to fd 1 ...
I1029 07:44:35.564039    2106 out.go:348] isatty.IsTerminal(1) = true
I1029 07:44:35.564052    2106 out.go:309] Setting ErrFile to fd 2...
I1029 07:44:35.564059    2106 out.go:348] isatty.IsTerminal(2) = true
I1029 07:44:35.564654    2106 root.go:333] Updating PATH: /Users/apple/.minikube/bin
I1029 07:44:35.570638    2106 out.go:303] Setting JSON to false
I1029 07:44:35.591983    2106 start.go:115] hostinfo: {"hostname":"Apples-MacBook-Pro.local","uptime":917,"bootTime":1666999758,"procs":420,"os":"darwin","platform":"darwin","platformFamily":"Standalone Workstation","platformVersion":"12.5.1","kernelVersion":"21.6.0","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"c7540c61-0420-58d7-8e90-2d60fa771794"}
W1029 07:44:35.592100    2106 start.go:123] gopshost.Virtualization returned error: not implemented yet
I1029 07:44:35.620532    2106 out.go:177] üòÑ  minikube v1.26.1 on Darwin 12.5.1
I1029 07:44:35.664318    2106 config.go:180] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.24.3
I1029 07:44:35.664358    2106 notify.go:193] Checking for updates...
I1029 07:44:35.665596    2106 driver.go:365] Setting default libvirt URI to qemu:///system
I1029 07:44:36.149170    2106 docker.go:137] docker version: linux-20.10.17
I1029 07:44:36.150477    2106 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1029 07:44:37.628962    2106 cli_runner.go:217] Completed: docker system info --format "{{json .}}": (1.478402968s)
I1029 07:44:37.630391    2106 info.go:265] docker info: {ID:CH7H:CBQI:3ORU:PLRN:PNIL:W3VA:Q5PZ:YEZL:KMD5:6OXZ:XAUR:PK6Y Containers:26 ContainersRunning:4 ContainersPaused:0 ContainersStopped:22 Images:50 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:63 OomKillDisable:false NGoroutines:61 SystemTime:2022-10-28 23:44:36.238306523 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:4 KernelVersion:5.10.104-linuxkit OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:8242126848 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:20.10.17 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:10c12954828e7c7c9b6e0ea9b0c02b01407d3ae1 Expected:10c12954828e7c7c9b6e0ea9b0c02b01407d3ae1} RuncCommit:{ID:v1.1.2-0-ga916309 Expected:v1.1.2-0-ga916309} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=default name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/local/lib/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.8.2] map[Name:compose Path:/usr/local/lib/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.7.0] map[Name:extension Path:/usr/local/lib/docker/cli-plugins/docker-extension SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.8] map[Name:sbom Path:/usr/local/lib/docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scan Path:/usr/local/lib/docker/cli-plugins/docker-scan SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.17.0]] Warnings:<nil>}}
I1029 07:44:37.671735    2106 out.go:177] ‚ú®  Using the docker driver based on existing profile
I1029 07:44:37.693117    2106 start.go:284] selected driver: docker
I1029 07:44:37.693192    2106 start.go:808] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.33@sha256:73b259e144d926189cf169ae5b46bbec4e08e4e2f2bd87296054c3244f70feb8 Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.24.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:calico NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.24.3 ContainerRuntime:docker ControlPlane:true Worker:true} {Name:m02 IP:192.168.49.3 Port:0 KubernetesVersion:v1.24.3 ContainerRuntime:docker ControlPlane:false Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:true ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath:}
I1029 07:44:37.693846    2106 start.go:819] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I1029 07:44:37.694428    2106 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1029 07:44:37.884688    2106 info.go:265] docker info: {ID:CH7H:CBQI:3ORU:PLRN:PNIL:W3VA:Q5PZ:YEZL:KMD5:6OXZ:XAUR:PK6Y Containers:26 ContainersRunning:4 ContainersPaused:0 ContainersStopped:22 Images:50 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:63 OomKillDisable:false NGoroutines:61 SystemTime:2022-10-28 23:44:37.792018942 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:4 KernelVersion:5.10.104-linuxkit OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:8242126848 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:20.10.17 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:10c12954828e7c7c9b6e0ea9b0c02b01407d3ae1 Expected:10c12954828e7c7c9b6e0ea9b0c02b01407d3ae1} RuncCommit:{ID:v1.1.2-0-ga916309 Expected:v1.1.2-0-ga916309} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=default name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/local/lib/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.8.2] map[Name:compose Path:/usr/local/lib/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.7.0] map[Name:extension Path:/usr/local/lib/docker/cli-plugins/docker-extension SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.8] map[Name:sbom Path:/usr/local/lib/docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scan Path:/usr/local/lib/docker/cli-plugins/docker-scan SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.17.0]] Warnings:<nil>}}
I1029 07:44:37.885526    2106 cni.go:95] Creating CNI manager for "calico"
I1029 07:44:37.886061    2106 start_flags.go:310] config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.33@sha256:73b259e144d926189cf169ae5b46bbec4e08e4e2f2bd87296054c3244f70feb8 Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.24.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:calico NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.24.3 ContainerRuntime:docker ControlPlane:true Worker:true} {Name:m02 IP:192.168.49.3 Port:0 KubernetesVersion:v1.24.3 ContainerRuntime:docker ControlPlane:false Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:true ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath:}
I1029 07:44:37.927724    2106 out.go:177] üëç  Starting control plane node minikube in cluster minikube
I1029 07:44:37.949628    2106 cache.go:120] Beginning downloading kic base image for docker with docker
I1029 07:44:37.974618    2106 out.go:177] üöú  Pulling base image ...
I1029 07:44:38.015293    2106 image.go:75] Checking for gcr.io/k8s-minikube/kicbase:v0.0.33@sha256:73b259e144d926189cf169ae5b46bbec4e08e4e2f2bd87296054c3244f70feb8 in local docker daemon
I1029 07:44:38.015299    2106 preload.go:132] Checking if preload exists for k8s version v1.24.3 and runtime docker
I1029 07:44:38.015440    2106 preload.go:148] Found local preload: /Users/apple/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.24.3-docker-overlay2-amd64.tar.lz4
I1029 07:44:38.015454    2106 cache.go:57] Caching tarball of preloaded images
I1029 07:44:38.017406    2106 preload.go:174] Found /Users/apple/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.24.3-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I1029 07:44:38.017460    2106 cache.go:60] Finished verifying existence of preloaded tar for  v1.24.3 on docker
I1029 07:44:38.018885    2106 profile.go:148] Saving config to /Users/apple/.minikube/profiles/minikube/config.json ...
I1029 07:44:38.111463    2106 image.go:79] Found gcr.io/k8s-minikube/kicbase:v0.0.33@sha256:73b259e144d926189cf169ae5b46bbec4e08e4e2f2bd87296054c3244f70feb8 in local docker daemon, skipping pull
I1029 07:44:38.111932    2106 cache.go:142] gcr.io/k8s-minikube/kicbase:v0.0.33@sha256:73b259e144d926189cf169ae5b46bbec4e08e4e2f2bd87296054c3244f70feb8 exists in daemon, skipping load
I1029 07:44:38.111949    2106 cache.go:208] Successfully downloaded all kic artifacts
I1029 07:44:38.112917    2106 start.go:371] acquiring machines lock for minikube: {Name:mk78a1be280d38a6eb69d283b8fcbb383a974461 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I1029 07:44:38.113118    2106 start.go:375] acquired machines lock for "minikube" in 171.882¬µs
I1029 07:44:38.113539    2106 start.go:95] Skipping create...Using existing machine configuration
I1029 07:44:38.113564    2106 fix.go:55] fixHost starting: 
I1029 07:44:38.113803    2106 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1029 07:44:38.196427    2106 fix.go:103] recreateIfNeeded on minikube: state=Stopped err=<nil>
W1029 07:44:38.196456    2106 fix.go:129] unexpected machine state, will restart: <nil>
I1029 07:44:38.216903    2106 out.go:177] üîÑ  Restarting existing docker container for "minikube" ...
I1029 07:44:38.255636    2106 cli_runner.go:164] Run: docker start minikube
I1029 07:44:38.773904    2106 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1029 07:44:38.885030    2106 kic.go:415] container "minikube" state is running.
I1029 07:44:38.886203    2106 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1029 07:44:39.007451    2106 profile.go:148] Saving config to /Users/apple/.minikube/profiles/minikube/config.json ...
I1029 07:44:39.008017    2106 machine.go:88] provisioning docker machine ...
I1029 07:44:39.008357    2106 ubuntu.go:169] provisioning hostname "minikube"
I1029 07:44:39.008442    2106 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1029 07:44:39.103518    2106 main.go:134] libmachine: Using SSH client type: native
I1029 07:44:39.104165    2106 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x43d9d20] 0x43dcd80 <nil>  [] 0s} 127.0.0.1 49332 <nil> <nil>}
I1029 07:44:39.104178    2106 main.go:134] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I1029 07:44:39.106304    2106 main.go:134] libmachine: Error dialing TCP: ssh: handshake failed: EOF
I1029 07:44:42.310206    2106 main.go:134] libmachine: SSH cmd err, output: <nil>: minikube

I1029 07:44:42.310742    2106 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1029 07:44:42.398521    2106 main.go:134] libmachine: Using SSH client type: native
I1029 07:44:42.398771    2106 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x43d9d20] 0x43dcd80 <nil>  [] 0s} 127.0.0.1 49332 <nil> <nil>}
I1029 07:44:42.398782    2106 main.go:134] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I1029 07:44:42.531703    2106 main.go:134] libmachine: SSH cmd err, output: <nil>: 
I1029 07:44:42.532632    2106 ubuntu.go:175] set auth options {CertDir:/Users/apple/.minikube CaCertPath:/Users/apple/.minikube/certs/ca.pem CaPrivateKeyPath:/Users/apple/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/Users/apple/.minikube/machines/server.pem ServerKeyPath:/Users/apple/.minikube/machines/server-key.pem ClientKeyPath:/Users/apple/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/Users/apple/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/Users/apple/.minikube}
I1029 07:44:42.532693    2106 ubuntu.go:177] setting up certificates
I1029 07:44:42.533616    2106 provision.go:83] configureAuth start
I1029 07:44:42.533754    2106 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1029 07:44:42.621975    2106 provision.go:138] copyHostCerts
I1029 07:44:42.623849    2106 exec_runner.go:144] found /Users/apple/.minikube/ca.pem, removing ...
I1029 07:44:42.624293    2106 exec_runner.go:207] rm: /Users/apple/.minikube/ca.pem
I1029 07:44:42.625136    2106 exec_runner.go:151] cp: /Users/apple/.minikube/certs/ca.pem --> /Users/apple/.minikube/ca.pem (1074 bytes)
I1029 07:44:42.625878    2106 exec_runner.go:144] found /Users/apple/.minikube/cert.pem, removing ...
I1029 07:44:42.625882    2106 exec_runner.go:207] rm: /Users/apple/.minikube/cert.pem
I1029 07:44:42.625948    2106 exec_runner.go:151] cp: /Users/apple/.minikube/certs/cert.pem --> /Users/apple/.minikube/cert.pem (1119 bytes)
I1029 07:44:42.626604    2106 exec_runner.go:144] found /Users/apple/.minikube/key.pem, removing ...
I1029 07:44:42.626609    2106 exec_runner.go:207] rm: /Users/apple/.minikube/key.pem
I1029 07:44:42.626696    2106 exec_runner.go:151] cp: /Users/apple/.minikube/certs/key.pem --> /Users/apple/.minikube/key.pem (1679 bytes)
I1029 07:44:42.627086    2106 provision.go:112] generating server cert: /Users/apple/.minikube/machines/server.pem ca-key=/Users/apple/.minikube/certs/ca.pem private-key=/Users/apple/.minikube/certs/ca-key.pem org=apple.minikube san=[192.168.49.2 127.0.0.1 localhost 127.0.0.1 minikube minikube]
I1029 07:44:42.761673    2106 provision.go:172] copyRemoteCerts
I1029 07:44:42.762354    2106 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I1029 07:44:42.762408    2106 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1029 07:44:42.846444    2106 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49332 SSHKeyPath:/Users/apple/.minikube/machines/minikube/id_rsa Username:docker}
I1029 07:44:42.948163    2106 ssh_runner.go:362] scp /Users/apple/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1074 bytes)
I1029 07:44:42.980615    2106 ssh_runner.go:362] scp /Users/apple/.minikube/machines/server.pem --> /etc/docker/server.pem (1196 bytes)
I1029 07:44:43.000315    2106 ssh_runner.go:362] scp /Users/apple/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I1029 07:44:43.021359    2106 provision.go:86] duration metric: configureAuth took 487.312632ms
I1029 07:44:43.021381    2106 ubuntu.go:193] setting minikube options for container-runtime
I1029 07:44:43.021555    2106 config.go:180] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.24.3
I1029 07:44:43.021601    2106 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1029 07:44:43.103502    2106 main.go:134] libmachine: Using SSH client type: native
I1029 07:44:43.103661    2106 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x43d9d20] 0x43dcd80 <nil>  [] 0s} 127.0.0.1 49332 <nil> <nil>}
I1029 07:44:43.103668    2106 main.go:134] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I1029 07:44:43.239677    2106 main.go:134] libmachine: SSH cmd err, output: <nil>: overlay

I1029 07:44:43.239696    2106 ubuntu.go:71] root file system type: overlay
I1029 07:44:43.240382    2106 provision.go:309] Updating docker unit: /lib/systemd/system/docker.service ...
I1029 07:44:43.240482    2106 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1029 07:44:43.332042    2106 main.go:134] libmachine: Using SSH client type: native
I1029 07:44:43.332362    2106 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x43d9d20] 0x43dcd80 <nil>  [] 0s} 127.0.0.1 49332 <nil> <nil>}
I1029 07:44:43.332426    2106 main.go:134] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I1029 07:44:43.491931    2106 main.go:134] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I1029 07:44:43.492890    2106 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1029 07:44:43.576889    2106 main.go:134] libmachine: Using SSH client type: native
I1029 07:44:43.577084    2106 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x43d9d20] 0x43dcd80 <nil>  [] 0s} 127.0.0.1 49332 <nil> <nil>}
I1029 07:44:43.577096    2106 main.go:134] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I1029 07:44:43.718585    2106 main.go:134] libmachine: SSH cmd err, output: <nil>: 
I1029 07:44:43.718605    2106 machine.go:91] provisioned docker machine in 4.710591581s
I1029 07:44:43.718633    2106 start.go:307] post-start starting for "minikube" (driver="docker")
I1029 07:44:43.718640    2106 start.go:335] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I1029 07:44:43.718760    2106 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I1029 07:44:43.718817    2106 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1029 07:44:43.806461    2106 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49332 SSHKeyPath:/Users/apple/.minikube/machines/minikube/id_rsa Username:docker}
I1029 07:44:43.907812    2106 ssh_runner.go:195] Run: cat /etc/os-release
I1029 07:44:43.915060    2106 main.go:134] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I1029 07:44:43.915080    2106 main.go:134] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I1029 07:44:43.915089    2106 main.go:134] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I1029 07:44:43.915093    2106 info.go:137] Remote host: Ubuntu 20.04.4 LTS
I1029 07:44:43.915108    2106 filesync.go:126] Scanning /Users/apple/.minikube/addons for local assets ...
I1029 07:44:43.915271    2106 filesync.go:126] Scanning /Users/apple/.minikube/files for local assets ...
I1029 07:44:43.915340    2106 start.go:310] post-start completed in 196.70194ms
I1029 07:44:43.915415    2106 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I1029 07:44:43.915464    2106 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1029 07:44:43.998832    2106 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49332 SSHKeyPath:/Users/apple/.minikube/machines/minikube/id_rsa Username:docker}
I1029 07:44:44.095954    2106 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I1029 07:44:44.105098    2106 fix.go:57] fixHost completed within 5.9915461s
I1029 07:44:44.105112    2106 start.go:82] releasing machines lock for "minikube", held for 5.992001475s
I1029 07:44:44.105221    2106 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1029 07:44:44.191976    2106 ssh_runner.go:195] Run: curl -sS -m 2 https://k8s.gcr.io/
I1029 07:44:44.191978    2106 ssh_runner.go:195] Run: systemctl --version
I1029 07:44:44.192041    2106 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1029 07:44:44.193053    2106 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1029 07:44:44.275584    2106 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49332 SSHKeyPath:/Users/apple/.minikube/machines/minikube/id_rsa Username:docker}
I1029 07:44:44.278293    2106 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49332 SSHKeyPath:/Users/apple/.minikube/machines/minikube/id_rsa Username:docker}
I1029 07:44:44.702107    2106 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I1029 07:44:44.718020    2106 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (233 bytes)
I1029 07:44:44.736039    2106 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1029 07:44:44.810922    2106 ssh_runner.go:195] Run: sudo systemctl restart cri-docker
I1029 07:44:45.317141    2106 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I1029 07:44:45.332064    2106 cruntime.go:273] skipping containerd shutdown because we are bound to it
I1029 07:44:45.332501    2106 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I1029 07:44:45.347146    2106 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
image-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I1029 07:44:45.364447    2106 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I1029 07:44:45.442934    2106 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I1029 07:44:45.524171    2106 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1029 07:44:45.603186    2106 ssh_runner.go:195] Run: sudo systemctl restart docker
I1029 07:44:45.897256    2106 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I1029 07:44:45.980425    2106 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1029 07:44:46.061307    2106 ssh_runner.go:195] Run: sudo systemctl start cri-docker.socket
I1029 07:44:46.074271    2106 start.go:450] Will wait 60s for socket path /var/run/cri-dockerd.sock
I1029 07:44:46.075182    2106 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I1029 07:44:46.080171    2106 start.go:471] Will wait 60s for crictl version
I1029 07:44:46.080243    2106 ssh_runner.go:195] Run: sudo crictl version
I1029 07:44:46.234956    2106 start.go:480] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  20.10.17
RuntimeApiVersion:  1.41.0
I1029 07:44:46.235040    2106 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1029 07:44:46.451656    2106 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1029 07:44:46.514078    2106 out.go:204] üê≥  Preparing Kubernetes v1.24.3 on Docker 20.10.17 ...
I1029 07:44:46.515382    2106 cli_runner.go:164] Run: docker exec -t minikube dig +short host.docker.internal
I1029 07:44:46.719967    2106 network.go:96] got host ip for mount in container by digging dns: 192.168.65.2
I1029 07:44:46.720667    2106 ssh_runner.go:195] Run: grep 192.168.65.2	host.minikube.internal$ /etc/hosts
I1029 07:44:46.726972    2106 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.65.2	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1029 07:44:46.740403    2106 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1029 07:44:46.825244    2106 preload.go:132] Checking if preload exists for k8s version v1.24.3 and runtime docker
I1029 07:44:46.825304    2106 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1029 07:44:46.861170    2106 docker.go:611] Got preloaded images: -- stdout --
busybox:latest
nginx:latest
k8s.gcr.io/kube-apiserver:v1.24.3
k8s.gcr.io/kube-controller-manager:v1.24.3
k8s.gcr.io/kube-scheduler:v1.24.3
k8s.gcr.io/kube-proxy:v1.24.3
k8s.gcr.io/etcd:3.5.3-0
k8s.gcr.io/pause:3.7
k8s.gcr.io/coredns/coredns:v1.8.6
k8s.gcr.io/pause:3.6
calico/node:v3.20.0
calico/pod2daemon-flexvol:v3.20.0
calico/cni:v3.20.0
calico/kube-controllers:v3.20.0
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1029 07:44:46.861611    2106 docker.go:542] Images already preloaded, skipping extraction
I1029 07:44:46.862071    2106 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1029 07:44:46.896900    2106 docker.go:611] Got preloaded images: -- stdout --
busybox:latest
nginx:latest
k8s.gcr.io/kube-apiserver:v1.24.3
k8s.gcr.io/kube-controller-manager:v1.24.3
k8s.gcr.io/kube-proxy:v1.24.3
k8s.gcr.io/kube-scheduler:v1.24.3
k8s.gcr.io/etcd:3.5.3-0
k8s.gcr.io/pause:3.7
k8s.gcr.io/coredns/coredns:v1.8.6
k8s.gcr.io/pause:3.6
calico/node:v3.20.0
calico/pod2daemon-flexvol:v3.20.0
calico/cni:v3.20.0
calico/kube-controllers:v3.20.0
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1029 07:44:46.897647    2106 cache_images.go:84] Images are preloaded, skipping loading
I1029 07:44:46.898100    2106 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I1029 07:44:47.314361    2106 cni.go:95] Creating CNI manager for "calico"
I1029 07:44:47.315732    2106 kubeadm.go:87] Using pod CIDR: 10.244.0.0/16
I1029 07:44:47.315761    2106 kubeadm.go:158] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.24.3 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NoTaintMaster:true NodeIP:192.168.49.2 CgroupDriver:systemd ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[]}
I1029 07:44:47.316215    2106 kubeadm.go:162] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: /var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.24.3
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: systemd
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I1029 07:44:47.318243    2106 kubeadm.go:961] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.24.3/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime=remote --container-runtime-endpoint=/var/run/cri-dockerd.sock --hostname-override=minikube --image-service-endpoint=/var/run/cri-dockerd.sock --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2 --runtime-request-timeout=15m

[Install]
 config:
{KubernetesVersion:v1.24.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:calico NodeIP: NodePort:8443 NodeName:}
I1029 07:44:47.318330    2106 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.24.3
I1029 07:44:47.332681    2106 binaries.go:44] Found k8s binaries, skipping transfer
I1029 07:44:47.332751    2106 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I1029 07:44:47.342239    2106 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (470 bytes)
I1029 07:44:47.359578    2106 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I1029 07:44:47.374814    2106 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2030 bytes)
I1029 07:44:47.395525    2106 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I1029 07:44:47.399908    2106 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1029 07:44:47.413618    2106 certs.go:54] Setting up /Users/apple/.minikube/profiles/minikube for IP: 192.168.49.2
I1029 07:44:47.413783    2106 certs.go:182] skipping minikubeCA CA generation: /Users/apple/.minikube/ca.key
I1029 07:44:47.414223    2106 certs.go:182] skipping proxyClientCA CA generation: /Users/apple/.minikube/proxy-client-ca.key
I1029 07:44:47.414309    2106 certs.go:298] skipping minikube-user signed cert generation: /Users/apple/.minikube/profiles/minikube/client.key
I1029 07:44:47.414866    2106 certs.go:298] skipping minikube signed cert generation: /Users/apple/.minikube/profiles/minikube/apiserver.key.dd3b5fb2
I1029 07:44:47.415332    2106 certs.go:298] skipping aggregator signed cert generation: /Users/apple/.minikube/profiles/minikube/proxy-client.key
I1029 07:44:47.415573    2106 certs.go:388] found cert: /Users/apple/.minikube/certs/Users/apple/.minikube/certs/ca-key.pem (1675 bytes)
I1029 07:44:47.415607    2106 certs.go:388] found cert: /Users/apple/.minikube/certs/Users/apple/.minikube/certs/ca.pem (1074 bytes)
I1029 07:44:47.415634    2106 certs.go:388] found cert: /Users/apple/.minikube/certs/Users/apple/.minikube/certs/cert.pem (1119 bytes)
I1029 07:44:47.415656    2106 certs.go:388] found cert: /Users/apple/.minikube/certs/Users/apple/.minikube/certs/key.pem (1679 bytes)
I1029 07:44:47.421070    2106 ssh_runner.go:362] scp /Users/apple/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1399 bytes)
I1029 07:44:47.449439    2106 ssh_runner.go:362] scp /Users/apple/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1679 bytes)
I1029 07:44:47.472875    2106 ssh_runner.go:362] scp /Users/apple/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I1029 07:44:47.496151    2106 ssh_runner.go:362] scp /Users/apple/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I1029 07:44:47.517370    2106 ssh_runner.go:362] scp /Users/apple/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I1029 07:44:47.540762    2106 ssh_runner.go:362] scp /Users/apple/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I1029 07:44:47.562548    2106 ssh_runner.go:362] scp /Users/apple/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I1029 07:44:47.587250    2106 ssh_runner.go:362] scp /Users/apple/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I1029 07:44:47.608385    2106 ssh_runner.go:362] scp /Users/apple/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I1029 07:44:47.631596    2106 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I1029 07:44:47.649738    2106 ssh_runner.go:195] Run: openssl version
I1029 07:44:47.660675    2106 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I1029 07:44:47.672787    2106 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I1029 07:44:47.678351    2106 certs.go:431] hashing: -rw-r--r-- 1 root root 1111 Aug 19 12:50 /usr/share/ca-certificates/minikubeCA.pem
I1029 07:44:47.678416    2106 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I1029 07:44:47.685748    2106 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I1029 07:44:47.695069    2106 kubeadm.go:395] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.33@sha256:73b259e144d926189cf169ae5b46bbec4e08e4e2f2bd87296054c3244f70feb8 Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.24.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:calico NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.24.3 ContainerRuntime:docker ControlPlane:true Worker:true} {Name:m02 IP:192.168.49.3 Port:0 KubernetesVersion:v1.24.3 ContainerRuntime:docker ControlPlane:false Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:true ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath:}
I1029 07:44:47.695184    2106 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I1029 07:44:47.730045    2106 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I1029 07:44:47.740825    2106 kubeadm.go:410] found existing configuration files, will attempt cluster restart
I1029 07:44:47.741284    2106 kubeadm.go:626] restartCluster start
I1029 07:44:47.741545    2106 ssh_runner.go:195] Run: sudo test -d /data/minikube
I1029 07:44:47.751325    2106 kubeadm.go:127] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I1029 07:44:47.751387    2106 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1029 07:44:47.849713    2106 kubeconfig.go:92] found "minikube" server: "https://127.0.0.1:54428"
I1029 07:44:47.849738    2106 kubeconfig.go:116] verify returned: got: 127.0.0.1:54428, want: 127.0.0.1:49331
I1029 07:44:47.851514    2106 lock.go:35] WriteFile acquiring /Users/apple/.kube/config: {Name:mk6c4066d2a31d6c55de45dd0f0224f5fa363ab0 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1029 07:44:47.868626    2106 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I1029 07:44:47.879042    2106 api_server.go:165] Checking apiserver status ...
I1029 07:44:47.879144    2106 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1029 07:44:47.891719    2106 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1029 07:44:48.092295    2106 api_server.go:165] Checking apiserver status ...
I1029 07:44:48.092669    2106 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1029 07:44:48.117519    2106 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1029 07:44:48.293042    2106 api_server.go:165] Checking apiserver status ...
I1029 07:44:48.293300    2106 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1029 07:44:48.318072    2106 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1029 07:44:48.492038    2106 api_server.go:165] Checking apiserver status ...
I1029 07:44:48.492271    2106 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1029 07:44:48.513709    2106 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1029 07:44:48.691863    2106 api_server.go:165] Checking apiserver status ...
I1029 07:44:48.691997    2106 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1029 07:44:48.707090    2106 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1029 07:44:48.891965    2106 api_server.go:165] Checking apiserver status ...
I1029 07:44:48.892052    2106 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1029 07:44:48.905418    2106 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1029 07:44:49.093046    2106 api_server.go:165] Checking apiserver status ...
I1029 07:44:49.093359    2106 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1029 07:44:49.117185    2106 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1029 07:44:49.293013    2106 api_server.go:165] Checking apiserver status ...
I1029 07:44:49.293322    2106 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1029 07:44:49.318203    2106 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1029 07:44:49.493056    2106 api_server.go:165] Checking apiserver status ...
I1029 07:44:49.493331    2106 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1029 07:44:49.518087    2106 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1029 07:44:49.693111    2106 api_server.go:165] Checking apiserver status ...
I1029 07:44:49.693403    2106 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1029 07:44:49.717108    2106 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1029 07:44:49.893029    2106 api_server.go:165] Checking apiserver status ...
I1029 07:44:49.893295    2106 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1029 07:44:49.916412    2106 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1029 07:44:50.092081    2106 api_server.go:165] Checking apiserver status ...
I1029 07:44:50.092378    2106 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1029 07:44:50.116565    2106 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1029 07:44:50.293109    2106 api_server.go:165] Checking apiserver status ...
I1029 07:44:50.293438    2106 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1029 07:44:50.318553    2106 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1029 07:44:50.492290    2106 api_server.go:165] Checking apiserver status ...
I1029 07:44:50.492568    2106 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1029 07:44:50.515881    2106 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1029 07:44:50.693011    2106 api_server.go:165] Checking apiserver status ...
I1029 07:44:50.693303    2106 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1029 07:44:50.717139    2106 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1029 07:44:50.893180    2106 api_server.go:165] Checking apiserver status ...
I1029 07:44:50.893476    2106 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1029 07:44:50.916923    2106 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1029 07:44:50.916933    2106 api_server.go:165] Checking apiserver status ...
I1029 07:44:50.917007    2106 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1029 07:44:50.930524    2106 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1029 07:44:50.930537    2106 kubeadm.go:601] needs reconfigure: apiserver error: timed out waiting for the condition
I1029 07:44:50.930547    2106 kubeadm.go:1092] stopping kube-system containers ...
I1029 07:44:50.930608    2106 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I1029 07:44:50.966611    2106 docker.go:443] Stopping containers: [1d0857dd7d66 b48d54bef813 0183947183c3 17ab07340cfc 3ed7d6d578d1 81307dbb1e6a d75dd84abfe7 887cd09b2042 ae8202518c8c edf164be52e8 f52bcc2527f6 e53ba22093f2 4bc7283e9578 7bc96f7dd37f 80f3da2d5c1c d8c20ccfa274 fa7ea1a28f18 66c111322780 9b0af2aac98b 41de401be4e4 02c1f8e3b73f 266af977f8a7]
I1029 07:44:50.966694    2106 ssh_runner.go:195] Run: docker stop 1d0857dd7d66 b48d54bef813 0183947183c3 17ab07340cfc 3ed7d6d578d1 81307dbb1e6a d75dd84abfe7 887cd09b2042 ae8202518c8c edf164be52e8 f52bcc2527f6 e53ba22093f2 4bc7283e9578 7bc96f7dd37f 80f3da2d5c1c d8c20ccfa274 fa7ea1a28f18 66c111322780 9b0af2aac98b 41de401be4e4 02c1f8e3b73f 266af977f8a7
I1029 07:44:51.004165    2106 ssh_runner.go:195] Run: sudo systemctl stop kubelet
I1029 07:44:51.016886    2106 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I1029 07:44:51.026235    2106 kubeadm.go:155] found existing configuration files:
-rw------- 1 root root 5639 Oct 28 06:26 /etc/kubernetes/admin.conf
-rw------- 1 root root 5656 Oct 28 06:26 /etc/kubernetes/controller-manager.conf
-rw------- 1 root root 1971 Oct 28 06:26 /etc/kubernetes/kubelet.conf
-rw------- 1 root root 5600 Oct 28 06:26 /etc/kubernetes/scheduler.conf

I1029 07:44:51.026298    2106 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I1029 07:44:51.035938    2106 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I1029 07:44:51.045949    2106 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I1029 07:44:51.056150    2106 kubeadm.go:166] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 1
stdout:

stderr:
I1029 07:44:51.056216    2106 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I1029 07:44:51.065871    2106 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I1029 07:44:51.075972    2106 kubeadm.go:166] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 1
stdout:

stderr:
I1029 07:44:51.076041    2106 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I1029 07:44:51.084919    2106 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I1029 07:44:51.094201    2106 kubeadm.go:703] reconfiguring cluster from /var/tmp/minikube/kubeadm.yaml
I1029 07:44:51.094213    2106 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.24.3:$PATH" kubeadm init phase certs all --config /var/tmp/minikube/kubeadm.yaml"
I1029 07:44:51.302357    2106 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.24.3:$PATH" kubeadm init phase kubeconfig all --config /var/tmp/minikube/kubeadm.yaml"
I1029 07:44:52.082672    2106 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.24.3:$PATH" kubeadm init phase kubelet-start --config /var/tmp/minikube/kubeadm.yaml"
I1029 07:44:52.621775    2106 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.24.3:$PATH" kubeadm init phase control-plane all --config /var/tmp/minikube/kubeadm.yaml"
I1029 07:44:52.689906    2106 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.24.3:$PATH" kubeadm init phase etcd local --config /var/tmp/minikube/kubeadm.yaml"
I1029 07:44:52.745183    2106 api_server.go:51] waiting for apiserver process to appear ...
I1029 07:44:52.745266    2106 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1029 07:44:53.260836    2106 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1029 07:44:53.759472    2106 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1029 07:44:54.259432    2106 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1029 07:44:54.273403    2106 api_server.go:71] duration metric: took 1.528220801s to wait for apiserver process to appear ...
I1029 07:44:54.273415    2106 api_server.go:87] waiting for apiserver healthz status ...
I1029 07:44:54.273426    2106 api_server.go:240] Checking apiserver healthz at https://127.0.0.1:49331/healthz ...
I1029 07:44:54.275444    2106 api_server.go:256] stopped: https://127.0.0.1:49331/healthz: Get "https://127.0.0.1:49331/healthz": EOF
I1029 07:44:54.775815    2106 api_server.go:240] Checking apiserver healthz at https://127.0.0.1:49331/healthz ...
I1029 07:44:54.778072    2106 api_server.go:256] stopped: https://127.0.0.1:49331/healthz: Get "https://127.0.0.1:49331/healthz": EOF
I1029 07:44:55.275895    2106 api_server.go:240] Checking apiserver healthz at https://127.0.0.1:49331/healthz ...
I1029 07:44:58.599606    2106 api_server.go:266] https://127.0.0.1:49331/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W1029 07:44:58.600335    2106 api_server.go:102] status: https://127.0.0.1:49331/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I1029 07:44:58.776524    2106 api_server.go:240] Checking apiserver healthz at https://127.0.0.1:49331/healthz ...
I1029 07:44:58.810768    2106 api_server.go:266] https://127.0.0.1:49331/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1029 07:44:58.810792    2106 api_server.go:102] status: https://127.0.0.1:49331/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1029 07:44:59.276298    2106 api_server.go:240] Checking apiserver healthz at https://127.0.0.1:49331/healthz ...
I1029 07:44:59.283915    2106 api_server.go:266] https://127.0.0.1:49331/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1029 07:44:59.283933    2106 api_server.go:102] status: https://127.0.0.1:49331/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1029 07:44:59.777098    2106 api_server.go:240] Checking apiserver healthz at https://127.0.0.1:49331/healthz ...
I1029 07:44:59.811278    2106 api_server.go:266] https://127.0.0.1:49331/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1029 07:44:59.811300    2106 api_server.go:102] status: https://127.0.0.1:49331/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1029 07:45:00.276727    2106 api_server.go:240] Checking apiserver healthz at https://127.0.0.1:49331/healthz ...
I1029 07:45:00.304179    2106 api_server.go:266] https://127.0.0.1:49331/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1029 07:45:00.304193    2106 api_server.go:102] status: https://127.0.0.1:49331/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1029 07:45:00.775635    2106 api_server.go:240] Checking apiserver healthz at https://127.0.0.1:49331/healthz ...
I1029 07:45:00.786893    2106 api_server.go:266] https://127.0.0.1:49331/healthz returned 200:
ok
I1029 07:45:00.806499    2106 api_server.go:140] control plane version: v1.24.3
I1029 07:45:00.806510    2106 api_server.go:130] duration metric: took 6.533107928s to wait for apiserver health ...
I1029 07:45:00.806516    2106 cni.go:95] Creating CNI manager for "calico"
I1029 07:45:00.825608    2106 out.go:177] üîó  Configuring Calico (Container Networking Interface) ...
I1029 07:45:00.844862    2106 cni.go:189] applying CNI manifest using /var/lib/minikube/binaries/v1.24.3/kubectl ...
I1029 07:45:00.844880    2106 ssh_runner.go:362] scp memory --> /var/tmp/minikube/cni.yaml (202050 bytes)
I1029 07:45:00.878691    2106 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.24.3/kubectl apply --kubeconfig=/var/lib/minikube/kubeconfig -f /var/tmp/minikube/cni.yaml
I1029 07:45:02.310718    2106 ssh_runner.go:235] Completed: sudo /var/lib/minikube/binaries/v1.24.3/kubectl apply --kubeconfig=/var/lib/minikube/kubeconfig -f /var/tmp/minikube/cni.yaml: (1.434086234s)
I1029 07:45:02.311263    2106 system_pods.go:43] waiting for kube-system pods to appear ...
I1029 07:45:02.336640    2106 system_pods.go:59] 11 kube-system pods found
I1029 07:45:02.336659    2106 system_pods.go:61] "calico-kube-controllers-c44b4545-28m7f" [b11dc76f-52e2-4dc3-b575-c17059267c9b] Running / Ready:ContainersNotReady (containers with unready status: [calico-kube-controllers]) / ContainersReady:ContainersNotReady (containers with unready status: [calico-kube-controllers])
I1029 07:45:02.336663    2106 system_pods.go:61] "calico-node-pmpd5" [52354293-5448-4bb4-a29a-58cc2884c4e1] Running
I1029 07:45:02.336677    2106 system_pods.go:61] "calico-node-vw5c8" [424eac02-d931-41ae-9519-6d7bc009dfa0] Running / Ready:ContainersNotReady (containers with unready status: [calico-node]) / ContainersReady:ContainersNotReady (containers with unready status: [calico-node])
I1029 07:45:02.336686    2106 system_pods.go:61] "coredns-6d4b75cb6d-nr4nw" [a93844ca-c27b-45a0-893f-988a5ff74c26] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I1029 07:45:02.336690    2106 system_pods.go:61] "etcd-minikube" [e9321f6c-8ca1-4f17-a976-17e3010b1e18] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I1029 07:45:02.336695    2106 system_pods.go:61] "kube-apiserver-minikube" [b11272e8-b981-4a12-83aa-8fda4ee6d1f5] Running
I1029 07:45:02.336698    2106 system_pods.go:61] "kube-controller-manager-minikube" [c7e47158-0750-4ea7-94a2-5630e52e6876] Running
I1029 07:45:02.336701    2106 system_pods.go:61] "kube-proxy-2fz2h" [ed475400-e647-4255-b788-f4a5fdcc553a] Running
I1029 07:45:02.336704    2106 system_pods.go:61] "kube-proxy-6s9dq" [68f8c0ea-e67b-487e-a56c-0047885f74e0] Running
I1029 07:45:02.336706    2106 system_pods.go:61] "kube-scheduler-minikube" [d2f312ec-447f-4286-8aa9-a494d2821183] Running
I1029 07:45:02.336710    2106 system_pods.go:61] "storage-provisioner" [26166ca8-49b5-4b84-b523-9d847b768e19] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I1029 07:45:02.336714    2106 system_pods.go:74] duration metric: took 25.501605ms to wait for pod list to return data ...
I1029 07:45:02.336721    2106 node_conditions.go:102] verifying NodePressure condition ...
I1029 07:45:02.344503    2106 node_conditions.go:122] node storage ephemeral capacity is 61255492Ki
I1029 07:45:02.344517    2106 node_conditions.go:123] node cpu capacity is 4
I1029 07:45:02.346584    2106 node_conditions.go:122] node storage ephemeral capacity is 61255492Ki
I1029 07:45:02.346591    2106 node_conditions.go:123] node cpu capacity is 4
I1029 07:45:02.346595    2106 node_conditions.go:105] duration metric: took 9.892477ms to run NodePressure ...
I1029 07:45:02.348675    2106 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.24.3:$PATH" kubeadm init phase addon all --config /var/tmp/minikube/kubeadm.yaml"
I1029 07:45:02.556606    2106 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I1029 07:45:02.565916    2106 ops.go:34] apiserver oom_adj: -16
I1029 07:45:02.565925    2106 kubeadm.go:630] restartCluster took 14.827285801s
I1029 07:45:02.565935    2106 kubeadm.go:397] StartCluster complete in 14.87352422s
I1029 07:45:02.566366    2106 settings.go:142] acquiring lock: {Name:mk692f39c99f57abbbaf0dc79849e928ca42cce9 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1029 07:45:02.569435    2106 settings.go:150] Updating kubeconfig:  /Users/apple/.kube/config
I1029 07:45:02.570364    2106 lock.go:35] WriteFile acquiring /Users/apple/.kube/config: {Name:mk6c4066d2a31d6c55de45dd0f0224f5fa363ab0 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1029 07:45:02.579475    2106 kapi.go:244] deployment "coredns" in namespace "kube-system" and context "minikube" rescaled to 1
I1029 07:45:02.579569    2106 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.24.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I1029 07:45:02.580203    2106 start.go:211] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.24.3 ContainerRuntime:docker ControlPlane:true Worker:true}
I1029 07:45:02.581260    2106 config.go:180] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.24.3
I1029 07:45:02.581287    2106 addons.go:412] enableAddons start: toEnable=map[default-storageclass:true storage-provisioner:true], additional=[]
I1029 07:45:02.607527    2106 addons.go:65] Setting storage-provisioner=true in profile "minikube"
I1029 07:45:02.607557    2106 out.go:177] üîé  Verifying Kubernetes components...
I1029 07:45:02.607571    2106 addons.go:65] Setting default-storageclass=true in profile "minikube"
I1029 07:45:02.660499    2106 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service kubelet
I1029 07:45:02.607979    2106 addons.go:153] Setting addon storage-provisioner=true in "minikube"
W1029 07:45:02.660518    2106 addons.go:162] addon storage-provisioner should already be in state true
I1029 07:45:02.660857    2106 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I1029 07:45:02.661942    2106 host.go:66] Checking if "minikube" exists ...
I1029 07:45:02.662304    2106 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1029 07:45:02.663552    2106 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1029 07:45:02.728030    2106 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1029 07:45:02.728571    2106 start.go:789] CoreDNS already contains "host.minikube.internal" host record, skipping...
I1029 07:45:03.148932    2106 api_server.go:51] waiting for apiserver process to appear ...
I1029 07:45:03.167440    2106 out.go:177]     ‚ñ™ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I1029 07:45:03.159896    2106 addons.go:153] Setting addon default-storageclass=true in "minikube"
W1029 07:45:03.167473    2106 addons.go:162] addon default-storageclass should already be in state true
I1029 07:45:03.167569    2106 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1029 07:45:03.186259    2106 host.go:66] Checking if "minikube" exists ...
I1029 07:45:03.186317    2106 addons.go:345] installing /etc/kubernetes/addons/storage-provisioner.yaml
I1029 07:45:03.186326    2106 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I1029 07:45:03.186370    2106 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1029 07:45:03.187250    2106 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1029 07:45:03.209689    2106 api_server.go:71] duration metric: took 630.732073ms to wait for apiserver process to appear ...
I1029 07:45:03.209712    2106 api_server.go:87] waiting for apiserver healthz status ...
I1029 07:45:03.209724    2106 api_server.go:240] Checking apiserver healthz at https://127.0.0.1:49331/healthz ...
I1029 07:45:03.222108    2106 api_server.go:266] https://127.0.0.1:49331/healthz returned 200:
ok
I1029 07:45:03.227754    2106 api_server.go:140] control plane version: v1.24.3
I1029 07:45:03.227765    2106 api_server.go:130] duration metric: took 18.087353ms to wait for apiserver health ...
I1029 07:45:03.227770    2106 system_pods.go:43] waiting for kube-system pods to appear ...
I1029 07:45:03.246107    2106 system_pods.go:59] 11 kube-system pods found
I1029 07:45:03.246122    2106 system_pods.go:61] "calico-kube-controllers-c44b4545-28m7f" [b11dc76f-52e2-4dc3-b575-c17059267c9b] Running / Ready:ContainersNotReady (containers with unready status: [calico-kube-controllers]) / ContainersReady:ContainersNotReady (containers with unready status: [calico-kube-controllers])
I1029 07:45:03.246127    2106 system_pods.go:61] "calico-node-pmpd5" [52354293-5448-4bb4-a29a-58cc2884c4e1] Running
I1029 07:45:03.246132    2106 system_pods.go:61] "calico-node-vw5c8" [424eac02-d931-41ae-9519-6d7bc009dfa0] Running / Ready:ContainersNotReady (containers with unready status: [calico-node]) / ContainersReady:ContainersNotReady (containers with unready status: [calico-node])
I1029 07:45:03.246138    2106 system_pods.go:61] "coredns-6d4b75cb6d-nr4nw" [a93844ca-c27b-45a0-893f-988a5ff74c26] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I1029 07:45:03.246143    2106 system_pods.go:61] "etcd-minikube" [e9321f6c-8ca1-4f17-a976-17e3010b1e18] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I1029 07:45:03.246150    2106 system_pods.go:61] "kube-apiserver-minikube" [b11272e8-b981-4a12-83aa-8fda4ee6d1f5] Running
I1029 07:45:03.246153    2106 system_pods.go:61] "kube-controller-manager-minikube" [c7e47158-0750-4ea7-94a2-5630e52e6876] Running
I1029 07:45:03.246157    2106 system_pods.go:61] "kube-proxy-2fz2h" [ed475400-e647-4255-b788-f4a5fdcc553a] Running
I1029 07:45:03.246159    2106 system_pods.go:61] "kube-proxy-6s9dq" [68f8c0ea-e67b-487e-a56c-0047885f74e0] Running
I1029 07:45:03.246162    2106 system_pods.go:61] "kube-scheduler-minikube" [d2f312ec-447f-4286-8aa9-a494d2821183] Running
I1029 07:45:03.246164    2106 system_pods.go:61] "storage-provisioner" [26166ca8-49b5-4b84-b523-9d847b768e19] Running
I1029 07:45:03.246168    2106 system_pods.go:74] duration metric: took 18.433536ms to wait for pod list to return data ...
I1029 07:45:03.246174    2106 kubeadm.go:572] duration metric: took 667.334223ms to wait for : map[apiserver:true system_pods:true] ...
I1029 07:45:03.246183    2106 node_conditions.go:102] verifying NodePressure condition ...
I1029 07:45:03.253907    2106 node_conditions.go:122] node storage ephemeral capacity is 61255492Ki
I1029 07:45:03.253917    2106 node_conditions.go:123] node cpu capacity is 4
I1029 07:45:03.253931    2106 node_conditions.go:122] node storage ephemeral capacity is 61255492Ki
I1029 07:45:03.253936    2106 node_conditions.go:123] node cpu capacity is 4
I1029 07:45:03.253939    2106 node_conditions.go:105] duration metric: took 7.769426ms to run NodePressure ...
I1029 07:45:03.253948    2106 start.go:216] waiting for startup goroutines ...
I1029 07:45:03.285642    2106 addons.go:345] installing /etc/kubernetes/addons/storageclass.yaml
I1029 07:45:03.285651    2106 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I1029 07:45:03.285732    2106 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1029 07:45:03.286804    2106 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49332 SSHKeyPath:/Users/apple/.minikube/machines/minikube/id_rsa Username:docker}
I1029 07:45:03.374171    2106 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49332 SSHKeyPath:/Users/apple/.minikube/machines/minikube/id_rsa Username:docker}
I1029 07:45:03.401038    2106 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.24.3/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I1029 07:45:03.480358    2106 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.24.3/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I1029 07:45:03.751396    2106 out.go:177] üåü  Enabled addons: storage-provisioner, default-storageclass
I1029 07:45:03.789759    2106 addons.go:414] enableAddons completed in 1.211928574s
I1029 07:45:03.791285    2106 config.go:180] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.24.3
I1029 07:45:03.791473    2106 profile.go:148] Saving config to /Users/apple/.minikube/profiles/minikube/config.json ...
I1029 07:45:03.811402    2106 out.go:177] üëç  Starting worker node minikube-m02 in cluster minikube
I1029 07:45:03.849508    2106 cache.go:120] Beginning downloading kic base image for docker with docker
I1029 07:45:03.886260    2106 out.go:177] üöú  Pulling base image ...
I1029 07:45:03.924639    2106 preload.go:132] Checking if preload exists for k8s version v1.24.3 and runtime docker
I1029 07:45:03.924657    2106 cache.go:57] Caching tarball of preloaded images
I1029 07:45:03.924903    2106 preload.go:174] Found /Users/apple/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.24.3-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I1029 07:45:03.924916    2106 cache.go:60] Finished verifying existence of preloaded tar for  v1.24.3 on docker
I1029 07:45:03.925068    2106 image.go:75] Checking for gcr.io/k8s-minikube/kicbase:v0.0.33@sha256:73b259e144d926189cf169ae5b46bbec4e08e4e2f2bd87296054c3244f70feb8 in local docker daemon
I1029 07:45:03.927252    2106 profile.go:148] Saving config to /Users/apple/.minikube/profiles/minikube/config.json ...
I1029 07:45:04.025809    2106 image.go:79] Found gcr.io/k8s-minikube/kicbase:v0.0.33@sha256:73b259e144d926189cf169ae5b46bbec4e08e4e2f2bd87296054c3244f70feb8 in local docker daemon, skipping pull
I1029 07:45:04.025824    2106 cache.go:142] gcr.io/k8s-minikube/kicbase:v0.0.33@sha256:73b259e144d926189cf169ae5b46bbec4e08e4e2f2bd87296054c3244f70feb8 exists in daemon, skipping load
I1029 07:45:04.025834    2106 cache.go:208] Successfully downloaded all kic artifacts
I1029 07:45:04.025884    2106 start.go:371] acquiring machines lock for minikube-m02: {Name:mk640ed8a7d64e195c66f34ce270eab43cfd6179 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I1029 07:45:04.026073    2106 start.go:375] acquired machines lock for "minikube-m02" in 173.756¬µs
I1029 07:45:04.026102    2106 start.go:95] Skipping create...Using existing machine configuration
I1029 07:45:04.026107    2106 fix.go:55] fixHost starting: m02
I1029 07:45:04.026728    2106 cli_runner.go:164] Run: docker container inspect minikube-m02 --format={{.State.Status}}
I1029 07:45:04.108299    2106 fix.go:103] recreateIfNeeded on minikube-m02: state=Stopped err=<nil>
W1029 07:45:04.108318    2106 fix.go:129] unexpected machine state, will restart: <nil>
I1029 07:45:04.163946    2106 out.go:177] üîÑ  Restarting existing docker container for "minikube-m02" ...
I1029 07:45:04.199125    2106 cli_runner.go:164] Run: docker start minikube-m02
I1029 07:45:04.648302    2106 cli_runner.go:164] Run: docker container inspect minikube-m02 --format={{.State.Status}}
I1029 07:45:04.765272    2106 kic.go:415] container "minikube-m02" state is running.
I1029 07:45:04.766944    2106 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube-m02
I1029 07:45:04.857828    2106 profile.go:148] Saving config to /Users/apple/.minikube/profiles/minikube/config.json ...
I1029 07:45:04.858330    2106 machine.go:88] provisioning docker machine ...
I1029 07:45:04.858349    2106 ubuntu.go:169] provisioning hostname "minikube-m02"
I1029 07:45:04.858400    2106 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02
I1029 07:45:04.953852    2106 main.go:134] libmachine: Using SSH client type: native
I1029 07:45:04.954586    2106 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x43d9d20] 0x43dcd80 <nil>  [] 0s} 127.0.0.1 49373 <nil> <nil>}
I1029 07:45:04.954598    2106 main.go:134] libmachine: About to run SSH command:
sudo hostname minikube-m02 && echo "minikube-m02" | sudo tee /etc/hostname
I1029 07:45:04.956243    2106 main.go:134] libmachine: Error dialing TCP: ssh: handshake failed: EOF
I1029 07:45:08.099451    2106 main.go:134] libmachine: SSH cmd err, output: <nil>: minikube-m02

I1029 07:45:08.099548    2106 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02
I1029 07:45:08.186709    2106 main.go:134] libmachine: Using SSH client type: native
I1029 07:45:08.186890    2106 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x43d9d20] 0x43dcd80 <nil>  [] 0s} 127.0.0.1 49373 <nil> <nil>}
I1029 07:45:08.186901    2106 main.go:134] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube-m02' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube-m02/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube-m02' | sudo tee -a /etc/hosts; 
			fi
		fi
I1029 07:45:08.311479    2106 main.go:134] libmachine: SSH cmd err, output: <nil>: 
I1029 07:45:08.311496    2106 ubuntu.go:175] set auth options {CertDir:/Users/apple/.minikube CaCertPath:/Users/apple/.minikube/certs/ca.pem CaPrivateKeyPath:/Users/apple/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/Users/apple/.minikube/machines/server.pem ServerKeyPath:/Users/apple/.minikube/machines/server-key.pem ClientKeyPath:/Users/apple/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/Users/apple/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/Users/apple/.minikube}
I1029 07:45:08.311509    2106 ubuntu.go:177] setting up certificates
I1029 07:45:08.311517    2106 provision.go:83] configureAuth start
I1029 07:45:08.311582    2106 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube-m02
I1029 07:45:08.421894    2106 provision.go:138] copyHostCerts
I1029 07:45:08.422046    2106 exec_runner.go:144] found /Users/apple/.minikube/cert.pem, removing ...
I1029 07:45:08.422052    2106 exec_runner.go:207] rm: /Users/apple/.minikube/cert.pem
I1029 07:45:08.422208    2106 exec_runner.go:151] cp: /Users/apple/.minikube/certs/cert.pem --> /Users/apple/.minikube/cert.pem (1119 bytes)
I1029 07:45:08.423371    2106 exec_runner.go:144] found /Users/apple/.minikube/key.pem, removing ...
I1029 07:45:08.423378    2106 exec_runner.go:207] rm: /Users/apple/.minikube/key.pem
I1029 07:45:08.423460    2106 exec_runner.go:151] cp: /Users/apple/.minikube/certs/key.pem --> /Users/apple/.minikube/key.pem (1679 bytes)
I1029 07:45:08.424073    2106 exec_runner.go:144] found /Users/apple/.minikube/ca.pem, removing ...
I1029 07:45:08.424087    2106 exec_runner.go:207] rm: /Users/apple/.minikube/ca.pem
I1029 07:45:08.424163    2106 exec_runner.go:151] cp: /Users/apple/.minikube/certs/ca.pem --> /Users/apple/.minikube/ca.pem (1074 bytes)
I1029 07:45:08.424451    2106 provision.go:112] generating server cert: /Users/apple/.minikube/machines/server.pem ca-key=/Users/apple/.minikube/certs/ca.pem private-key=/Users/apple/.minikube/certs/ca-key.pem org=apple.minikube-m02 san=[192.168.49.3 127.0.0.1 localhost 127.0.0.1 minikube minikube-m02]
I1029 07:45:08.575907    2106 provision.go:172] copyRemoteCerts
I1029 07:45:08.575971    2106 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I1029 07:45:08.576005    2106 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02
I1029 07:45:08.657591    2106 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49373 SSHKeyPath:/Users/apple/.minikube/machines/minikube-m02/id_rsa Username:docker}
I1029 07:45:08.759522    2106 ssh_runner.go:362] scp /Users/apple/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1074 bytes)
I1029 07:45:08.786319    2106 ssh_runner.go:362] scp /Users/apple/.minikube/machines/server.pem --> /etc/docker/server.pem (1208 bytes)
I1029 07:45:08.809529    2106 ssh_runner.go:362] scp /Users/apple/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I1029 07:45:08.837671    2106 provision.go:86] duration metric: configureAuth took 526.894565ms
I1029 07:45:08.837688    2106 ubuntu.go:193] setting minikube options for container-runtime
I1029 07:45:08.837884    2106 config.go:180] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.24.3
I1029 07:45:08.837936    2106 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02
I1029 07:45:08.921986    2106 main.go:134] libmachine: Using SSH client type: native
I1029 07:45:08.922212    2106 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x43d9d20] 0x43dcd80 <nil>  [] 0s} 127.0.0.1 49373 <nil> <nil>}
I1029 07:45:08.922219    2106 main.go:134] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I1029 07:45:09.053092    2106 main.go:134] libmachine: SSH cmd err, output: <nil>: overlay

I1029 07:45:09.053105    2106 ubuntu.go:71] root file system type: overlay
I1029 07:45:09.053294    2106 provision.go:309] Updating docker unit: /lib/systemd/system/docker.service ...
I1029 07:45:09.053388    2106 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02
I1029 07:45:09.135520    2106 main.go:134] libmachine: Using SSH client type: native
I1029 07:45:09.135731    2106 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x43d9d20] 0x43dcd80 <nil>  [] 0s} 127.0.0.1 49373 <nil> <nil>}
I1029 07:45:09.135786    2106 main.go:134] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure

Environment="NO_PROXY=192.168.49.2"


# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I1029 07:45:09.279134    2106 main.go:134] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure

Environment=NO_PROXY=192.168.49.2


# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I1029 07:45:09.279223    2106 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02
I1029 07:45:09.357666    2106 main.go:134] libmachine: Using SSH client type: native
I1029 07:45:09.357853    2106 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x43d9d20] 0x43dcd80 <nil>  [] 0s} 127.0.0.1 49373 <nil> <nil>}
I1029 07:45:09.357864    2106 main.go:134] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I1029 07:45:09.494170    2106 main.go:134] libmachine: SSH cmd err, output: <nil>: 
I1029 07:45:09.494193    2106 machine.go:91] provisioned docker machine in 4.643211987s
I1029 07:45:09.494202    2106 start.go:307] post-start starting for "minikube-m02" (driver="docker")
I1029 07:45:09.494208    2106 start.go:335] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I1029 07:45:09.494335    2106 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I1029 07:45:09.494388    2106 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02
I1029 07:45:09.579534    2106 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49373 SSHKeyPath:/Users/apple/.minikube/machines/minikube-m02/id_rsa Username:docker}
I1029 07:45:09.672929    2106 ssh_runner.go:195] Run: cat /etc/os-release
I1029 07:45:09.681135    2106 main.go:134] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I1029 07:45:09.681151    2106 main.go:134] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I1029 07:45:09.681159    2106 main.go:134] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I1029 07:45:09.681163    2106 info.go:137] Remote host: Ubuntu 20.04.4 LTS
I1029 07:45:09.681170    2106 filesync.go:126] Scanning /Users/apple/.minikube/addons for local assets ...
I1029 07:45:09.681288    2106 filesync.go:126] Scanning /Users/apple/.minikube/files for local assets ...
I1029 07:45:09.681322    2106 start.go:310] post-start completed in 187.363505ms
I1029 07:45:09.681373    2106 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I1029 07:45:09.681406    2106 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02
I1029 07:45:09.763460    2106 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49373 SSHKeyPath:/Users/apple/.minikube/machines/minikube-m02/id_rsa Username:docker}
I1029 07:45:09.856568    2106 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I1029 07:45:09.866705    2106 fix.go:57] fixHost completed within 5.85000609s
I1029 07:45:09.866722    2106 start.go:82] releasing machines lock for "minikube-m02", held for 5.850059505s
I1029 07:45:09.866827    2106 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube-m02
I1029 07:45:09.971714    2106 out.go:177] üåê  Found network options:
I1029 07:45:09.990040    2106 out.go:177]     ‚ñ™ NO_PROXY=192.168.49.2
W1029 07:45:10.011285    2106 proxy.go:118] fail to check proxy env: Error ip not in block
W1029 07:45:10.011879    2106 proxy.go:118] fail to check proxy env: Error ip not in block
I1029 07:45:10.012078    2106 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I1029 07:45:10.012084    2106 ssh_runner.go:195] Run: curl -sS -m 2 https://k8s.gcr.io/
I1029 07:45:10.012137    2106 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02
I1029 07:45:10.012149    2106 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02
I1029 07:45:10.102924    2106 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49373 SSHKeyPath:/Users/apple/.minikube/machines/minikube-m02/id_rsa Username:docker}
I1029 07:45:10.103263    2106 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49373 SSHKeyPath:/Users/apple/.minikube/machines/minikube-m02/id_rsa Username:docker}
I1029 07:45:10.263190    2106 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (233 bytes)
I1029 07:45:10.283206    2106 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1029 07:45:10.393595    2106 ssh_runner.go:195] Run: sudo systemctl restart cri-docker
I1029 07:45:10.599889    2106 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I1029 07:45:10.617754    2106 cruntime.go:273] skipping containerd shutdown because we are bound to it
I1029 07:45:10.617828    2106 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I1029 07:45:10.633701    2106 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
image-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I1029 07:45:10.654742    2106 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I1029 07:45:10.756074    2106 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I1029 07:45:10.846547    2106 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1029 07:45:10.935765    2106 ssh_runner.go:195] Run: sudo systemctl restart docker
I1029 07:45:11.238854    2106 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I1029 07:45:11.323817    2106 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1029 07:45:11.409388    2106 ssh_runner.go:195] Run: sudo systemctl start cri-docker.socket
I1029 07:45:11.425837    2106 start.go:450] Will wait 60s for socket path /var/run/cri-dockerd.sock
I1029 07:45:11.425931    2106 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I1029 07:45:11.431483    2106 start.go:471] Will wait 60s for crictl version
I1029 07:45:11.431552    2106 ssh_runner.go:195] Run: sudo crictl version
I1029 07:45:11.481156    2106 start.go:480] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  20.10.17
RuntimeApiVersion:  1.41.0
I1029 07:45:11.481218    2106 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1029 07:45:11.521920    2106 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1029 07:45:11.587924    2106 out.go:204] üê≥  Preparing Kubernetes v1.24.3 on Docker 20.10.17 ...
I1029 07:45:11.606124    2106 out.go:177]     ‚ñ™ env NO_PROXY=192.168.49.2
I1029 07:45:11.625915    2106 cli_runner.go:164] Run: docker exec -t minikube-m02 dig +short host.docker.internal
I1029 07:45:11.802819    2106 network.go:96] got host ip for mount in container by digging dns: 192.168.65.2
I1029 07:45:11.802945    2106 ssh_runner.go:195] Run: grep 192.168.65.2	host.minikube.internal$ /etc/hosts
I1029 07:45:11.812247    2106 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.65.2	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1029 07:45:11.827924    2106 certs.go:54] Setting up /Users/apple/.minikube/profiles/minikube for IP: 192.168.49.3
I1029 07:45:11.828089    2106 certs.go:182] skipping minikubeCA CA generation: /Users/apple/.minikube/ca.key
I1029 07:45:11.828601    2106 certs.go:182] skipping proxyClientCA CA generation: /Users/apple/.minikube/proxy-client-ca.key
I1029 07:45:11.828803    2106 certs.go:388] found cert: /Users/apple/.minikube/certs/Users/apple/.minikube/certs/ca-key.pem (1675 bytes)
I1029 07:45:11.828849    2106 certs.go:388] found cert: /Users/apple/.minikube/certs/Users/apple/.minikube/certs/ca.pem (1074 bytes)
I1029 07:45:11.828881    2106 certs.go:388] found cert: /Users/apple/.minikube/certs/Users/apple/.minikube/certs/cert.pem (1119 bytes)
I1029 07:45:11.828912    2106 certs.go:388] found cert: /Users/apple/.minikube/certs/Users/apple/.minikube/certs/key.pem (1679 bytes)
I1029 07:45:11.829344    2106 ssh_runner.go:362] scp /Users/apple/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I1029 07:45:11.857168    2106 ssh_runner.go:362] scp /Users/apple/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I1029 07:45:11.890085    2106 ssh_runner.go:362] scp /Users/apple/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I1029 07:45:11.922601    2106 ssh_runner.go:362] scp /Users/apple/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I1029 07:45:11.953847    2106 ssh_runner.go:362] scp /Users/apple/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I1029 07:45:11.979363    2106 ssh_runner.go:195] Run: openssl version
I1029 07:45:11.986734    2106 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I1029 07:45:11.997964    2106 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I1029 07:45:12.004105    2106 certs.go:431] hashing: -rw-r--r-- 1 root root 1111 Aug 19 12:50 /usr/share/ca-certificates/minikubeCA.pem
I1029 07:45:12.004176    2106 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I1029 07:45:12.014524    2106 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I1029 07:45:12.023920    2106 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I1029 07:45:12.120887    2106 cni.go:95] Creating CNI manager for "calico"
I1029 07:45:12.120896    2106 kubeadm.go:87] Using pod CIDR: 10.244.0.0/16
I1029 07:45:12.120906    2106 kubeadm.go:158] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.3 APIServerPort:8443 KubernetesVersion:v1.24.3 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube-m02 DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NoTaintMaster:true NodeIP:192.168.49.3 CgroupDriver:systemd ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[]}
I1029 07:45:12.120998    2106 kubeadm.go:162] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.3
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: /var/run/cri-dockerd.sock
  name: "minikube-m02"
  kubeletExtraArgs:
    node-ip: 192.168.49.3
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.24.3
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: systemd
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I1029 07:45:12.121050    2106 kubeadm.go:961] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.24.3/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime=remote --container-runtime-endpoint=/var/run/cri-dockerd.sock --hostname-override=minikube-m02 --image-service-endpoint=/var/run/cri-dockerd.sock --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.3 --runtime-request-timeout=15m

[Install]
 config:
{KubernetesVersion:v1.24.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:calico NodeIP: NodePort:8443 NodeName:}
I1029 07:45:12.121121    2106 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.24.3
I1029 07:45:12.133541    2106 binaries.go:44] Found k8s binaries, skipping transfer
I1029 07:45:12.133610    2106 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system
I1029 07:45:12.145297    2106 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (474 bytes)
I1029 07:45:12.163901    2106 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I1029 07:45:12.182030    2106 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I1029 07:45:12.191953    2106 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1029 07:45:12.205650    2106 host.go:66] Checking if "minikube" exists ...
I1029 07:45:12.205883    2106 config.go:180] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.24.3
I1029 07:45:12.205853    2106 start.go:285] JoinCluster: &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.33@sha256:73b259e144d926189cf169ae5b46bbec4e08e4e2f2bd87296054c3244f70feb8 Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.24.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:calico NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.24.3 ContainerRuntime:docker ControlPlane:true Worker:true} {Name:m02 IP:192.168.49.3 Port:0 KubernetesVersion:v1.24.3 ContainerRuntime:docker ControlPlane:false Worker:true}] Addons:map[ambassador:false auto-pause:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false istio:false istio-provisioner:false kong:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false volumesnapshots:false] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:true ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath:}
I1029 07:45:12.205939    2106 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.24.3:$PATH" kubeadm token create --print-join-command --ttl=0"
I1029 07:45:12.205976    2106 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1029 07:45:12.300151    2106 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49332 SSHKeyPath:/Users/apple/.minikube/machines/minikube/id_rsa Username:docker}
I1029 07:45:12.463927    2106 start.go:298] removing existing worker node "m02" before attempting to rejoin cluster: &{Name:m02 IP:192.168.49.3 Port:0 KubernetesVersion:v1.24.3 ContainerRuntime:docker ControlPlane:false Worker:true}
I1029 07:45:12.463949    2106 host.go:66] Checking if "minikube" exists ...
I1029 07:45:12.464232    2106 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.24.3/kubectl drain minikube-m02 --force --grace-period=1 --skip-wait-for-delete-timeout=1 --disable-eviction --ignore-daemonsets --delete-emptydir-data --delete-local-data
I1029 07:45:12.464267    2106 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1029 07:45:12.548144    2106 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49332 SSHKeyPath:/Users/apple/.minikube/machines/minikube/id_rsa Username:docker}
I1029 07:45:15.785589    2106 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.24.3/kubectl drain minikube-m02 --force --grace-period=1 --skip-wait-for-delete-timeout=1 --disable-eviction --ignore-daemonsets --delete-emptydir-data --delete-local-data: (3.324685154s)
I1029 07:45:15.785606    2106 node.go:109] successfully drained node "m02"
I1029 07:45:15.798736    2106 node.go:125] successfully deleted node "m02"
I1029 07:45:15.798747    2106 start.go:302] successfully removed existing worker node "m02" from cluster: &{Name:m02 IP:192.168.49.3 Port:0 KubernetesVersion:v1.24.3 ContainerRuntime:docker ControlPlane:false Worker:true}
I1029 07:45:15.799211    2106 start.go:306] trying to join worker node "m02" to cluster: &{Name:m02 IP:192.168.49.3 Port:0 KubernetesVersion:v1.24.3 ContainerRuntime:docker ControlPlane:false Worker:true}
I1029 07:45:15.799229    2106 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.24.3:$PATH" kubeadm join control-plane.minikube.internal:8443 --token 7idw01.dc13mo2w0gqj8g0x --discovery-token-ca-cert-hash sha256:07eb9d94883d2271b3660ea3df33045ef2e5d220272c932dd1ad2e55d22a04cd --ignore-preflight-errors=all --cri-socket /var/run/cri-dockerd.sock --node-name=minikube-m02"
I1029 07:45:16.952572    2106 ssh_runner.go:235] Completed: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.24.3:$PATH" kubeadm join control-plane.minikube.internal:8443 --token 7idw01.dc13mo2w0gqj8g0x --discovery-token-ca-cert-hash sha256:07eb9d94883d2271b3660ea3df33045ef2e5d220272c932dd1ad2e55d22a04cd --ignore-preflight-errors=all --cri-socket /var/run/cri-dockerd.sock --node-name=minikube-m02": (1.154333237s)
I1029 07:45:16.952584    2106 ssh_runner.go:195] Run: /bin/bash -c "sudo systemctl daemon-reload && sudo systemctl enable kubelet && sudo systemctl start kubelet"
I1029 07:45:17.125653    2106 start.go:287] JoinCluster complete in 4.924601597s
I1029 07:45:17.125669    2106 cni.go:95] Creating CNI manager for "calico"
I1029 07:45:17.125887    2106 cni.go:189] applying CNI manifest using /var/lib/minikube/binaries/v1.24.3/kubectl ...
I1029 07:45:17.125893    2106 ssh_runner.go:362] scp memory --> /var/tmp/minikube/cni.yaml (202050 bytes)
I1029 07:45:17.153425    2106 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.24.3/kubectl apply --kubeconfig=/var/lib/minikube/kubeconfig -f /var/tmp/minikube/cni.yaml
I1029 07:45:17.662557    2106 start.go:211] Will wait 6m0s for node &{Name:m02 IP:192.168.49.3 Port:0 KubernetesVersion:v1.24.3 ContainerRuntime:docker ControlPlane:false Worker:true}
I1029 07:45:17.683843    2106 out.go:177] üîé  Verifying Kubernetes components...
I1029 07:45:17.721164    2106 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service kubelet
I1029 07:45:17.745699    2106 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1029 07:45:17.844395    2106 kubeadm.go:572] duration metric: took 181.940257ms to wait for : map[apiserver:true system_pods:true] ...
I1029 07:45:17.844413    2106 node_conditions.go:102] verifying NodePressure condition ...
I1029 07:45:17.849407    2106 node_conditions.go:122] node storage ephemeral capacity is 61255492Ki
I1029 07:45:17.849417    2106 node_conditions.go:123] node cpu capacity is 4
I1029 07:45:17.849427    2106 node_conditions.go:122] node storage ephemeral capacity is 61255492Ki
I1029 07:45:17.849429    2106 node_conditions.go:123] node cpu capacity is 4
I1029 07:45:17.849432    2106 node_conditions.go:105] duration metric: took 5.020578ms to run NodePressure ...
I1029 07:45:17.849444    2106 start.go:216] waiting for startup goroutines ...
I1029 07:45:17.890196    2106 start.go:506] kubectl: 1.24.2, cluster: 1.24.3 (minor skew: 0)
I1029 07:45:17.908805    2106 out.go:177] üèÑ  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default

* 
* ==> Docker <==
* -- Logs begin at Tue 2022-11-01 14:14:22 UTC, end at Thu 2022-11-03 02:23:58 UTC. --
Nov 02 00:22:19 minikube dockerd[675]: time="2022-11-02T00:22:19.766895096Z" level=info msg="ignoring event" container=eb4972bccacbc4935bed7fc0ae9c3c8794b00db8cf9fa1fb6f4c38a77cbb06ed module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 02 00:24:38 minikube dockerd[675]: time="2022-11-02T00:24:38.698215190Z" level=info msg="ignoring event" container=f6e04bdf41d48855872fa982ed3994541c7893b3cf5d94e7d59503e49eeed401 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 02 00:27:37 minikube dockerd[675]: time="2022-11-02T00:27:37.338145741Z" level=error msg="stream copy error: reading from a closed fifo"
Nov 02 00:27:37 minikube dockerd[675]: time="2022-11-02T00:27:37.338060311Z" level=error msg="stream copy error: reading from a closed fifo"
Nov 02 00:27:37 minikube dockerd[675]: time="2022-11-02T00:27:37.425608908Z" level=error msg="Error running exec 594fbd71f007509a31ec9d5817132068f2cadf926de22f4864390df085783f0c in container: OCI runtime exec failed: exec failed: cannot exec in a stopped container: unknown"
Nov 02 00:27:37 minikube dockerd[675]: time="2022-11-02T00:27:37.747501289Z" level=error msg="stream copy error: reading from a closed fifo"
Nov 02 00:27:37 minikube dockerd[675]: time="2022-11-02T00:27:37.747733214Z" level=error msg="stream copy error: reading from a closed fifo"
Nov 02 00:27:37 minikube dockerd[675]: time="2022-11-02T00:27:37.748342514Z" level=error msg="Error running exec 2a56b9c22d44228bb55ee237dc3196a803a8cc3578ca51520c993dbec96839fb in container: cannot exec in a deleted state: unknown"
Nov 02 00:27:37 minikube dockerd[675]: time="2022-11-02T00:27:37.751587276Z" level=info msg="ignoring event" container=dc1b6c2f0d665a6b91186c326415a42c81d0cab2acccec33260457a1f38edfc5 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 02 00:29:27 minikube dockerd[675]: time="2022-11-02T00:29:27.700654760Z" level=info msg="ignoring event" container=5fa36649b12b7c432a7bf2fe98db48e8c3e1f30d099c3f0a1b6c1308e2b5e541 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 02 00:34:06 minikube dockerd[675]: time="2022-11-02T00:34:06.871265653Z" level=info msg="ignoring event" container=11906ad3da123d7ca9dd27fa46cac4c05dfc91df979065b1e0ab36cc4a39fa61 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 02 00:34:52 minikube dockerd[675]: time="2022-11-02T00:34:52.137262270Z" level=info msg="ignoring event" container=1fa53bbc31109c17cf7c40f40c05e8990aa048f11a23ee206742034dc20d58bc module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 02 00:49:07 minikube dockerd[675]: time="2022-11-02T00:49:07.099550540Z" level=error msg="Handler for GET /v1.40/containers/a9f0682bd7c4034d81245137794fde5a285af595696db1551d0e4993246cf46f/json returned error: write unix /var/run/docker.sock->@: write: broken pipe"
Nov 02 00:49:07 minikube dockerd[675]: http: superfluous response.WriteHeader call from github.com/docker/docker/api/server/httputils.WriteJSON (httputils_write_json.go:11)
Nov 02 00:49:07 minikube dockerd[675]: time="2022-11-02T00:49:07.165792151Z" level=error msg="Handler for GET /v1.40/containers/ee323878966b02b524c1bd047639463352313721e2c45ab8d1a75953434f06e0/json returned error: write unix /var/run/docker.sock->@: write: broken pipe"
Nov 02 00:49:07 minikube dockerd[675]: http: superfluous response.WriteHeader call from github.com/docker/docker/api/server/httputils.WriteJSON (httputils_write_json.go:11)
Nov 02 01:06:28 minikube dockerd[675]: time="2022-11-02T01:06:28.697800834Z" level=info msg="ignoring event" container=c8450f5d01ef4263093a4511f040d5caed5fd0c5dc3515723f01eca954f7e16f module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 02 01:07:08 minikube dockerd[675]: time="2022-11-02T01:07:08.651857679Z" level=info msg="ignoring event" container=7c1b7b4a00b21ab45b8c6ec8d476aa84e8638c5dbf5b221be45828d13bac8126 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 02 01:08:42 minikube dockerd[675]: time="2022-11-02T01:08:42.498872462Z" level=info msg="ignoring event" container=04a05cc1c845d329ee874566593c18cf2a2f96b3be529c32b9d169fef8bfecf3 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 02 01:08:42 minikube dockerd[675]: time="2022-11-02T01:08:42.798306823Z" level=error msg="stream copy error: reading from a closed fifo"
Nov 02 01:08:42 minikube dockerd[675]: time="2022-11-02T01:08:42.798339222Z" level=error msg="stream copy error: reading from a closed fifo"
Nov 02 01:08:42 minikube dockerd[675]: time="2022-11-02T01:08:42.833682239Z" level=error msg="Error running exec 8dcad1a88695c002015a5850a5cc7cc9c3e1629c8476c6f021f02b58f1e55980 in container: OCI runtime exec failed: exec failed: unable to start container process: read init-p: connection reset by peer: unknown"
Nov 02 01:08:42 minikube dockerd[675]: time="2022-11-02T01:08:42.861121665Z" level=info msg="ignoring event" container=a9f0682bd7c4034d81245137794fde5a285af595696db1551d0e4993246cf46f module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 02 01:11:20 minikube dockerd[675]: time="2022-11-02T01:11:20.105430965Z" level=error msg="stream copy error: reading from a closed fifo"
Nov 02 01:11:20 minikube dockerd[675]: time="2022-11-02T01:11:20.106714877Z" level=error msg="stream copy error: reading from a closed fifo"
Nov 02 01:11:20 minikube dockerd[675]: time="2022-11-02T01:11:20.107115350Z" level=error msg="Error running exec 813045f3fbea8fa22d09fd7049ab0fee66b733742b43ac6a9c18f572fc837271 in container: cannot exec in a stopped state: unknown"
Nov 02 01:11:20 minikube dockerd[675]: time="2022-11-02T01:11:20.175764954Z" level=info msg="ignoring event" container=765ba7074cc7a234e240c98a421cbcd77db1efeaafe386973f66d9a3abb46a41 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 02 01:11:52 minikube dockerd[675]: time="2022-11-02T01:11:52.251457075Z" level=info msg="ignoring event" container=4163789da29be9ff1cf6aeda03aff029fc47872db1c3c2ad8782256d67cb365a module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 02 01:14:22 minikube dockerd[675]: time="2022-11-02T01:14:22.205350502Z" level=error msg="stream copy error: reading from a closed fifo"
Nov 02 01:14:22 minikube dockerd[675]: time="2022-11-02T01:14:22.205440648Z" level=error msg="stream copy error: reading from a closed fifo"
Nov 02 01:14:22 minikube dockerd[675]: time="2022-11-02T01:14:22.205568326Z" level=error msg="Error running exec 3c56b8169e56606ab7b0bd14ffbf1e97aa8a98f576b581ce73958e7f7bcc72b7 in container: cannot exec in a stopped state: unknown"
Nov 02 01:14:22 minikube dockerd[675]: time="2022-11-02T01:14:22.224465124Z" level=info msg="ignoring event" container=f67f32eef1a7fca1c1e530f2247d988421e81885967591011414536eae662683 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 02 01:14:22 minikube dockerd[675]: time="2022-11-02T01:14:22.267111767Z" level=error msg="Error setting up exec command in container f67f32eef1a7fca1c1e530f2247d988421e81885967591011414536eae662683: Container f67f32eef1a7fca1c1e530f2247d988421e81885967591011414536eae662683 is not running"
Nov 02 01:40:48 minikube dockerd[675]: time="2022-11-02T01:40:48.553695937Z" level=error msg="collecting stats for b68b77200ba1ee96d2dac62cd65ae436d52bd9c0180a1043556fd1e7155678cb: no metrics received"
Nov 02 01:40:48 minikube dockerd[675]: time="2022-11-02T01:40:48.777962355Z" level=info msg="ignoring event" container=b68b77200ba1ee96d2dac62cd65ae436d52bd9c0180a1043556fd1e7155678cb module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 02 06:03:32 minikube dockerd[675]: time="2022-11-02T06:03:32.686965410Z" level=error msg="Handler for GET /v1.40/containers/ee323878966b02b524c1bd047639463352313721e2c45ab8d1a75953434f06e0/json returned error: write unix /var/run/docker.sock->@: write: broken pipe"
Nov 02 06:03:32 minikube dockerd[675]: http: superfluous response.WriteHeader call from github.com/docker/docker/api/server/httputils.WriteJSON (httputils_write_json.go:11)
Nov 02 06:03:32 minikube dockerd[675]: time="2022-11-02T06:03:32.691548651Z" level=error msg="Handler for GET /v1.40/containers/ee323878966b02b524c1bd047639463352313721e2c45ab8d1a75953434f06e0/json returned error: write unix /var/run/docker.sock->@: write: broken pipe"
Nov 02 06:03:32 minikube dockerd[675]: http: superfluous response.WriteHeader call from github.com/docker/docker/api/server/httputils.WriteJSON (httputils_write_json.go:11)
Nov 02 06:03:32 minikube dockerd[675]: time="2022-11-02T06:03:32.792079185Z" level=error msg="Handler for GET /v1.40/containers/36f6508bca889baddf22e0421a6a6457d6a1ea87c9b616c5d62b682452246851/json returned error: write unix /var/run/docker.sock->@: write: broken pipe"
Nov 02 06:03:32 minikube dockerd[675]: http: superfluous response.WriteHeader call from github.com/docker/docker/api/server/httputils.WriteJSON (httputils_write_json.go:11)
Nov 02 06:03:32 minikube dockerd[675]: time="2022-11-02T06:03:32.792368178Z" level=error msg="Handler for GET /v1.40/containers/36f6508bca889baddf22e0421a6a6457d6a1ea87c9b616c5d62b682452246851/json returned error: write unix /var/run/docker.sock->@: write: broken pipe"
Nov 02 06:03:32 minikube dockerd[675]: http: superfluous response.WriteHeader call from github.com/docker/docker/api/server/httputils.WriteJSON (httputils_write_json.go:11)
Nov 02 06:03:32 minikube dockerd[675]: time="2022-11-02T06:03:32.792861454Z" level=error msg="Handler for GET /v1.40/containers/36f6508bca889baddf22e0421a6a6457d6a1ea87c9b616c5d62b682452246851/json returned error: write unix /var/run/docker.sock->@: write: broken pipe"
Nov 02 06:03:32 minikube dockerd[675]: http: superfluous response.WriteHeader call from github.com/docker/docker/api/server/httputils.WriteJSON (httputils_write_json.go:11)
Nov 02 06:04:07 minikube dockerd[675]: time="2022-11-02T06:04:07.183191821Z" level=error msg="stream copy error: reading from a closed fifo"
Nov 02 06:04:07 minikube dockerd[675]: time="2022-11-02T06:04:07.183910022Z" level=error msg="stream copy error: reading from a closed fifo"
Nov 02 06:04:07 minikube dockerd[675]: time="2022-11-02T06:04:07.192745148Z" level=error msg="Error running exec 8356f6f297dcd103129ca7055f21ccac56bd868aa0007da3dc07ef6c4a23a907 in container: OCI runtime exec failed: exec failed: cannot exec in a stopped container: unknown"
Nov 02 06:04:07 minikube dockerd[675]: time="2022-11-02T06:04:07.362926744Z" level=info msg="ignoring event" container=36f6508bca889baddf22e0421a6a6457d6a1ea87c9b616c5d62b682452246851 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 02 06:04:23 minikube dockerd[675]: time="2022-11-02T06:04:23.810149993Z" level=info msg="ignoring event" container=be2dc8db51cf1dd1d597d4fda54b5186b12afb570349ac741bc995f50351b219 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 02 06:04:55 minikube dockerd[675]: time="2022-11-02T06:04:55.397585610Z" level=error msg="stream copy error: reading from a closed fifo"
Nov 02 06:04:55 minikube dockerd[675]: time="2022-11-02T06:04:55.397593515Z" level=error msg="stream copy error: reading from a closed fifo"
Nov 02 06:04:55 minikube dockerd[675]: time="2022-11-02T06:04:55.399751032Z" level=error msg="Error running exec 253e2cd2e78f91d44d264b82c915a8c620a3e8ea164e0f4166fae7822ea503af in container: OCI runtime exec failed: exec failed: unable to create new parent process: namespace path: lstat /proc/4168464/ns/ipc: no such file or directory: unknown"
Nov 02 06:04:55 minikube dockerd[675]: time="2022-11-02T06:04:55.515706888Z" level=info msg="ignoring event" container=ebcb7c85f8ecc2e537d463171689ed71f2fadb9dbd8f5031d1bbd9e6669e7436 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 02 06:05:31 minikube dockerd[675]: time="2022-11-02T06:05:31.884800884Z" level=info msg="ignoring event" container=f19c05815228d5c3fbe754e58ff408010bb814c253b5499076fc41371e03ef88 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 02 06:06:35 minikube dockerd[675]: time="2022-11-02T06:06:35.856523807Z" level=info msg="ignoring event" container=e4dceea6981031f9270dfb44753bf83a9616030725be17f380d056898ca14e54 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 03 00:40:47 minikube dockerd[675]: time="2022-11-03T00:40:47.712500881Z" level=info msg="ignoring event" container=bbf7ccdfe0751a30564b8a72fcff7ba3cd944aacfb913589265df05e38cc33f0 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 03 02:21:35 minikube dockerd[675]: time="2022-11-03T02:21:35.669659348Z" level=warning msg="reference for unknown type: " digest="sha256:5516d103a9c2ecc4f026efbd4b40662ce22dc1f824fb129ed121460aaa5c47f8" remote="k8s.gcr.io/ingress-nginx/controller@sha256:5516d103a9c2ecc4f026efbd4b40662ce22dc1f824fb129ed121460aaa5c47f8"
Nov 03 02:21:45 minikube dockerd[675]: time="2022-11-03T02:21:45.075600683Z" level=info msg="ignoring event" container=57948df216455e7c1b264a84b9c27299e1158813ce8c8638f7c0635af14b7325 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 03 02:21:46 minikube dockerd[675]: time="2022-11-03T02:21:46.192582938Z" level=info msg="ignoring event" container=2c9a28a27831e75b4a053058001dd73692c74d3f778c7736b22659e3fbe43133 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"

* 
* ==> container status <==
* CONTAINER           IMAGE                                                                                                         CREATED             STATE               NAME                      ATTEMPT             POD ID
8c8d2878a0e36       k8s.gcr.io/ingress-nginx/controller@sha256:5516d103a9c2ecc4f026efbd4b40662ce22dc1f824fb129ed121460aaa5c47f8   2 minutes ago       Running             controller                0                   e949a2122e351
a67187a329e10       6e38f40d628db                                                                                                 2 hours ago         Running             storage-provisioner       36                  48595375df9f3
b73bec9ee264f       76ba70f4748f9                                                                                                 20 hours ago        Running             calico-kube-controllers   25                  ae3028160dd35
e4dceea698103       76ba70f4748f9                                                                                                 20 hours ago        Exited              calico-kube-controllers   24                  ae3028160dd35
bbf7ccdfe0751       6e38f40d628db                                                                                                 25 hours ago        Exited              storage-provisioner       35                  48595375df9f3
74159c15e37d5       nginx@sha256:943c25b4b66b332184d5ba6bb18234273551593016c0e0ae906bab111548239f                                 4 days ago          Running             nginx                     0                   a66028d5d9cf1
7f6a5955f1691       a4ca41631cc7a                                                                                                 5 days ago          Running             coredns                   1                   d15b93f5dc686
ee323878966b0       5ef66b403f4f0                                                                                                 5 days ago          Running             calico-node               1                   01806600e2f03
6aa4de0485e10       5991877ebc118                                                                                                 5 days ago          Exited              flexvol-driver            0                   01806600e2f03
6dcfe32b19799       4945b742b8e66                                                                                                 5 days ago          Exited              install-cni               0                   01806600e2f03
85b859ad60417       4945b742b8e66                                                                                                 5 days ago          Exited              upgrade-ipam              1                   01806600e2f03
4a37bf14a0752       2ae1ba6417cbc                                                                                                 5 days ago          Running             kube-proxy                1                   a9cc8892f4b51
f3c92cc44756c       3a5aa3a515f5d                                                                                                 5 days ago          Running             kube-scheduler            1                   6f7b845686d7e
600b8a6250d84       aebe758cef4cd                                                                                                 5 days ago          Running             etcd                      1                   33c250c219950
127a41c5cb7de       586c112956dfc                                                                                                 5 days ago          Running             kube-controller-manager   1                   2f7599bfa8132
2881ee0022547       d521dd763e2e3                                                                                                 5 days ago          Running             kube-apiserver            1                   5bfbe481e4cef
0183947183c31       a4ca41631cc7a                                                                                                 5 days ago          Exited              coredns                   0                   17ab07340cfca
81307dbb1e6a8       calico/node@sha256:7f9aa7e31fbcea7be64b153f8bcfd494de023679ec10d851a05667f0adb42650                           5 days ago          Exited              calico-node               0                   7bc96f7dd37f7
f52bcc2527f6a       2ae1ba6417cbc                                                                                                 5 days ago          Exited              kube-proxy                0                   e53ba22093f2e
80f3da2d5c1c1       aebe758cef4cd                                                                                                 5 days ago          Exited              etcd                      0                   9b0af2aac98bf
d8c20ccfa2749       586c112956dfc                                                                                                 5 days ago          Exited              kube-controller-manager   0                   02c1f8e3b73f3
fa7ea1a28f187       3a5aa3a515f5d                                                                                                 5 days ago          Exited              kube-scheduler            0                   41de401be4e43
66c1113227806       d521dd763e2e3                                                                                                 5 days ago          Exited              kube-apiserver            0                   266af977f8a7c

* 
* ==> coredns [0183947183c3] <==
* .:53
[INFO] plugin/reload: Running configuration MD5 = c23ed519c17e71ee396ed052e6209e94
CoreDNS-1.8.6
linux/amd64, go1.17.1, 13a9191
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s

* 
* ==> coredns [7f6a5955f169] <==
* [WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 2.109154842s
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.593143819s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.396076571s
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.366429694s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.127732602s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.013645344s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.101031885s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.325033165s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.177174268s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 2.300607554s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 3.313986909s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 2.547449639s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.004669605s
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.115608902s
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 2.901907989s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.178594024s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.224295289s
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": dial tcp :8080: i/o timeout (Client.Timeout exceeded while awaiting headers)
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 2.165438255s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.91024131s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.339622403s
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 2.292989005s
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.907222349s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.761055723s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.929855713s
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.53250726s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.290924296s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.726175345s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.668922395s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.293763877s
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.087489955s
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 2.579566022s
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 2.058188261s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 24.31799536s
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.01361321s
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 2.710786252s
W1102 00:27:25.090120       1 reflector.go:441] pkg/mod/k8s.io/client-go@v0.22.2/tools/cache/reflector.go:167: watch of *v1.Service ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W1102 00:27:25.189831       1 reflector.go:441] pkg/mod/k8s.io/client-go@v0.22.2/tools/cache/reflector.go:167: watch of *v1.EndpointSlice ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W1102 00:27:25.071282       1 reflector.go:441] pkg/mod/k8s.io/client-go@v0.22.2/tools/cache/reflector.go:167: watch of *v1.Namespace ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W1102 01:38:08.812332       1 reflector.go:441] pkg/mod/k8s.io/client-go@v0.22.2/tools/cache/reflector.go:167: watch of *v1.Namespace ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W1102 01:38:08.813177       1 reflector.go:441] pkg/mod/k8s.io/client-go@v0.22.2/tools/cache/reflector.go:167: watch of *v1.EndpointSlice ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W1102 01:38:08.812386       1 reflector.go:441] pkg/mod/k8s.io/client-go@v0.22.2/tools/cache/reflector.go:167: watch of *v1.Service ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding

* 
* ==> describe nodes <==
* Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=62e108c3dfdec8029a890ad6d8ef96b6461426dc
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2022_10_28T14_26_45_0700
                    minikube.k8s.io/version=v1.26.1
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    projectcalico.org/IPv4Address: 192.168.49.2/24
                    projectcalico.org/IPv4IPIPTunnelAddr: 10.244.120.64
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Fri, 28 Oct 2022 06:26:39 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Thu, 03 Nov 2022 02:23:53 +0000
Conditions:
  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----                 ------  -----------------                 ------------------                ------                       -------
  NetworkUnavailable   False   Fri, 28 Oct 2022 23:45:04 +0000   Fri, 28 Oct 2022 23:45:04 +0000   CalicoIsUp                   Calico is running on this node
  MemoryPressure       False   Thu, 03 Nov 2022 02:22:07 +0000   Wed, 02 Nov 2022 01:23:04 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure         False   Thu, 03 Nov 2022 02:22:07 +0000   Wed, 02 Nov 2022 01:23:04 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure          False   Thu, 03 Nov 2022 02:22:07 +0000   Wed, 02 Nov 2022 01:23:04 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready                True    Thu, 03 Nov 2022 02:22:07 +0000   Wed, 02 Nov 2022 01:23:04 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                4
  ephemeral-storage:  61255492Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             8048952Ki
  pods:               110
Allocatable:
  cpu:                4
  ephemeral-storage:  61255492Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             8048952Ki
  pods:               110
System Info:
  Machine ID:                 4c192b04687c403f8fbb9bc7975b21b3
  System UUID:                c6d8dbf4-1c22-4006-8254-08257ef94fb6
  Boot ID:                    5a3c2f72-a54c-48ad-951b-d3bb1182bd7d
  Kernel Version:             5.10.104-linuxkit
  OS Image:                   Ubuntu 20.04.4 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://20.10.17
  Kubelet Version:            v1.24.3
  Kube-Proxy Version:         v1.24.3
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (11 in total)
  Namespace                   Name                                         CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                         ------------  ----------  ---------------  -------------  ---
  default                     nginx-8f458dc5b-pxpjt                        0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         4d13h
  ingress-nginx               ingress-nginx-controller-755dfbfc65-gw44w    100m (2%!)(MISSING)     0 (0%!)(MISSING)      90Mi (1%!)(MISSING)        0 (0%!)(MISSING)         2m30s
  kube-system                 calico-kube-controllers-c44b4545-28m7f       0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         5d19h
  kube-system                 calico-node-vw5c8                            250m (6%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         5d19h
  kube-system                 coredns-6d4b75cb6d-nr4nw                     100m (2%!)(MISSING)     0 (0%!)(MISSING)      70Mi (0%!)(MISSING)        170Mi (2%!)(MISSING)     5d19h
  kube-system                 etcd-minikube                                100m (2%!)(MISSING)     0 (0%!)(MISSING)      100Mi (1%!)(MISSING)       0 (0%!)(MISSING)         5d19h
  kube-system                 kube-apiserver-minikube                      250m (6%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         5d19h
  kube-system                 kube-controller-manager-minikube             200m (5%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         5d19h
  kube-system                 kube-proxy-6s9dq                             0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         5d19h
  kube-system                 kube-scheduler-minikube                      100m (2%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         5d19h
  kube-system                 storage-provisioner                          0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         5d19h
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests     Limits
  --------           --------     ------
  cpu                1100m (27%!)(MISSING)  0 (0%!)(MISSING)
  memory             260Mi (3%!)(MISSING)   170Mi (2%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)       0 (0%!)(MISSING)
  hugepages-1Gi      0 (0%!)(MISSING)       0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)       0 (0%!)(MISSING)
Events:              <none>


Name:               minikube-m02
Roles:              <none>
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube-m02
                    kubernetes.io/os=linux
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: /var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    projectcalico.org/IPv4Address: 192.168.49.3/24
                    projectcalico.org/IPv4IPIPTunnelAddr: 10.244.205.240
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Fri, 28 Oct 2022 23:45:16 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube-m02
  AcquireTime:     <unset>
  RenewTime:       Thu, 03 Nov 2022 02:23:58 +0000
Conditions:
  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----                 ------  -----------------                 ------------------                ------                       -------
  NetworkUnavailable   False   Fri, 28 Oct 2022 23:45:24 +0000   Fri, 28 Oct 2022 23:45:24 +0000   CalicoIsUp                   Calico is running on this node
  MemoryPressure       False   Thu, 03 Nov 2022 02:22:39 +0000   Wed, 02 Nov 2022 06:06:43 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure         False   Thu, 03 Nov 2022 02:22:39 +0000   Wed, 02 Nov 2022 06:06:43 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure          False   Thu, 03 Nov 2022 02:22:39 +0000   Wed, 02 Nov 2022 06:06:43 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready                True    Thu, 03 Nov 2022 02:22:39 +0000   Wed, 02 Nov 2022 06:06:43 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.3
  Hostname:    minikube-m02
Capacity:
  cpu:                4
  ephemeral-storage:  61255492Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             8048952Ki
  pods:               110
Allocatable:
  cpu:                4
  ephemeral-storage:  61255492Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             8048952Ki
  pods:               110
System Info:
  Machine ID:                 4c192b04687c403f8fbb9bc7975b21b3
  System UUID:                49a6de18-2930-4a23-a3ea-079b5479ce24
  Boot ID:                    5a3c2f72-a54c-48ad-951b-d3bb1182bd7d
  Kernel Version:             5.10.104-linuxkit
  OS Image:                   Ubuntu 20.04.4 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://20.10.17
  Kubelet Version:            v1.24.3
  Kube-Proxy Version:         v1.24.3
PodCIDR:                      10.244.1.0/24
PodCIDRs:                     10.244.1.0/24
Non-terminated Pods:          (4 in total)
  Namespace                   Name                      CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                      ------------  ----------  ---------------  -------------  ---
  default                     hello-685bd55bcd-nqlm4    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         85m
  kube-system                 calico-node-pmpd5         250m (6%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         5d19h
  kube-system                 kube-proxy-2fz2h          0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         5d19h
  ns-a                        nginx                     0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         3h23m
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests   Limits
  --------           --------   ------
  cpu                250m (6%!)(MISSING)  0 (0%!)(MISSING)
  memory             0 (0%!)(MISSING)     0 (0%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)     0 (0%!)(MISSING)
  hugepages-1Gi      0 (0%!)(MISSING)     0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)     0 (0%!)(MISSING)
Events:              <none>

* 
* ==> dmesg <==
* [  +0.000001]  curve25519_x86_64 libcurve25519_generic
[  +0.000013] Code: 48 83 c4 28 c3 cc cc cc cc cc cc cc cc cc cc cc cc cc cc cc cc cc cc cc cc cc cc cc cc cc cc cc cc cc cc b8 18 00 00 00 0f 05 <c3> cc cc cc cc cc cc cc cc cc cc cc cc cc cc cc cc cc cc cc cc cc
[  +0.000000]  libchacha20poly1305
[  +0.000002] RSP: 002b:000000c00006fe90 EFLAGS: 00000202
[  +0.000000]  libblake2s
[  +0.000001]  ORIG_RAX: 0000000000000018
[  +0.000001]  blake2s_x86_64
[  +0.000001] RAX: ffffffffffffffda RBX: 0000000000000004 RCX: 000000000046aaa7
[  +0.000001]  libblake2s_generic
[  +0.000001] RDX: 0000000000000001 RSI: 0000000000000004 RDI: 0000000000000004
[  +0.000000]  xfrm_user
[  +0.000001] RBP: 000000c00006fed0 R08: 0000000000000002 R09: 0000000000000000
[  +0.000001]  xfrm_algo
[  +0.000001] R10: 0000000000148eae R11: 0000000000000202 R12: 000000c00006ff18
[  +0.000001]  grpcfuse(O)
[  +0.000000] R13: 000000c000060000 R14: 000000c0000024e0 R15: 00007f80d88eb134
[  +0.000001]  vmw_vsock_virtio_transport vmw_vsock_virtio_transport_common vsock
[  +0.000006] CPU: 0 PID: 7797 Comm: dockerd Tainted: G           O L  T 5.10.104-linuxkit #1
[  +0.000001] Hardware name:  BHYVE, BIOS 1.00 03/14/2014
[  +0.000003] RIP: 0033:0x55d46c3709a3
[  +0.000003] Code: 44 24 78 48 8b 4c 24 30 48 8b 74 24 28 4c 8b 44 24 38 4c 8b 8c 24 98 00 00 00 4c 8b 54 24 60 eb 26 4c 89 c8 48 89 d3 4c 89 c1 <bf> 01 00 00 00 48 8b ac 24 88 00 00 00 48 81 c4 90 00 00 00 c3 4c
[  +0.000001] RSP: 002b:000000c000f1a978 EFLAGS: 00000246
[  +0.000001] RAX: 000000c0015aa537 RBX: 0000000000000007 RCX: 000000000000024f
[  +0.000000] RDX: 0000000000000007 RSI: 0000000000000007 RDI: 000000000000013f
[  +0.000001] RBP: 000000c000f1aa00 R08: 000000000000024f R09: 000000c0015aa537
[  +0.000001] R10: 0000000000000000 R11: 0000000000000000 R12: 000000c000f1a9f8
[  +0.000001] R13: 0000000000000001 R14: 000000c0019229c0 R15: 0000000000000021
[  +0.000001] FS:  00007f000ffff700 GS:  0000000000000000
[  +0.094747] Hangcheck: hangcheck value past margin!
[Nov 2 02:35] INFO: task coredns:9857 blocked for more than 123 seconds.
[  +0.004640]       Tainted: G           O L  T 5.10.104-linuxkit #1
[  +0.004850] "echo 0 > /proc/sys/kernel/hung_task_timeout_secs" disables this message.
[Nov 2 02:37] INFO: task coredns:9857 blocked for more than 245 seconds.
[  +0.002347]       Tainted: G           O L  T 5.10.104-linuxkit #1
[  +0.001997] "echo 0 > /proc/sys/kernel/hung_task_timeout_secs" disables this message.
[Nov 2 02:39] INFO: task coredns:9857 blocked for more than 381 seconds.
[  +0.012025]       Tainted: G           O L  T 5.10.104-linuxkit #1
[  +0.005902] "echo 0 > /proc/sys/kernel/hung_task_timeout_secs" disables this message.
[Nov 2 04:12] Hangcheck: hangcheck value past margin!
[  +0.001811] BUG: workqueue lockup - pool
[  +0.000514] INFO: task coredns:9857 blocked for more than 5943 seconds.
[  +0.000006]       Tainted: G           O L  T 5.10.104-linuxkit #1
[  +0.000002] "echo 0 > /proc/sys/kernel/hung_task_timeout_secs" disables this message.
[  +0.037026]  cpus=0 node=0 flags=0x0 nice=0 stuck for 4398s!
[Nov 2 04:14] INFO: task coredns:9857 blocked for more than 6066 seconds.
[  +0.003418]       Tainted: G           O L  T 5.10.104-linuxkit #1
[  +0.002777] "echo 0 > /proc/sys/kernel/hung_task_timeout_secs" disables this message.
[Nov 2 04:16] INFO: task coredns:9857 blocked for more than 6189 seconds.
[  +0.006302]       Tainted: G           O L  T 5.10.104-linuxkit #1
[  +0.004427] "echo 0 > /proc/sys/kernel/hung_task_timeout_secs" disables this message.
[Nov 2 04:17] Hangcheck: hangcheck value past margin!
[Nov 2 04:18] INFO: task coredns:9857 blocked for more than 6312 seconds.
[  +0.005569]       Tainted: G           O L  T 5.10.104-linuxkit #1
[  +0.003584] "echo 0 > /proc/sys/kernel/hung_task_timeout_secs" disables this message.
[Nov 2 04:21] INFO: task coredns:9857 blocked for more than 6495 seconds.
[  +0.003172]       Tainted: G           O L  T 5.10.104-linuxkit #1
[  +0.002376] "echo 0 > /proc/sys/kernel/hung_task_timeout_secs" disables this message.
[Nov 2 04:29] Hangcheck: hangcheck value past margin!
[Nov 2 05:04] Hangcheck: hangcheck value past margin!
[Nov 2 05:29] Hangcheck: hangcheck value past margin!

* 
* ==> etcd [600b8a6250d8] <==
* {"level":"warn","ts":"2022-11-03T00:40:46.015Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"191.436379ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2022-11-03T00:40:46.016Z","caller":"traceutil/trace.go:171","msg":"trace[208623034] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:401352; }","duration":"191.576684ms","start":"2022-11-03T00:40:45.824Z","end":"2022-11-03T00:40:46.015Z","steps":["trace[208623034] 'agreement among raft nodes before linearized reading'  (duration: 191.420622ms)"],"step_count":1}
{"level":"info","ts":"2022-11-03T00:42:43.959Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":401167}
{"level":"info","ts":"2022-11-03T00:42:43.964Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":401167,"took":"2.939184ms"}
{"level":"info","ts":"2022-11-03T00:47:43.753Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":401478}
{"level":"info","ts":"2022-11-03T00:47:43.766Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":401478,"took":"12.79949ms"}
{"level":"info","ts":"2022-11-03T00:52:43.555Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":401799}
{"level":"info","ts":"2022-11-03T00:52:43.559Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":401799,"took":"2.247766ms"}
{"level":"info","ts":"2022-11-03T00:54:07.932Z","caller":"traceutil/trace.go:171","msg":"trace[882821287] linearizableReadLoop","detail":"{readStateIndex:479225; appliedIndex:479225; }","duration":"100.598109ms","start":"2022-11-03T00:54:07.832Z","end":"2022-11-03T00:54:07.932Z","steps":["trace[882821287] 'read index received'  (duration: 100.591556ms)","trace[882821287] 'applied index is now lower than readState.Index'  (duration: 5.595¬µs)"],"step_count":2}
{"level":"warn","ts":"2022-11-03T00:54:07.935Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"103.271912ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/namespaces/default\" ","response":"range_response_count:1 size:344"}
{"level":"info","ts":"2022-11-03T00:54:07.935Z","caller":"traceutil/trace.go:171","msg":"trace[1076958610] range","detail":"{range_begin:/registry/namespaces/default; range_end:; response_count:1; response_revision:402224; }","duration":"103.446849ms","start":"2022-11-03T00:54:07.831Z","end":"2022-11-03T00:54:07.935Z","steps":["trace[1076958610] 'agreement among raft nodes before linearized reading'  (duration: 100.754228ms)"],"step_count":1}
{"level":"info","ts":"2022-11-03T00:57:43.318Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":402119}
{"level":"info","ts":"2022-11-03T00:57:43.320Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":402119,"took":"958.943¬µs"}
{"level":"info","ts":"2022-11-03T01:02:43.113Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":402470}
{"level":"info","ts":"2022-11-03T01:02:43.115Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":402470,"took":"676.065¬µs"}
{"level":"info","ts":"2022-11-03T01:03:52.427Z","caller":"etcdserver/server.go:1383","msg":"triggering snapshot","local-member-id":"aec36adc501070cc","local-member-applied-index":480048,"local-member-snapshot-index":470047,"local-member-snapshot-count":10000}
{"level":"info","ts":"2022-11-03T01:03:52.438Z","caller":"etcdserver/server.go:2394","msg":"saved snapshot","snapshot-index":480048}
{"level":"info","ts":"2022-11-03T01:03:52.438Z","caller":"etcdserver/server.go:2424","msg":"compacted Raft logs","compact-index":475048}
{"level":"info","ts":"2022-11-03T01:04:13.540Z","caller":"fileutil/purge.go:77","msg":"purged","path":"/var/lib/minikube/etcd/member/snap/0000000000000003-0000000000068fdb.snap"}
{"level":"info","ts":"2022-11-03T01:07:42.912Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":402851}
{"level":"info","ts":"2022-11-03T01:07:42.915Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":402851,"took":"1.500531ms"}
{"level":"info","ts":"2022-11-03T01:12:42.692Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":403211}
{"level":"info","ts":"2022-11-03T01:12:42.694Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":403211,"took":"1.093957ms"}
{"level":"info","ts":"2022-11-03T01:17:42.495Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":403563}
{"level":"info","ts":"2022-11-03T01:17:42.498Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":403563,"took":"2.038165ms"}
{"level":"info","ts":"2022-11-03T01:22:42.296Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":403893}
{"level":"info","ts":"2022-11-03T01:22:42.297Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":403893,"took":"689.907¬µs"}
{"level":"info","ts":"2022-11-03T01:27:42.054Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":404233}
{"level":"info","ts":"2022-11-03T01:27:42.055Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":404233,"took":"764.823¬µs"}
{"level":"info","ts":"2022-11-03T01:32:41.853Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":404572}
{"level":"info","ts":"2022-11-03T01:32:41.855Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":404572,"took":"1.290214ms"}
{"level":"info","ts":"2022-11-03T01:37:41.653Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":404892}
{"level":"info","ts":"2022-11-03T01:37:41.657Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":404892,"took":"2.941371ms"}
{"level":"info","ts":"2022-11-03T01:42:41.426Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":405212}
{"level":"info","ts":"2022-11-03T01:42:41.428Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":405212,"took":"1.0065ms"}
{"level":"info","ts":"2022-11-03T01:47:41.223Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":405534}
{"level":"info","ts":"2022-11-03T01:47:41.228Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":405534,"took":"3.881277ms"}
{"level":"info","ts":"2022-11-03T01:52:41.019Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":405850}
{"level":"info","ts":"2022-11-03T01:52:41.020Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":405850,"took":"901.417¬µs"}
{"level":"info","ts":"2022-11-03T01:57:40.792Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":406170}
{"level":"info","ts":"2022-11-03T01:57:40.794Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":406170,"took":"1.128874ms"}
{"level":"info","ts":"2022-11-03T02:02:40.592Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":406493}
{"level":"info","ts":"2022-11-03T02:02:40.593Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":406493,"took":"1.113753ms"}
{"level":"info","ts":"2022-11-03T02:07:40.392Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":406816}
{"level":"info","ts":"2022-11-03T02:07:40.394Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":406816,"took":"512.229¬µs"}
{"level":"info","ts":"2022-11-03T02:12:40.141Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":407142}
{"level":"info","ts":"2022-11-03T02:12:40.144Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":407142,"took":"2.569382ms"}
{"level":"info","ts":"2022-11-03T02:17:39.937Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":407475}
{"level":"info","ts":"2022-11-03T02:17:39.938Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":407475,"took":"552.297¬µs"}
{"level":"info","ts":"2022-11-03T02:21:31.366Z","caller":"traceutil/trace.go:171","msg":"trace[970160894] transaction","detail":"{read_only:false; response_revision:408095; number_of_response:1; }","duration":"103.2222ms","start":"2022-11-03T02:21:31.252Z","end":"2022-11-03T02:21:31.355Z","steps":["trace[970160894] 'compare'  (duration: 92.158611ms)"],"step_count":1}
{"level":"warn","ts":"2022-11-03T02:21:31.374Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"100.644355ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/pods/ingress-nginx/ingress-nginx-controller-755dfbfc65-gw44w\" ","response":"range_response_count:1 size:4682"}
{"level":"info","ts":"2022-11-03T02:21:31.375Z","caller":"traceutil/trace.go:171","msg":"trace[487503548] range","detail":"{range_begin:/registry/pods/ingress-nginx/ingress-nginx-controller-755dfbfc65-gw44w; range_end:; response_count:1; response_revision:408095; }","duration":"101.309422ms","start":"2022-11-03T02:21:31.274Z","end":"2022-11-03T02:21:31.375Z","steps":["trace[487503548] 'agreement among raft nodes before linearized reading'  (duration: 99.741883ms)"],"step_count":1}
{"level":"warn","ts":"2022-11-03T02:21:31.375Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"107.666358ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/pods/ingress-nginx/ingress-nginx-controller-6bf7bc7f94-zd9qj\" ","response":"range_response_count:1 size:6011"}
{"level":"info","ts":"2022-11-03T02:21:31.379Z","caller":"traceutil/trace.go:171","msg":"trace[1654414346] range","detail":"{range_begin:/registry/pods/ingress-nginx/ingress-nginx-controller-6bf7bc7f94-zd9qj; range_end:; response_count:1; response_revision:408094; }","duration":"134.800269ms","start":"2022-11-03T02:21:31.243Z","end":"2022-11-03T02:21:31.378Z","steps":["trace[1654414346] 'range keys from in-memory index tree'  (duration: 107.257254ms)"],"step_count":1}
{"level":"warn","ts":"2022-11-03T02:21:31.391Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"105.547441ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/replicasets/ingress-nginx/ingress-nginx-controller-755dfbfc65\" ","response":"range_response_count:1 size:5264"}
{"level":"info","ts":"2022-11-03T02:21:31.392Z","caller":"traceutil/trace.go:171","msg":"trace[668465056] range","detail":"{range_begin:/registry/replicasets/ingress-nginx/ingress-nginx-controller-755dfbfc65; range_end:; response_count:1; response_revision:408094; }","duration":"149.441118ms","start":"2022-11-03T02:21:31.242Z","end":"2022-11-03T02:21:31.392Z","steps":["trace[668465056] 'range keys from in-memory index tree'  (duration: 104.757335ms)"],"step_count":1}
{"level":"warn","ts":"2022-11-03T02:21:31.572Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"110.253731ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/replicasets/ingress-nginx/ingress-nginx-controller-755dfbfc65\" ","response":"range_response_count:1 size:5264"}
{"level":"info","ts":"2022-11-03T02:21:31.572Z","caller":"traceutil/trace.go:171","msg":"trace[1640785143] range","detail":"{range_begin:/registry/replicasets/ingress-nginx/ingress-nginx-controller-755dfbfc65; range_end:; response_count:1; response_revision:408096; }","duration":"110.338504ms","start":"2022-11-03T02:21:31.462Z","end":"2022-11-03T02:21:31.572Z","steps":["trace[1640785143] 'range keys from in-memory index tree'  (duration: 109.878542ms)"],"step_count":1}
{"level":"info","ts":"2022-11-03T02:22:39.740Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":407809}
{"level":"info","ts":"2022-11-03T02:22:39.742Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":407809,"took":"823.032¬µs"}

* 
* ==> etcd [80f3da2d5c1c] <==
* {"level":"info","ts":"2022-10-28T12:54:38.047Z","caller":"traceutil/trace.go:171","msg":"trace[1942163733] linearizableReadLoop","detail":"{readStateIndex:27656; appliedIndex:27656; }","duration":"127.784384ms","start":"2022-10-28T12:54:37.919Z","end":"2022-10-28T12:54:38.047Z","steps":["trace[1942163733] 'read index received'  (duration: 127.769905ms)","trace[1942163733] 'applied index is now lower than readState.Index'  (duration: 12.733¬µs)"],"step_count":2}
{"level":"warn","ts":"2022-10-28T12:54:38.173Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"253.085848ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/apiregistration.k8s.io/apiservices/\" range_end:\"/registry/apiregistration.k8s.io/apiservices0\" count_only:true ","response":"range_response_count:0 size:8"}
{"level":"info","ts":"2022-10-28T12:54:38.177Z","caller":"traceutil/trace.go:171","msg":"trace[1493619216] range","detail":"{range_begin:/registry/apiregistration.k8s.io/apiservices/; range_end:/registry/apiregistration.k8s.io/apiservices0; response_count:0; response_revision:22832; }","duration":"257.833363ms","start":"2022-10-28T12:54:37.919Z","end":"2022-10-28T12:54:38.177Z","steps":["trace[1493619216] 'agreement among raft nodes before linearized reading'  (duration: 149.302311ms)","trace[1493619216] 'get authentication metadata'  (duration: 103.207467ms)"],"step_count":2}
{"level":"warn","ts":"2022-10-28T12:54:38.208Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"368.671006ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/flowschemas/\" range_end:\"/registry/flowschemas0\" count_only:true ","response":"range_response_count:0 size:8"}
{"level":"info","ts":"2022-10-28T12:54:38.210Z","caller":"traceutil/trace.go:171","msg":"trace[1762325086] range","detail":"{range_begin:/registry/flowschemas/; range_end:/registry/flowschemas0; response_count:0; response_revision:22832; }","duration":"369.859084ms","start":"2022-10-28T12:54:37.839Z","end":"2022-10-28T12:54:38.209Z","steps":["trace[1762325086] 'count revisions from in-memory index tree'  (duration: 367.92825ms)"],"step_count":1}
{"level":"warn","ts":"2022-10-28T12:54:38.211Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2022-10-28T12:54:37.839Z","time spent":"372.013994ms","remote":"127.0.0.1:45334","response type":"/etcdserverpb.KV/Range","request count":0,"request size":50,"response count":13,"response size":32,"request content":"key:\"/registry/flowschemas/\" range_end:\"/registry/flowschemas0\" count_only:true "}
{"level":"warn","ts":"2022-10-28T12:54:38.263Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"188.330197ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/cronjobs/\" range_end:\"/registry/cronjobs0\" count_only:true ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2022-10-28T12:54:38.263Z","caller":"traceutil/trace.go:171","msg":"trace[364006160] range","detail":"{range_begin:/registry/cronjobs/; range_end:/registry/cronjobs0; response_count:0; response_revision:22832; }","duration":"188.826698ms","start":"2022-10-28T12:54:38.074Z","end":"2022-10-28T12:54:38.263Z","steps":["trace[364006160] 'agreement among raft nodes before linearized reading'  (duration: 188.094682ms)"],"step_count":1}
{"level":"warn","ts":"2022-10-28T12:54:38.266Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"202.940658ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/runtimeclasses/\" range_end:\"/registry/runtimeclasses0\" count_only:true ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2022-10-28T12:54:38.266Z","caller":"traceutil/trace.go:171","msg":"trace[592159158] range","detail":"{range_begin:/registry/runtimeclasses/; range_end:/registry/runtimeclasses0; response_count:0; response_revision:22832; }","duration":"203.000468ms","start":"2022-10-28T12:54:38.063Z","end":"2022-10-28T12:54:38.266Z","steps":["trace[592159158] 'agreement among raft nodes before linearized reading'  (duration: 202.8888ms)"],"step_count":1}
{"level":"warn","ts":"2022-10-28T12:54:38.356Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"203.248092ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/\" range_end:\"/registry/services/endpoints0\" count_only:true ","response":"range_response_count:0 size:8"}
{"level":"info","ts":"2022-10-28T12:54:38.356Z","caller":"traceutil/trace.go:171","msg":"trace[611627373] range","detail":"{range_begin:/registry/services/endpoints/; range_end:/registry/services/endpoints0; response_count:0; response_revision:22832; }","duration":"203.364382ms","start":"2022-10-28T12:54:38.152Z","end":"2022-10-28T12:54:38.356Z","steps":["trace[611627373] 'agreement among raft nodes before linearized reading'  (duration: 83.323997ms)","trace[611627373] 'count revisions from in-memory index tree'  (duration: 119.885344ms)"],"step_count":2}
{"level":"info","ts":"2022-10-28T12:54:38.395Z","caller":"traceutil/trace.go:171","msg":"trace[1401818317] linearizableReadLoop","detail":"{readStateIndex:27656; appliedIndex:27656; }","duration":"105.989733ms","start":"2022-10-28T12:54:38.289Z","end":"2022-10-28T12:54:38.395Z","steps":["trace[1401818317] 'read index received'  (duration: 105.978293ms)","trace[1401818317] 'applied index is now lower than readState.Index'  (duration: 9.777¬µs)"],"step_count":2}
{"level":"warn","ts":"2022-10-28T12:54:38.396Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"106.75375ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/daemonsets/\" range_end:\"/registry/daemonsets0\" count_only:true ","response":"range_response_count:0 size:8"}
{"level":"info","ts":"2022-10-28T12:54:39.111Z","caller":"traceutil/trace.go:171","msg":"trace[852005263] range","detail":"{range_begin:/registry/daemonsets/; range_end:/registry/daemonsets0; response_count:0; response_revision:22832; }","duration":"820.220256ms","start":"2022-10-28T12:54:38.289Z","end":"2022-10-28T12:54:39.109Z","steps":["trace[852005263] 'agreement among raft nodes before linearized reading'  (duration: 106.659774ms)"],"step_count":1}
{"level":"info","ts":"2022-10-28T12:54:39.412Z","caller":"traceutil/trace.go:171","msg":"trace[317524866] linearizableReadLoop","detail":"{readStateIndex:27656; appliedIndex:27656; }","duration":"744.232193ms","start":"2022-10-28T12:54:38.668Z","end":"2022-10-28T12:54:39.412Z","steps":["trace[317524866] 'read index received'  (duration: 714.371485ms)","trace[317524866] 'applied index is now lower than readState.Index'  (duration: 26.211¬µs)"],"step_count":2}
{"level":"warn","ts":"2022-10-28T12:54:39.413Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"280.023972ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/controllers/\" range_end:\"/registry/controllers0\" count_only:true ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2022-10-28T12:54:39.414Z","caller":"traceutil/trace.go:171","msg":"trace[1918246735] range","detail":"{range_begin:/registry/controllers/; range_end:/registry/controllers0; response_count:0; response_revision:22832; }","duration":"280.415412ms","start":"2022-10-28T12:54:39.133Z","end":"2022-10-28T12:54:39.414Z","steps":["trace[1918246735] 'agreement among raft nodes before linearized reading'  (duration: 279.556074ms)"],"step_count":1}
{"level":"warn","ts":"2022-10-28T12:54:39.427Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"1.071782129s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/configmaps/ingress-nginx/ingress-controller-leader\" ","response":"range_response_count:1 size:604"}
{"level":"info","ts":"2022-10-28T12:54:39.428Z","caller":"traceutil/trace.go:171","msg":"trace[1659422273] range","detail":"{range_begin:/registry/configmaps/ingress-nginx/ingress-controller-leader; range_end:; response_count:1; response_revision:22832; }","duration":"1.072181394s","start":"2022-10-28T12:54:38.356Z","end":"2022-10-28T12:54:39.428Z","steps":["trace[1659422273] 'agreement among raft nodes before linearized reading'  (duration: 1.058861068s)"],"step_count":1}
{"level":"warn","ts":"2022-10-28T12:54:39.428Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2022-10-28T12:54:38.356Z","time spent":"1.072283101s","remote":"127.0.0.1:45238","response type":"/etcdserverpb.KV/Range","request count":0,"request size":62,"response count":1,"response size":628,"request content":"key:\"/registry/configmaps/ingress-nginx/ingress-controller-leader\" "}
{"level":"warn","ts":"2022-10-28T12:54:39.444Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"180.12124ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/namespaces/kube-public\" ","response":"range_response_count:1 size:354"}
{"level":"info","ts":"2022-10-28T12:54:39.444Z","caller":"traceutil/trace.go:171","msg":"trace[1558713857] range","detail":"{range_begin:/registry/namespaces/kube-public; range_end:; response_count:1; response_revision:22832; }","duration":"181.003698ms","start":"2022-10-28T12:54:39.263Z","end":"2022-10-28T12:54:39.444Z","steps":["trace[1558713857] 'agreement among raft nodes before linearized reading'  (duration: 179.444874ms)"],"step_count":1}
{"level":"warn","ts":"2022-10-28T12:54:39.456Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"261.584707ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/crd.projectcalico.org/globalnetworksets/\" range_end:\"/registry/crd.projectcalico.org/globalnetworksets0\" count_only:true ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2022-10-28T12:54:39.456Z","caller":"traceutil/trace.go:171","msg":"trace[1678208242] range","detail":"{range_begin:/registry/crd.projectcalico.org/globalnetworksets/; range_end:/registry/crd.projectcalico.org/globalnetworksets0; response_count:0; response_revision:22832; }","duration":"261.670618ms","start":"2022-10-28T12:54:39.195Z","end":"2022-10-28T12:54:39.456Z","steps":["trace[1678208242] 'agreement among raft nodes before linearized reading'  (duration: 261.520077ms)"],"step_count":1}
{"level":"warn","ts":"2022-10-28T12:54:39.527Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2022-10-28T12:54:38.289Z","time spent":"822.201943ms","remote":"127.0.0.1:45342","response type":"/etcdserverpb.KV/Range","request count":0,"request size":48,"response count":2,"response size":32,"request content":"key:\"/registry/daemonsets/\" range_end:\"/registry/daemonsets0\" count_only:true "}
{"level":"warn","ts":"2022-10-28T12:54:41.083Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"300.427752ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2022-10-28T12:54:41.085Z","caller":"traceutil/trace.go:171","msg":"trace[363621529] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:22832; }","duration":"337.644329ms","start":"2022-10-28T12:54:40.745Z","end":"2022-10-28T12:54:41.083Z","steps":["trace[363621529] 'agreement among raft nodes before linearized reading'  (duration: 337.359364ms)"],"step_count":1}
{"level":"info","ts":"2022-10-28T12:54:40.978Z","caller":"traceutil/trace.go:171","msg":"trace[2028949305] linearizableReadLoop","detail":"{readStateIndex:27656; appliedIndex:27656; }","duration":"195.18671ms","start":"2022-10-28T12:54:40.783Z","end":"2022-10-28T12:54:40.978Z","steps":["trace[2028949305] 'read index received'  (duration: 195.153967ms)","trace[2028949305] 'applied index is now lower than readState.Index'  (duration: 23.067¬µs)"],"step_count":2}
{"level":"warn","ts":"2022-10-28T12:54:41.096Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"276.24392ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/crd.projectcalico.org/networkpolicies/\" range_end:\"/registry/crd.projectcalico.org/networkpolicies0\" count_only:true ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2022-10-28T12:54:41.096Z","caller":"traceutil/trace.go:171","msg":"trace[764638541] range","detail":"{range_begin:/registry/crd.projectcalico.org/networkpolicies/; range_end:/registry/crd.projectcalico.org/networkpolicies0; response_count:0; response_revision:22832; }","duration":"276.321816ms","start":"2022-10-28T12:54:40.820Z","end":"2022-10-28T12:54:41.096Z","steps":["trace[764638541] 'agreement among raft nodes before linearized reading'  (duration: 276.205435ms)"],"step_count":1}
{"level":"warn","ts":"2022-10-28T12:54:41.098Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"236.192377ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/persistentvolumes/\" range_end:\"/registry/persistentvolumes0\" count_only:true ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2022-10-28T12:54:41.098Z","caller":"traceutil/trace.go:171","msg":"trace[1760281162] range","detail":"{range_begin:/registry/persistentvolumes/; range_end:/registry/persistentvolumes0; response_count:0; response_revision:22832; }","duration":"236.284841ms","start":"2022-10-28T12:54:40.861Z","end":"2022-10-28T12:54:41.098Z","steps":["trace[1760281162] 'agreement among raft nodes before linearized reading'  (duration: 236.14892ms)"],"step_count":1}
{"level":"warn","ts":"2022-10-28T12:54:41.141Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2022-10-28T12:54:40.745Z","time spent":"395.706379ms","remote":"127.0.0.1:45278","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":30,"request content":"key:\"/registry/health\" "}
{"level":"warn","ts":"2022-10-28T12:54:42.301Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"452.557294ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128016688749613409 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/leases/kube-node-lease/minikube\" mod_revision:22826 > success:<request_put:<key:\"/registry/leases/kube-node-lease/minikube\" value_size:474 >> failure:<request_range:<key:\"/registry/leases/kube-node-lease/minikube\" > >>","response":"size:18"}
{"level":"info","ts":"2022-10-28T12:54:42.302Z","caller":"traceutil/trace.go:171","msg":"trace[283201277] transaction","detail":"{read_only:false; response_revision:22833; number_of_response:1; }","duration":"786.128337ms","start":"2022-10-28T12:54:41.516Z","end":"2022-10-28T12:54:42.302Z","steps":["trace[283201277] 'process raft request'  (duration: 124.177568ms)","trace[283201277] 'get key's previous created_revision and leaseID' {req_type:put; key:/registry/leases/kube-node-lease/minikube; req_size:520; } (duration: 149.884142ms)","trace[283201277] 'store kv pair into bolt db' {req_type:put; key:/registry/leases/kube-node-lease/minikube; req_size:520; } (duration: 302.199504ms)"],"step_count":3}
{"level":"warn","ts":"2022-10-28T12:54:42.303Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2022-10-28T12:54:41.516Z","time spent":"786.41626ms","remote":"127.0.0.1:45282","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":523,"response count":0,"response size":42,"request content":"compare:<target:MOD key:\"/registry/leases/kube-node-lease/minikube\" mod_revision:22826 > success:<request_put:<key:\"/registry/leases/kube-node-lease/minikube\" value_size:474 >> failure:<request_range:<key:\"/registry/leases/kube-node-lease/minikube\" > >"}
{"level":"info","ts":"2022-10-28T12:54:42.303Z","caller":"traceutil/trace.go:171","msg":"trace[1199406331] linearizableReadLoop","detail":"{readStateIndex:27657; appliedIndex:27656; }","duration":"642.123411ms","start":"2022-10-28T12:54:41.661Z","end":"2022-10-28T12:54:42.303Z","steps":["trace[1199406331] 'read index received'  (duration: 226.211¬µs)","trace[1199406331] 'applied index is now lower than readState.Index'  (duration: 641.895218ms)"],"step_count":2}
{"level":"info","ts":"2022-10-28T12:54:42.303Z","caller":"traceutil/trace.go:171","msg":"trace[416759100] transaction","detail":"{read_only:false; response_revision:22834; number_of_response:1; }","duration":"642.035679ms","start":"2022-10-28T12:54:41.661Z","end":"2022-10-28T12:54:42.303Z","steps":["trace[416759100] 'process raft request'  (duration: 639.943348ms)"],"step_count":1}
{"level":"warn","ts":"2022-10-28T12:54:42.303Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2022-10-28T12:54:41.661Z","time spent":"642.200782ms","remote":"127.0.0.1:45282","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":539,"response count":0,"response size":42,"request content":"compare:<target:MOD key:\"/registry/leases/kube-node-lease/minikube-m02\" mod_revision:22830 > success:<request_put:<key:\"/registry/leases/kube-node-lease/minikube-m02\" value_size:486 >> failure:<request_range:<key:\"/registry/leases/kube-node-lease/minikube-m02\" > >"}
{"level":"warn","ts":"2022-10-28T12:54:42.306Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"645.193527ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/leases/ingress-nginx/ingress-controller-leader\" ","response":"range_response_count:1 size:512"}
{"level":"info","ts":"2022-10-28T12:54:42.306Z","caller":"traceutil/trace.go:171","msg":"trace[2007307755] range","detail":"{range_begin:/registry/leases/ingress-nginx/ingress-controller-leader; range_end:; response_count:1; response_revision:22834; }","duration":"645.304711ms","start":"2022-10-28T12:54:41.661Z","end":"2022-10-28T12:54:42.306Z","steps":["trace[2007307755] 'agreement among raft nodes before linearized reading'  (duration: 645.127011ms)"],"step_count":1}
{"level":"warn","ts":"2022-10-28T12:54:42.306Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2022-10-28T12:54:41.661Z","time spent":"645.517ms","remote":"127.0.0.1:45282","response type":"/etcdserverpb.KV/Range","request count":0,"request size":58,"response count":1,"response size":536,"request content":"key:\"/registry/leases/ingress-nginx/ingress-controller-leader\" "}
{"level":"warn","ts":"2022-10-28T12:54:42.307Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2022-10-28T12:54:41.690Z","time spent":"616.281907ms","remote":"127.0.0.1:45224","response type":"/etcdserverpb.Lease/LeaseGrant","request count":-1,"request size":-1,"response count":-1,"response size":-1,"request content":""}
{"level":"warn","ts":"2022-10-28T12:54:42.324Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"150.211217ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2022-10-28T12:54:42.325Z","caller":"traceutil/trace.go:171","msg":"trace[1968767869] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:22834; }","duration":"150.337647ms","start":"2022-10-28T12:54:42.174Z","end":"2022-10-28T12:54:42.324Z","steps":["trace[1968767869] 'agreement among raft nodes before linearized reading'  (duration: 149.906214ms)"],"step_count":1}
{"level":"warn","ts":"2022-10-28T12:54:42.666Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"244.891008ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2022-10-28T12:54:42.666Z","caller":"traceutil/trace.go:171","msg":"trace[1423559761] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:22835; }","duration":"245.575841ms","start":"2022-10-28T12:54:42.421Z","end":"2022-10-28T12:54:42.666Z","steps":["trace[1423559761] 'agreement among raft nodes before linearized reading'  (duration: 69.363431ms)","trace[1423559761] 'range keys from in-memory index tree'  (duration: 175.506245ms)"],"step_count":2}
{"level":"warn","ts":"2022-10-28T12:54:42.672Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"177.014322ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128016688749613419 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/events/ingress-nginx/ingress-nginx-controller-6bf7bc7f94-2zz8k.17223cd033b47673\" mod_revision:0 > success:<request_put:<key:\"/registry/events/ingress-nginx/ingress-nginx-controller-6bf7bc7f94-2zz8k.17223cd033b47673\" value_size:754 lease:8128016688749613411 >> failure:<>>","response":"size:18"}
{"level":"info","ts":"2022-10-28T12:54:42.673Z","caller":"traceutil/trace.go:171","msg":"trace[1195287654] transaction","detail":"{read_only:false; response_revision:22836; number_of_response:1; }","duration":"248.118748ms","start":"2022-10-28T12:54:42.425Z","end":"2022-10-28T12:54:42.673Z","steps":["trace[1195287654] 'process raft request'  (duration: 65.321312ms)","trace[1195287654] 'compare'  (duration: 174.955295ms)"],"step_count":2}
{"level":"info","ts":"2022-10-28T12:54:42.673Z","caller":"traceutil/trace.go:171","msg":"trace[2027818252] transaction","detail":"{read_only:false; response_revision:22837; number_of_response:1; }","duration":"233.678178ms","start":"2022-10-28T12:54:42.439Z","end":"2022-10-28T12:54:42.673Z","steps":["trace[2027818252] 'process raft request'  (duration: 233.134944ms)"],"step_count":1}
{"level":"info","ts":"2022-10-28T12:54:42.673Z","caller":"traceutil/trace.go:171","msg":"trace[504089501] linearizableReadLoop","detail":"{readStateIndex:27663; appliedIndex:27661; }","duration":"177.534113ms","start":"2022-10-28T12:54:42.496Z","end":"2022-10-28T12:54:42.673Z","steps":["trace[504089501] 'read index received'  (duration: 88.63506ms)","trace[504089501] 'applied index is now lower than readState.Index'  (duration: 88.896626ms)"],"step_count":2}
{"level":"warn","ts":"2022-10-28T12:54:42.674Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"177.887572ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2022-10-28T12:54:42.674Z","caller":"traceutil/trace.go:171","msg":"trace[528831077] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:22837; }","duration":"177.941015ms","start":"2022-10-28T12:54:42.496Z","end":"2022-10-28T12:54:42.674Z","steps":["trace[528831077] 'agreement among raft nodes before linearized reading'  (duration: 177.652129ms)"],"step_count":1}
{"level":"info","ts":"2022-10-28T12:54:42.684Z","caller":"traceutil/trace.go:171","msg":"trace[517349613] transaction","detail":"{read_only:false; response_revision:22838; number_of_response:1; }","duration":"168.560663ms","start":"2022-10-28T12:54:42.515Z","end":"2022-10-28T12:54:42.684Z","steps":["trace[517349613] 'process raft request'  (duration: 157.954398ms)"],"step_count":1}
{"level":"warn","ts":"2022-10-28T12:54:42.913Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"119.453485ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128016688749613430 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/leases/ingress-nginx/ingress-controller-leader\" mod_revision:22828 > success:<request_put:<key:\"/registry/leases/ingress-nginx/ingress-controller-leader\" value_size:429 >> failure:<request_range:<key:\"/registry/leases/ingress-nginx/ingress-controller-leader\" > >>","response":"size:18"}
{"level":"info","ts":"2022-10-28T12:54:42.915Z","caller":"traceutil/trace.go:171","msg":"trace[2133925298] transaction","detail":"{read_only:false; response_revision:22840; number_of_response:1; }","duration":"124.062007ms","start":"2022-10-28T12:54:42.790Z","end":"2022-10-28T12:54:42.914Z","steps":["trace[2133925298] 'compare'  (duration: 119.211404ms)"],"step_count":1}
{"level":"info","ts":"2022-10-28T12:54:42.915Z","caller":"traceutil/trace.go:171","msg":"trace[2133277657] transaction","detail":"{read_only:false; response_revision:22841; number_of_response:1; }","duration":"121.53196ms","start":"2022-10-28T12:54:42.794Z","end":"2022-10-28T12:54:42.915Z","steps":["trace[2133277657] 'process raft request'  (duration: 119.934055ms)"],"step_count":1}
{"level":"info","ts":"2022-10-28T12:56:20.166Z","caller":"osutil/interrupt_unix.go:64","msg":"received signal; shutting down","signal":"terminated"}
{"level":"info","ts":"2022-10-28T12:56:20.175Z","caller":"embed/etcd.go:368","msg":"closing etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}

* 
* ==> kernel <==
*  02:24:02 up 5 days,  2:45,  0 users,  load average: 1.92, 1.61, 1.18
Linux minikube 5.10.104-linuxkit #1 SMP Thu Mar 17 17:08:06 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 20.04.4 LTS"

* 
* ==> kube-apiserver [2881ee002254] <==
* W1102 21:18:21.047034       1 watcher.go:229] watch chan error: etcdserver: mvcc: required revision has been compacted
W1102 21:30:15.056613       1 watcher.go:229] watch chan error: etcdserver: mvcc: required revision has been compacted
W1102 21:40:19.650089       1 watcher.go:229] watch chan error: etcdserver: mvcc: required revision has been compacted
W1102 21:54:43.104710       1 watcher.go:229] watch chan error: etcdserver: mvcc: required revision has been compacted
W1102 22:07:45.884071       1 watcher.go:229] watch chan error: etcdserver: mvcc: required revision has been compacted
W1102 22:14:02.114256       1 watcher.go:229] watch chan error: etcdserver: mvcc: required revision has been compacted
W1102 22:29:00.915134       1 watcher.go:229] watch chan error: etcdserver: mvcc: required revision has been compacted
W1102 22:41:56.529995       1 watcher.go:229] watch chan error: etcdserver: mvcc: required revision has been compacted
W1102 22:51:01.782676       1 watcher.go:229] watch chan error: etcdserver: mvcc: required revision has been compacted
W1102 23:17:50.747224       1 watcher.go:229] watch chan error: etcdserver: mvcc: required revision has been compacted
W1102 23:31:14.309695       1 watcher.go:229] watch chan error: etcdserver: mvcc: required revision has been compacted
W1102 23:38:08.673669       1 watcher.go:229] watch chan error: etcdserver: mvcc: required revision has been compacted
W1102 23:54:33.416563       1 watcher.go:229] watch chan error: etcdserver: mvcc: required revision has been compacted
W1103 00:16:48.986448       1 watcher.go:229] watch chan error: etcdserver: mvcc: required revision has been compacted
W1103 00:28:07.618879       1 watcher.go:229] watch chan error: etcdserver: mvcc: required revision has been compacted
E1103 00:40:44.561861       1 writers.go:118] apiserver was unable to write a JSON response: http: Handler timeout
E1103 00:40:44.562312       1 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
I1103 00:40:44.565671       1 trace.go:205] Trace[602392150]: "Get" url:/api/v1/namespaces/default,user-agent:kube-apiserver/v1.24.3 (linux/amd64) kubernetes/aef86a9,audit-id:b8772fe3-c8ba-4ac4-808d-0bb4f55e4610,client:127.0.0.1,accept:application/vnd.kubernetes.protobuf, */*,protocol:HTTP/2.0 (03-Nov-2022 00:40:43.713) (total time: 850ms):
Trace[602392150]: ---"About to write a response" 850ms (00:40:44.564)
Trace[602392150]: [850.509482ms] [850.509482ms] END
E1103 00:40:44.720247       1 writers.go:131] apiserver was unable to write a fallback JSON response: http: Handler timeout
I1103 00:40:44.721648       1 trace.go:205] Trace[176494080]: "Get" url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,audit-id:5ada78c6-5930-4ead-995a-1f68af8f8318,client:10.244.120.64,accept:application/json, */*,protocol:HTTP/2.0 (03-Nov-2022 00:40:43.430) (total time: 1291ms):
Trace[176494080]: ---"About to write a response" 1120ms (00:40:44.551)
Trace[176494080]: ---"Writing http response done" 170ms (00:40:44.721)
Trace[176494080]: [1.291003058s] [1.291003058s] END
E1103 00:40:44.729787       1 timeout.go:141] post-timeout activity - time-elapsed: 155.133891ms, GET "/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath" result: <nil>
I1103 00:40:44.864674       1 trace.go:205] Trace[1622609551]: "Update" url:/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube-m02,user-agent:kubelet/v1.24.3 (linux/amd64) kubernetes/aef86a9,audit-id:94f0fb81-23c2-4859-b003-def6ba8938f8,client:192.168.49.3,accept:application/vnd.kubernetes.protobuf,application/json,protocol:HTTP/2.0 (03-Nov-2022 00:40:44.206) (total time: 658ms):
Trace[1622609551]: ---"Object stored in database" 544ms (00:40:44.864)
Trace[1622609551]: [658.032694ms] [658.032694ms] END
I1103 00:40:46.005169       1 trace.go:205] Trace[1111183869]: "GuaranteedUpdate etcd3" type:*coordination.Lease (03-Nov-2022 00:40:45.113) (total time: 891ms):
Trace[1111183869]: ---"Transaction committed" 777ms (00:40:46.004)
Trace[1111183869]: [891.48675ms] [891.48675ms] END
I1103 00:40:46.005444       1 trace.go:205] Trace[10730049]: "Update" url:/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube,user-agent:kubelet/v1.24.3 (linux/amd64) kubernetes/aef86a9,audit-id:0e2da58c-0fd9-42e1-9985-51be35e55e86,client:192.168.49.2,accept:application/vnd.kubernetes.protobuf,application/json,protocol:HTTP/2.0 (03-Nov-2022 00:40:45.109) (total time: 896ms):
Trace[10730049]: ---"Object stored in database" 895ms (00:40:46.005)
Trace[10730049]: [896.118159ms] [896.118159ms] END
I1103 00:40:46.019435       1 trace.go:205] Trace[2129398962]: "List(recursive=true) etcd3" key:/masterleases/,resourceVersion:0,resourceVersionMatch:NotOlderThan,limit:0,continue: (03-Nov-2022 00:40:45.226) (total time: 792ms):
Trace[2129398962]: [792.92449ms] [792.92449ms] END
I1103 00:40:46.032363       1 trace.go:205] Trace[59179163]: "GuaranteedUpdate etcd3" type:*core.Event (03-Nov-2022 00:40:45.521) (total time: 510ms):
Trace[59179163]: ---"initial value restored" 510ms (00:40:46.032)
Trace[59179163]: [510.841639ms] [510.841639ms] END
I1103 00:40:46.032958       1 trace.go:205] Trace[244615922]: "Patch" url:/api/v1/namespaces/kube-system/events/kube-apiserver-minikube.1722640647feb746,user-agent:kubelet/v1.24.3 (linux/amd64) kubernetes/aef86a9,audit-id:7037026c-e6c0-4d77-8dcb-31282193d69c,client:192.168.49.2,accept:application/vnd.kubernetes.protobuf,application/json,protocol:HTTP/2.0 (03-Nov-2022 00:40:45.520) (total time: 511ms):
Trace[244615922]: ---"About to apply patch" 511ms (00:40:46.032)
Trace[244615922]: [511.847599ms] [511.847599ms] END
I1103 00:40:46.035396       1 trace.go:205] Trace[1079175309]: "GuaranteedUpdate etcd3" type:*core.ConfigMap (03-Nov-2022 00:40:45.505) (total time: 529ms):
Trace[1079175309]: ---"Transaction committed" 527ms (00:40:46.035)
Trace[1079175309]: [529.48658ms] [529.48658ms] END
I1103 00:40:46.035546       1 trace.go:205] Trace[1724630472]: "Update" url:/api/v1/namespaces/ingress-nginx/configmaps/ingress-controller-leader,user-agent:nginx-ingress-controller/v1.3.0 (linux/amd64) ingress-nginx/2b7b74854d90ad9b4b96a5011b9e8b67d20bfb8f,audit-id:e492be5c-3c1d-4d4d-9df2-00c99cbbc5f8,client:10.244.120.77,accept:application/json, */*,protocol:HTTP/2.0 (03-Nov-2022 00:40:45.505) (total time: 530ms):
Trace[1724630472]: ---"Object stored in database" 529ms (00:40:46.035)
Trace[1724630472]: [530.280565ms] [530.280565ms] END
W1103 00:50:06.753999       1 watcher.go:229] watch chan error: etcdserver: mvcc: required revision has been compacted
I1103 01:00:10.471701       1 alloc.go:327] "allocated clusterIPs" service="default/hello" clusterIPs=map[IPv4:10.109.147.135]
I1103 01:19:15.087503       1 controller.go:611] quota admission added evaluator for: ingresses.networking.k8s.io
W1103 01:37:24.875576       1 watcher.go:229] watch chan error: etcdserver: mvcc: required revision has been compacted
W1103 01:51:40.754283       1 watcher.go:229] watch chan error: etcdserver: mvcc: required revision has been compacted
I1103 02:08:56.267636       1 trace.go:205] Trace[1961150493]: "Call validating webhook" configuration:ingress-nginx-admission,webhook:validate.nginx.ingress.kubernetes.io,resource:networking.k8s.io/v1, Resource=ingresses,subresource:,operation:CREATE,UID:bba5e799-885a-4aa6-9b16-5b9f6b4e0242 (03-Nov-2022 02:08:52.693) (total time: 3574ms):
Trace[1961150493]: ---"Request completed" 3574ms (02:08:56.267)
Trace[1961150493]: [3.574015831s] [3.574015831s] END
I1103 02:08:56.274371       1 trace.go:205] Trace[1104391521]: "Create" url:/apis/networking.k8s.io/v1/namespaces/default/ingresses,user-agent:kubectl/v1.24.2 (darwin/amd64) kubernetes/f66044f,audit-id:dd4f0fc0-d650-49b5-bc25-32d4ba58d116,client:192.168.49.1,accept:application/json,protocol:HTTP/2.0 (03-Nov-2022 02:08:52.692) (total time: 3581ms):
Trace[1104391521]: ---"Object stored in database" 3581ms (02:08:56.274)
Trace[1104391521]: [3.581820575s] [3.581820575s] END

* 
* ==> kube-apiserver [66c111322780] <==
* I1028 12:54:38.355626       1 trace.go:205] Trace[1446706380]: "Get" url:/api/v1/namespaces/default,user-agent:kube-apiserver/v1.24.3 (linux/amd64) kubernetes/aef86a9,audit-id:bf4dbd61-4793-4483-a855-250fc85d297c,client:127.0.0.1,accept:application/vnd.kubernetes.protobuf, */*,protocol:HTTP/2.0 (28-Oct-2022 12:54:37.848) (total time: 506ms):
Trace[1446706380]: ---"About to write a response" 506ms (12:54:38.354)
Trace[1446706380]: [506.421475ms] [506.421475ms] END
I1028 12:54:38.396025       1 trace.go:205] Trace[469911878]: "Get" url:/api/v1/namespaces/kube-system,user-agent:kube-apiserver/v1.24.3 (linux/amd64) kubernetes/aef86a9,audit-id:1464c3df-c063-45d3-ab2d-cbde3b4e96df,client:127.0.0.1,accept:application/vnd.kubernetes.protobuf, */*,protocol:HTTP/2.0 (28-Oct-2022 12:54:37.882) (total time: 509ms):
Trace[469911878]: ---"About to write a response" 508ms (12:54:38.391)
Trace[469911878]: [509.120775ms] [509.120775ms] END
I1028 12:54:39.687820       1 trace.go:205] Trace[1144354658]: "Get" url:/api/v1/namespaces/default/services/kubernetes,user-agent:kube-apiserver/v1.24.3 (linux/amd64) kubernetes/aef86a9,audit-id:517cdedb-8f42-4a3b-a584-89588e2d92a4,client:127.0.0.1,accept:application/vnd.kubernetes.protobuf, */*,protocol:HTTP/2.0 (28-Oct-2022 12:54:38.492) (total time: 1195ms):
Trace[1144354658]: ---"About to Get from storage" 135ms (12:54:38.627)
Trace[1144354658]: ---"About to write a response" 1059ms (12:54:39.687)
Trace[1144354658]: [1.195633153s] [1.195633153s] END
I1028 12:54:39.733320       1 trace.go:205] Trace[1691975542]: "Get" url:/api/v1/namespaces/kube-public,user-agent:kube-apiserver/v1.24.3 (linux/amd64) kubernetes/aef86a9,audit-id:5a8527c9-55e3-4763-904d-c8dc5e786b52,client:127.0.0.1,accept:application/vnd.kubernetes.protobuf, */*,protocol:HTTP/2.0 (28-Oct-2022 12:54:38.588) (total time: 1145ms):
Trace[1691975542]: ---"About to write a response" 1145ms (12:54:39.733)
Trace[1691975542]: [1.145199764s] [1.145199764s] END
{"level":"warn","ts":"2022-10-28T12:54:39.552Z","logger":"etcd-client","caller":"v3/retry_interceptor.go:62","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc0018b9180/127.0.0.1:2379","attempt":0,"error":"rpc error: code = Canceled desc = context canceled"}
E1028 12:54:39.733685       1 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"context canceled"}: context canceled
E1028 12:54:39.733787       1 writers.go:118] apiserver was unable to write a JSON response: http: Handler timeout
E1028 12:54:39.805977       1 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
I1028 12:54:40.255185       1 trace.go:205] Trace[646380668]: "Get" url:/api/v1/namespaces/ingress-nginx/configmaps/ingress-controller-leader,user-agent:nginx-ingress-controller/v1.3.0 (linux/amd64) ingress-nginx/2b7b74854d90ad9b4b96a5011b9e8b67d20bfb8f,audit-id:46df5103-64c2-49d9-8c7e-b142fd52844b,client:192.168.49.3,accept:application/json, */*,protocol:HTTP/2.0 (28-Oct-2022 12:54:39.641) (total time: 613ms):
Trace[646380668]: ---"About to write a response" 611ms (12:54:40.253)
Trace[646380668]: [613.15696ms] [613.15696ms] END
E1028 12:54:40.844021       1 writers.go:131] apiserver was unable to write a fallback JSON response: http: Handler timeout
I1028 12:54:40.844404       1 trace.go:205] Trace[1544604202]: "Get" url:/api/v1/namespaces/ingress-nginx/configmaps/ingress-controller-leader,user-agent:nginx-ingress-controller/v1.3.0 (linux/amd64) ingress-nginx/2b7b74854d90ad9b4b96a5011b9e8b67d20bfb8f,audit-id:3ba959e7-868d-47eb-be40-f5a112dfde3d,client:192.168.49.3,accept:application/json, */*,protocol:HTTP/2.0 (28-Oct-2022 12:54:37.739) (total time: 3104ms):
Trace[1544604202]: [3.104877223s] [3.104877223s] END
E1028 12:54:40.846228       1 timeout.go:141] post-timeout activity - time-elapsed: 1.339606726s, GET "/api/v1/namespaces/ingress-nginx/configmaps/ingress-controller-leader" result: <nil>
I1028 12:54:40.848613       1 trace.go:205] Trace[1379320623]: "Get" url:/api/v1/namespaces/ingress-nginx/services/ingress-nginx-controller,user-agent:nginx-ingress-controller/v1.3.0 (linux/amd64) ingress-nginx/2b7b74854d90ad9b4b96a5011b9e8b67d20bfb8f,audit-id:9a2f0372-f69c-4c16-bb63-17be12bc7ede,client:192.168.49.3,accept:application/json, */*,protocol:HTTP/2.0 (28-Oct-2022 12:54:39.167) (total time: 1680ms):
Trace[1379320623]: ---"About to write a response" 686ms (12:54:39.854)
Trace[1379320623]: ---"Writing http response done" 994ms (12:54:40.848)
Trace[1379320623]: [1.68072558s] [1.68072558s] END
W1028 12:54:42.181246       1 watcher.go:229] watch chan error: etcdserver: mvcc: required revision has been compacted
W1028 12:54:42.182008       1 watcher.go:229] watch chan error: etcdserver: mvcc: required revision has been compacted
W1028 12:54:42.182611       1 watcher.go:229] watch chan error: etcdserver: mvcc: required revision has been compacted
I1028 12:54:42.341577       1 trace.go:205] Trace[2022659629]: "GuaranteedUpdate etcd3" type:*coordination.Lease (28-Oct-2022 12:54:41.149) (total time: 1191ms):
Trace[2022659629]: ---"Transaction prepared" 282ms (12:54:41.432)
Trace[2022659629]: ---"Transaction committed" 909ms (12:54:42.341)
Trace[2022659629]: [1.191551231s] [1.191551231s] END
I1028 12:54:42.342959       1 trace.go:205] Trace[356657347]: "Update" url:/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube-m02,user-agent:kubelet/v1.24.3 (linux/amd64) kubernetes/aef86a9,audit-id:42c88443-f908-49e1-9492-9dc8295bfb5c,client:192.168.49.3,accept:application/vnd.kubernetes.protobuf,application/json,protocol:HTTP/2.0 (28-Oct-2022 12:54:41.128) (total time: 1213ms):
Trace[356657347]: ---"Object stored in database" 1202ms (12:54:42.341)
Trace[356657347]: [1.213582853s] [1.213582853s] END
I1028 12:54:42.379772       1 trace.go:205] Trace[1174805284]: "GuaranteedUpdate etcd3" type:*coordination.Lease (28-Oct-2022 12:54:41.439) (total time: 940ms):
Trace[1174805284]: ---"Transaction committed" 910ms (12:54:42.379)
Trace[1174805284]: [940.4275ms] [940.4275ms] END
I1028 12:54:42.380097       1 trace.go:205] Trace[263701139]: "Update" url:/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube,user-agent:kubelet/v1.24.3 (linux/amd64) kubernetes/aef86a9,audit-id:54d30ea5-6c70-4b6f-8593-41d0bee52ef6,client:192.168.49.2,accept:application/vnd.kubernetes.protobuf,application/json,protocol:HTTP/2.0 (28-Oct-2022 12:54:41.397) (total time: 982ms):
Trace[263701139]: ---"Object stored in database" 940ms (12:54:42.379)
Trace[263701139]: [982.193098ms] [982.193098ms] END
I1028 12:54:42.380953       1 trace.go:205] Trace[1935282679]: "Get" url:/apis/coordination.k8s.io/v1/namespaces/ingress-nginx/leases/ingress-controller-leader,user-agent:nginx-ingress-controller/v1.3.0 (linux/amd64) ingress-nginx/2b7b74854d90ad9b4b96a5011b9e8b67d20bfb8f,audit-id:ca005d5c-3180-4d55-8a14-4d2044cdf553,client:192.168.49.3,accept:application/json, */*,protocol:HTTP/2.0 (28-Oct-2022 12:54:41.371) (total time: 1009ms):
Trace[1935282679]: ---"About to write a response" 1008ms (12:54:42.380)
Trace[1935282679]: [1.00911152s] [1.00911152s] END
I1028 12:54:42.524212       1 trace.go:205] Trace[229101135]: "Create" url:/api/v1/namespaces/kube-system/events,user-agent:kubelet/v1.24.3 (linux/amd64) kubernetes/aef86a9,audit-id:b5564be3-82be-43cf-bab4-fac7b5a4d103,client:192.168.49.2,accept:application/vnd.kubernetes.protobuf,application/json,protocol:HTTP/2.0 (28-Oct-2022 12:54:41.457) (total time: 1066ms):
Trace[229101135]: ---"Object stored in database" 1066ms (12:54:42.523)
Trace[229101135]: [1.066850919s] [1.066850919s] END
I1028 12:54:42.677203       1 trace.go:205] Trace[373788037]: "GuaranteedUpdate etcd3" type:*v1.Endpoints (28-Oct-2022 12:54:40.969) (total time: 1707ms):
Trace[373788037]: ---"initial value restored" 834ms (12:54:41.803)
Trace[373788037]: ---"Transaction prepared" 606ms (12:54:42.409)
Trace[373788037]: ---"Transaction committed" 267ms (12:54:42.677)
Trace[373788037]: [1.707895716s] [1.707895716s] END
I1028 12:54:42.681212       1 trace.go:205] Trace[2058663754]: "Create" url:/api/v1/namespaces/ingress-nginx/events,user-agent:kubelet/v1.24.3 (linux/amd64) kubernetes/aef86a9,audit-id:c4b93ff8-e400-49ee-b6e9-b0891ba16207,client:192.168.49.3,accept:application/vnd.kubernetes.protobuf,application/json,protocol:HTTP/2.0 (28-Oct-2022 12:54:41.416) (total time: 1265ms):
Trace[2058663754]: ---"Object stored in database" 1258ms (12:54:42.680)
Trace[2058663754]: [1.265045832s] [1.265045832s] END
W1028 12:54:43.187172       1 cacher.go:150] Terminating all watchers from cacher *unstructured.Unstructured
W1028 12:54:43.187465       1 cacher.go:150] Terminating all watchers from cacher *storage.VolumeAttachment

* 
* ==> kube-controller-manager [127a41c5cb7d] <==
* I1102 01:14:21.374945       1 event.go:294] "Event occurred" object="kube-system/kube-apiserver-minikube" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I1102 01:14:21.447141       1 event.go:294] "Event occurred" object="kube-system/storage-provisioner" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I1102 01:14:21.470797       1 event.go:294] "Event occurred" object="kube-system/kube-proxy-6s9dq" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I1102 01:14:21.480282       1 event.go:294] "Event occurred" object="kube-system/kube-scheduler-minikube" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I1102 01:14:21.583170       1 event.go:294] "Event occurred" object="kube-system/calico-node-vw5c8" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I1102 01:14:21.654317       1 event.go:294] "Event occurred" object="default/nginx-8f458dc5b-pxpjt" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
W1102 01:14:31.368696       1 topologycache.go:199] Can't get CPU or zone information for minikube-m02 node
I1102 01:14:31.654718       1 event.go:294] "Event occurred" object="kube-system/coredns-6d4b75cb6d-nr4nw" fieldPath="" kind="Pod" apiVersion="" type="Normal" reason="TaintManagerEviction" message="Cancelling deletion of Pod kube-system/coredns-6d4b75cb6d-nr4nw"
I1102 01:14:31.658593       1 event.go:294] "Event occurred" object="ingress-nginx/ingress-nginx-controller-6bf7bc7f94-zd9qj" fieldPath="" kind="Pod" apiVersion="" type="Normal" reason="TaintManagerEviction" message="Cancelling deletion of Pod ingress-nginx/ingress-nginx-controller-6bf7bc7f94-zd9qj"
I1102 01:14:31.658691       1 event.go:294] "Event occurred" object="kube-system/calico-kube-controllers-c44b4545-28m7f" fieldPath="" kind="Pod" apiVersion="" type="Normal" reason="TaintManagerEviction" message="Cancelling deletion of Pod kube-system/calico-kube-controllers-c44b4545-28m7f"
I1102 01:14:31.658705       1 event.go:294] "Event occurred" object="kube-system/storage-provisioner" fieldPath="" kind="Pod" apiVersion="" type="Normal" reason="TaintManagerEviction" message="Cancelling deletion of Pod kube-system/storage-provisioner"
I1102 01:14:31.658714       1 event.go:294] "Event occurred" object="default/nginx-8f458dc5b-pxpjt" fieldPath="" kind="Pod" apiVersion="" type="Normal" reason="TaintManagerEviction" message="Cancelling deletion of Pod default/nginx-8f458dc5b-pxpjt"
E1102 01:21:39.584963       1 node_lifecycle_controller.go:1108] Error updating node minikube: Operation cannot be fulfilled on nodes "minikube": the object has been modified; please apply your changes to the latest version and try again
W1102 01:23:04.041259       1 topologycache.go:199] Can't get CPU or zone information for minikube-m02 node
I1102 01:23:04.131542       1 event.go:294] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="NodeNotReady" message="Node minikube status is now: NodeNotReady"
I1102 01:23:04.246409       1 event.go:294] "Event occurred" object="kube-system/coredns-6d4b75cb6d-nr4nw" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I1102 01:23:04.334088       1 event.go:294] "Event occurred" object="ingress-nginx/ingress-nginx-controller-6bf7bc7f94-zd9qj" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I1102 01:23:04.452240       1 event.go:294] "Event occurred" object="kube-system/kube-apiserver-minikube" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I1102 01:23:04.515565       1 event.go:294] "Event occurred" object="kube-system/storage-provisioner" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I1102 01:23:04.557891       1 event.go:294] "Event occurred" object="kube-system/kube-proxy-6s9dq" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I1102 01:23:04.617022       1 event.go:294] "Event occurred" object="kube-system/kube-scheduler-minikube" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I1102 01:23:04.641192       1 event.go:294] "Event occurred" object="kube-system/calico-node-vw5c8" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I1102 01:23:04.663346       1 event.go:294] "Event occurred" object="kube-system/kube-controller-manager-minikube" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I1102 01:23:04.754395       1 event.go:294] "Event occurred" object="kube-system/calico-kube-controllers-c44b4545-28m7f" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I1102 01:23:04.819172       1 event.go:294] "Event occurred" object="kube-system/etcd-minikube" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
W1102 01:23:04.855670       1 topologycache.go:199] Can't get CPU or zone information for minikube-m02 node
I1102 01:23:04.858272       1 event.go:294] "Event occurred" object="default/nginx-8f458dc5b-pxpjt" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
E1102 01:28:52.952921       1 node_lifecycle_controller.go:1108] Error updating node minikube: Operation cannot be fulfilled on nodes "minikube": the object has been modified; please apply your changes to the latest version and try again
E1102 01:38:18.840508       1 node_lifecycle_controller.go:1108] Error updating node minikube: Operation cannot be fulfilled on nodes "minikube": the object has been modified; please apply your changes to the latest version and try again
E1102 01:40:24.048741       1 resource_quota_controller.go:413] failed to discover resources: the server was unable to return a response in the time allotted, but may still be processing the request
W1102 01:40:24.051605       1 garbagecollector.go:749] failed to discover preferred resources: the server was unable to return a response in the time allotted, but may still be processing the request
E1102 01:40:47.062512       1 node_lifecycle_controller.go:1108] Error updating node minikube: the server was unable to return a response in the time allotted, but may still be processing the request (put nodes minikube)
I1102 01:40:51.566214       1 event.go:294] "Event occurred" object="minikube-m02" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="NodeNotReady" message="Node minikube-m02 status is now: NodeNotReady"
I1102 01:40:51.682728       1 event.go:294] "Event occurred" object="kube-system/kube-proxy-2fz2h" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I1102 01:40:51.832621       1 event.go:294] "Event occurred" object="kube-system/calico-node-pmpd5" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
E1102 06:03:31.005933       1 controller_utils.go:197] unable to taint [&Taint{Key:node.kubernetes.io/unreachable,Value:,Effect:NoExecute,TimeAdded:2022-11-02 01:40:56.890796886 +0000 UTC m=+352824.396030108,}] unresponsive Node "minikube-m02": stream error when reading response body, may be caused by closed connection. Please retry. Original error: stream error: stream ID 301; INTERNAL_ERROR; received from peer
W1102 06:04:31.941633       1 client_builder_dynamic.go:197] get or create service account failed: the server was unable to return a response in the time allotted, but may still be processing the request (get serviceaccounts generic-garbage-collector)
W1102 06:04:31.941664       1 client_builder_dynamic.go:197] get or create service account failed: the server was unable to return a response in the time allotted, but may still be processing the request (get serviceaccounts node-controller)
W1102 06:04:31.941895       1 client_builder_dynamic.go:197] get or create service account failed: the server was unable to return a response in the time allotted, but may still be processing the request (get serviceaccounts resourcequota-controller)
W1102 06:05:32.560948       1 client_builder_dynamic.go:197] get or create service account failed: the server was unable to return a response in the time allotted, but may still be processing the request (get serviceaccounts node-controller)
W1102 06:05:32.560980       1 client_builder_dynamic.go:197] get or create service account failed: the server was unable to return a response in the time allotted, but may still be processing the request (get serviceaccounts generic-garbage-collector)
W1102 06:05:32.561013       1 client_builder_dynamic.go:197] get or create service account failed: the server was unable to return a response in the time allotted, but may still be processing the request (get serviceaccounts resourcequota-controller)
W1102 06:06:33.548141       1 client_builder_dynamic.go:197] get or create service account failed: the server was unable to return a response in the time allotted, but may still be processing the request (get serviceaccounts resourcequota-controller)
W1102 06:06:33.548199       1 client_builder_dynamic.go:197] get or create service account failed: the server was unable to return a response in the time allotted, but may still be processing the request (get serviceaccounts node-controller)
W1102 06:06:33.548211       1 client_builder_dynamic.go:197] get or create service account failed: the server was unable to return a response in the time allotted, but may still be processing the request (get serviceaccounts generic-garbage-collector)
W1102 06:06:47.716477       1 topologycache.go:199] Can't get CPU or zone information for minikube-m02 node
I1102 16:44:21.064261       1 job_controller.go:498] enqueueing job ingress-nginx/ingress-nginx-admission-create
I1102 16:44:21.064372       1 job_controller.go:498] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I1102 22:52:50.518736       1 namespace_controller.go:185] Namespace has been deleted ns-b
I1102 22:52:50.518739       1 namespace_controller.go:185] Namespace has been deleted ns-a
I1103 00:53:53.855946       1 event.go:294] "Event occurred" object="default/mc" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set mc-7f47959c54 to 1"
I1103 00:53:53.885914       1 event.go:294] "Event occurred" object="default/mc-7f47959c54" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: mc-7f47959c54-ccjlk"
I1103 00:57:50.807018       1 event.go:294] "Event occurred" object="default/mc" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set mc-7f47959c54 to 1"
I1103 00:57:50.818347       1 event.go:294] "Event occurred" object="default/mc-7f47959c54" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: mc-7f47959c54-cpnpg"
I1103 00:58:21.953251       1 event.go:294] "Event occurred" object="default/hello" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set hello-685bd55bcd to 1"
I1103 00:58:21.964233       1 event.go:294] "Event occurred" object="default/hello-685bd55bcd" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: hello-685bd55bcd-nqlm4"
I1103 02:21:30.779120       1 event.go:294] "Event occurred" object="ingress-nginx/ingress-nginx-controller" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set ingress-nginx-controller-755dfbfc65 to 1"
I1103 02:21:30.950940       1 event.go:294] "Event occurred" object="ingress-nginx/ingress-nginx-controller-755dfbfc65" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: ingress-nginx-controller-755dfbfc65-gw44w"
I1103 02:21:30.980557       1 event.go:294] "Event occurred" object="ingress-nginx/ingress-nginx-controller" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set ingress-nginx-controller-6bf7bc7f94 to 0"
I1103 02:21:31.754997       1 event.go:294] "Event occurred" object="ingress-nginx/ingress-nginx-controller-6bf7bc7f94" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: ingress-nginx-controller-6bf7bc7f94-zd9qj"

* 
* ==> kube-controller-manager [d8c20ccfa274] <==
* I1028 11:11:55.493427       1 job_controller.go:498] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I1028 11:11:55.501265       1 job_controller.go:498] enqueueing job ingress-nginx/ingress-nginx-admission-create
I1028 11:11:55.508014       1 event.go:294] "Event occurred" object="ingress-nginx/ingress-nginx-controller-6bf7bc7f94" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: ingress-nginx-controller-6bf7bc7f94-2zz8k"
I1028 11:11:55.621726       1 job_controller.go:498] enqueueing job ingress-nginx/ingress-nginx-admission-create
I1028 11:11:55.760334       1 job_controller.go:498] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I1028 11:11:55.798459       1 job_controller.go:498] enqueueing job ingress-nginx/ingress-nginx-admission-create
I1028 11:11:55.913798       1 job_controller.go:498] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I1028 11:11:58.031971       1 job_controller.go:498] enqueueing job ingress-nginx/ingress-nginx-admission-create
I1028 11:11:58.170966       1 job_controller.go:498] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I1028 11:12:04.750859       1 job_controller.go:498] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I1028 11:12:04.909393       1 job_controller.go:498] enqueueing job ingress-nginx/ingress-nginx-admission-create
I1028 11:12:05.777035       1 job_controller.go:498] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I1028 11:12:05.943810       1 job_controller.go:498] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I1028 11:12:06.063688       1 job_controller.go:498] enqueueing job ingress-nginx/ingress-nginx-admission-create
I1028 11:12:06.975276       1 job_controller.go:498] enqueueing job ingress-nginx/ingress-nginx-admission-create
I1028 11:12:06.987251       1 job_controller.go:498] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I1028 11:12:07.065806       1 event.go:294] "Event occurred" object="ingress-nginx/ingress-nginx-admission-create" fieldPath="" kind="Job" apiVersion="batch/v1" type="Normal" reason="Completed" message="Job completed"
I1028 11:12:07.077954       1 job_controller.go:498] enqueueing job ingress-nginx/ingress-nginx-admission-create
I1028 11:12:07.191799       1 job_controller.go:498] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I1028 11:12:08.003736       1 job_controller.go:498] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I1028 11:12:08.192417       1 event.go:294] "Event occurred" object="ingress-nginx/ingress-nginx-admission-patch" fieldPath="" kind="Job" apiVersion="batch/v1" type="Normal" reason="Completed" message="Job completed"
I1028 11:12:08.208904       1 job_controller.go:498] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I1028 11:22:29.852632       1 event.go:294] "Event occurred" object="default/cricket" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set cricket-6c5dd9b58b to 3"
I1028 11:22:29.880716       1 event.go:294] "Event occurred" object="default/cricket-6c5dd9b58b" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: cricket-6c5dd9b58b-pbrcb"
I1028 11:22:29.892467       1 event.go:294] "Event occurred" object="default/cricket-6c5dd9b58b" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: cricket-6c5dd9b58b-pxdmd"
I1028 11:22:29.900015       1 event.go:294] "Event occurred" object="default/cricket-6c5dd9b58b" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: cricket-6c5dd9b58b-rlq98"
I1028 11:26:31.940204       1 event.go:294] "Event occurred" object="default/football" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set football-55b7db499d to 2"
I1028 11:26:31.968136       1 event.go:294] "Event occurred" object="default/football-55b7db499d" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: football-55b7db499d-szxv6"
I1028 11:26:32.000502       1 event.go:294] "Event occurred" object="default/football-55b7db499d" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: football-55b7db499d-mc7zh"
I1028 11:27:15.315263       1 event.go:294] "Event occurred" object="default/football" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set football-77f977f7b8 to 1"
I1028 11:27:15.465941       1 event.go:294] "Event occurred" object="default/football-77f977f7b8" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: football-77f977f7b8-xvz55"
I1028 12:11:33.303110       1 event.go:294] "Event occurred" object="default/cricket" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set cricket-84d9fb8dd9 to 3"
I1028 12:11:33.344790       1 event.go:294] "Event occurred" object="default/football" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set football-7868575f9 to 2"
I1028 12:11:33.378242       1 event.go:294] "Event occurred" object="default/football-7868575f9" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: football-7868575f9-9dc67"
I1028 12:11:33.389729       1 event.go:294] "Event occurred" object="default/cricket-84d9fb8dd9" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: cricket-84d9fb8dd9-kvrvb"
I1028 12:11:33.437963       1 event.go:294] "Event occurred" object="default/football-7868575f9" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: football-7868575f9-n6prs"
I1028 12:11:33.438044       1 event.go:294] "Event occurred" object="default/cricket-84d9fb8dd9" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: cricket-84d9fb8dd9-psbbx"
I1028 12:11:33.458940       1 event.go:294] "Event occurred" object="default/cricket-84d9fb8dd9" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: cricket-84d9fb8dd9-7r7pb"
I1028 12:37:08.129298       1 event.go:294] "Event occurred" object="default/cricket" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set cricket-84d9fb8dd9 to 3"
I1028 12:37:08.159842       1 event.go:294] "Event occurred" object="default/football" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set football-7868575f9 to 2"
I1028 12:37:08.171515       1 event.go:294] "Event occurred" object="default/cricket-84d9fb8dd9" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: cricket-84d9fb8dd9-bd2gf"
I1028 12:37:08.185190       1 event.go:294] "Event occurred" object="default/football-7868575f9" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: football-7868575f9-6v2rn"
I1028 12:37:08.222813       1 event.go:294] "Event occurred" object="default/cricket-84d9fb8dd9" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: cricket-84d9fb8dd9-cv9mk"
I1028 12:37:08.310834       1 event.go:294] "Event occurred" object="default/football-7868575f9" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: football-7868575f9-snzfl"
I1028 12:37:08.328469       1 event.go:294] "Event occurred" object="default/cricket-84d9fb8dd9" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: cricket-84d9fb8dd9-2bjn4"
I1028 12:47:11.506457       1 event.go:294] "Event occurred" object="default/cricket" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set cricket-84d9fb8dd9 to 3"
I1028 12:47:11.519518       1 event.go:294] "Event occurred" object="default/cricket-84d9fb8dd9" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: cricket-84d9fb8dd9-7vzxj"
I1028 12:47:11.528995       1 event.go:294] "Event occurred" object="default/cricket-84d9fb8dd9" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: cricket-84d9fb8dd9-wrpjs"
I1028 12:47:11.535044       1 event.go:294] "Event occurred" object="default/cricket-84d9fb8dd9" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: cricket-84d9fb8dd9-ftnj7"
I1028 12:47:11.632950       1 event.go:294] "Event occurred" object="default/football" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set football-7868575f9 to 2"
I1028 12:47:11.663988       1 event.go:294] "Event occurred" object="default/football-7868575f9" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: football-7868575f9-w28jv"
I1028 12:47:11.673499       1 event.go:294] "Event occurred" object="default/football-7868575f9" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: football-7868575f9-2r46j"
I1028 12:49:17.824840       1 event.go:294] "Event occurred" object="default/cricket" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set cricket-84d9fb8dd9 to 3"
I1028 12:49:17.869585       1 event.go:294] "Event occurred" object="default/cricket-84d9fb8dd9" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: cricket-84d9fb8dd9-ssfgp"
I1028 12:49:17.892920       1 event.go:294] "Event occurred" object="default/cricket-84d9fb8dd9" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: cricket-84d9fb8dd9-7ps9h"
I1028 12:49:17.967205       1 event.go:294] "Event occurred" object="default/cricket-84d9fb8dd9" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: cricket-84d9fb8dd9-zrxcm"
I1028 12:49:17.993400       1 event.go:294] "Event occurred" object="default/football" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set football-7868575f9 to 2"
I1028 12:49:18.094641       1 event.go:294] "Event occurred" object="default/football-7868575f9" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: football-7868575f9-8s4lv"
I1028 12:49:18.181076       1 event.go:294] "Event occurred" object="default/football-7868575f9" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: football-7868575f9-lczfc"
I1028 12:49:18.196812       1 event.go:294] "Event occurred" object="cricket" fieldPath="" kind="Endpoints" apiVersion="v1" type="Warning" reason="FailedToCreateEndpoint" message="Failed to create endpoint for service default/cricket: endpoints \"cricket\" already exists"

* 
* ==> kube-proxy [4a37bf14a075] <==
* Trace[441024176]: [2.631588082s] [2.631588082s] END
I1102 00:34:20.472396       1 trace.go:205] Trace[805904441]: "iptables save" (02-Nov-2022 00:34:08.888) (total time: 11125ms):
Trace[805904441]: [11.12549431s] [11.12549431s] END
I1102 00:34:51.865180       1 trace.go:205] Trace[1269851612]: "iptables ChainExists" (02-Nov-2022 00:34:20.752) (total time: 31150ms):
Trace[1269851612]: [31.150122649s] [31.150122649s] END
I1102 00:37:16.724350       1 trace.go:205] Trace[1828855947]: "iptables ChainExists" (02-Nov-2022 00:37:12.400) (total time: 4323ms):
Trace[1828855947]: [4.323153575s] [4.323153575s] END
I1102 01:06:14.547897       1 trace.go:205] Trace[1804091245]: "iptables ChainExists" (02-Nov-2022 01:06:10.831) (total time: 3715ms):
Trace[1804091245]: [3.715916891s] [3.715916891s] END
I1102 01:06:14.807190       1 trace.go:205] Trace[1568365584]: "iptables ChainExists" (02-Nov-2022 01:06:10.590) (total time: 4215ms):
Trace[1568365584]: [4.21598948s] [4.21598948s] END
I1102 01:07:45.543080       1 trace.go:205] Trace[1295716052]: "iptables ChainExists" (02-Nov-2022 01:07:40.634) (total time: 4908ms):
Trace[1295716052]: [4.908661499s] [4.908661499s] END
I1102 01:07:45.543134       1 trace.go:205] Trace[2082544435]: "iptables ChainExists" (02-Nov-2022 01:07:40.553) (total time: 4989ms):
Trace[2082544435]: [4.989312352s] [4.989312352s] END
I1102 01:08:16.628936       1 trace.go:205] Trace[1972049436]: "iptables ChainExists" (02-Nov-2022 01:08:10.509) (total time: 6118ms):
Trace[1972049436]: [6.118905953s] [6.118905953s] END
I1102 01:08:16.677412       1 trace.go:205] Trace[1227088754]: "iptables ChainExists" (02-Nov-2022 01:08:11.147) (total time: 5530ms):
Trace[1227088754]: [5.530104221s] [5.530104221s] END
I1102 01:09:14.267804       1 trace.go:205] Trace[1314796340]: "iptables ChainExists" (02-Nov-2022 01:09:10.454) (total time: 3806ms):
Trace[1314796340]: [3.806168784s] [3.806168784s] END
I1102 01:09:15.074038       1 trace.go:205] Trace[1912290199]: "iptables ChainExists" (02-Nov-2022 01:09:10.668) (total time: 4262ms):
Trace[1912290199]: [4.26296564s] [4.26296564s] END
I1102 01:10:00.954218       1 trace.go:205] Trace[1795590735]: "iptables ChainExists" (02-Nov-2022 01:09:54.488) (total time: 6427ms):
Trace[1795590735]: [6.427139318s] [6.427139318s] END
I1102 01:10:02.345030       1 trace.go:205] Trace[1340353850]: "iptables ChainExists" (02-Nov-2022 01:09:54.613) (total time: 7731ms):
Trace[1340353850]: [7.731187811s] [7.731187811s] END
I1102 01:10:45.441429       1 trace.go:205] Trace[14827946]: "iptables ChainExists" (02-Nov-2022 01:10:40.738) (total time: 4702ms):
Trace[14827946]: [4.702471477s] [4.702471477s] END
I1102 01:10:45.535621       1 trace.go:205] Trace[1762900279]: "iptables ChainExists" (02-Nov-2022 01:10:40.399) (total time: 5132ms):
Trace[1762900279]: [5.132954413s] [5.132954413s] END
I1102 01:11:18.991716       1 trace.go:205] Trace[713796232]: "iptables ChainExists" (02-Nov-2022 01:11:10.510) (total time: 8481ms):
Trace[713796232]: [8.48107445s] [8.48107445s] END
I1102 01:11:19.313420       1 trace.go:205] Trace[1368052422]: "iptables ChainExists" (02-Nov-2022 01:11:10.472) (total time: 8840ms):
Trace[1368052422]: [8.840773917s] [8.840773917s] END
I1102 01:11:45.521046       1 trace.go:205] Trace[914261861]: "iptables ChainExists" (02-Nov-2022 01:11:40.343) (total time: 5176ms):
Trace[914261861]: [5.1768604s] [5.1768604s] END
I1102 01:11:46.140481       1 trace.go:205] Trace[1331805954]: "iptables ChainExists" (02-Nov-2022 01:11:40.489) (total time: 5650ms):
Trace[1331805954]: [5.650364445s] [5.650364445s] END
I1102 01:12:12.851164       1 trace.go:205] Trace[184842613]: "iptables ChainExists" (02-Nov-2022 01:12:10.584) (total time: 2239ms):
Trace[184842613]: [2.239358703s] [2.239358703s] END
I1102 01:12:13.719318       1 trace.go:205] Trace[138863755]: "iptables ChainExists" (02-Nov-2022 01:12:10.470) (total time: 3230ms):
Trace[138863755]: [3.230807366s] [3.230807366s] END
I1102 01:14:18.796436       1 trace.go:205] Trace[499483676]: "iptables ChainExists" (02-Nov-2022 01:12:40.314) (total time: 98556ms):
Trace[499483676]: [1m38.556040656s] [1m38.556040656s] END
I1102 01:14:18.869753       1 trace.go:205] Trace[1757475396]: "iptables ChainExists" (02-Nov-2022 01:12:40.463) (total time: 98480ms):
Trace[1757475396]: [1m38.480272319s] [1m38.480272319s] END
I1102 01:38:10.245742       1 trace.go:205] Trace[1880871978]: "iptables ChainExists" (02-Nov-2022 01:37:44.478) (total time: 25426ms):
Trace[1880871978]: [25.426730694s] [25.426730694s] END
W1102 01:40:47.163557       1 reflector.go:442] vendor/k8s.io/client-go/informers/factory.go:134: watch of *v1.Node ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W1102 01:40:47.170180       1 reflector.go:442] vendor/k8s.io/client-go/informers/factory.go:134: watch of *v1.Service ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
I1102 01:40:47.236605       1 trace.go:205] Trace[59793888]: "iptables ChainExists" (02-Nov-2022 01:40:14.764) (total time: 32497ms):
Trace[59793888]: [32.497938801s] [32.497938801s] END
W1102 01:40:47.458453       1 reflector.go:442] vendor/k8s.io/client-go/informers/factory.go:134: watch of *v1.EndpointSlice ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
I1102 01:40:47.743035       1 trace.go:205] Trace[1306292708]: "iptables ChainExists" (02-Nov-2022 01:40:14.498) (total time: 33269ms):
Trace[1306292708]: [33.269582844s] [33.269582844s] END
I1102 01:41:11.761748       1 trace.go:205] Trace[475653529]: "iptables ChainExists" (02-Nov-2022 01:41:09.141) (total time: 2619ms):
Trace[475653529]: [2.619457578s] [2.619457578s] END
I1103 00:40:43.295830       1 trace.go:205] Trace[1745518868]: "iptables ChainExists" (03-Nov-2022 00:40:40.822) (total time: 2379ms):
Trace[1745518868]: [2.379487098s] [2.379487098s] END

* 
* ==> kube-proxy [f52bcc2527f6] <==
* I1028 06:26:59.576559       1 node.go:163] Successfully retrieved node IP: 192.168.49.2
I1028 06:26:59.576900       1 server_others.go:138] "Detected node IP" address="192.168.49.2"
I1028 06:26:59.577106       1 server_others.go:578] "Unknown proxy mode, assuming iptables proxy" proxyMode=""
I1028 06:26:59.687233       1 server_others.go:206] "Using iptables Proxier"
I1028 06:26:59.687294       1 server_others.go:213] "kube-proxy running in dual-stack mode" ipFamily=IPv4
I1028 06:26:59.687348       1 server_others.go:214] "Creating dualStackProxier for iptables"
I1028 06:26:59.687361       1 server_others.go:501] "Detect-local-mode set to ClusterCIDR, but no IPv6 cluster CIDR defined, , defaulting to no-op detect-local for IPv6"
I1028 06:26:59.687390       1 proxier.go:259] "Setting route_localnet=1, use nodePortAddresses to filter loopback addresses for NodePorts to skip it https://issues.k8s.io/90259"
I1028 06:26:59.687993       1 proxier.go:259] "Setting route_localnet=1, use nodePortAddresses to filter loopback addresses for NodePorts to skip it https://issues.k8s.io/90259"
I1028 06:26:59.694085       1 server.go:661] "Version info" version="v1.24.3"
I1028 06:26:59.694107       1 server.go:663] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1028 06:26:59.695111       1 config.go:317] "Starting service config controller"
I1028 06:26:59.695300       1 shared_informer.go:255] Waiting for caches to sync for service config
I1028 06:26:59.695421       1 config.go:226] "Starting endpoint slice config controller"
I1028 06:26:59.695445       1 shared_informer.go:255] Waiting for caches to sync for endpoint slice config
I1028 06:26:59.700764       1 config.go:444] "Starting node config controller"
I1028 06:26:59.700865       1 shared_informer.go:255] Waiting for caches to sync for node config
I1028 06:26:59.796523       1 shared_informer.go:262] Caches are synced for endpoint slice config
I1028 06:26:59.796644       1 shared_informer.go:262] Caches are synced for service config
I1028 06:26:59.801116       1 shared_informer.go:262] Caches are synced for node config

* 
* ==> kube-scheduler [f3c92cc44756] <==
* I1028 23:44:55.835846       1 serving.go:348] Generated self-signed cert in-memory
W1028 23:44:58.618916       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W1028 23:44:58.618987       1 authentication.go:346] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W1028 23:44:58.619021       1 authentication.go:347] Continuing without authentication configuration. This may treat all requests as anonymous.
W1028 23:44:58.619026       1 authentication.go:348] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I1028 23:44:58.646959       1 server.go:147] "Starting Kubernetes Scheduler" version="v1.24.3"
I1028 23:44:58.647003       1 server.go:149] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1028 23:44:58.652981       1 secure_serving.go:210] Serving securely on 127.0.0.1:10259
I1028 23:44:58.653626       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1028 23:44:58.653701       1 shared_informer.go:255] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I1028 23:44:58.653774       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I1028 23:44:58.754366       1 shared_informer.go:262] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
timeout waiting for SETTINGS frames from 127.0.0.1:38802
W1102 01:14:18.736760       1 reflector.go:442] vendor/k8s.io/client-go/informers/factory.go:134: watch of *v1.PersistentVolume ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W1102 01:14:18.660053       1 reflector.go:442] vendor/k8s.io/client-go/informers/factory.go:134: watch of *v1.ReplicaSet ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W1102 01:14:18.737595       1 reflector.go:442] vendor/k8s.io/client-go/informers/factory.go:134: watch of *v1.Service ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W1102 01:14:18.737729       1 reflector.go:442] vendor/k8s.io/client-go/informers/factory.go:134: watch of *v1.Pod ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W1102 01:14:18.737759       1 reflector.go:442] vendor/k8s.io/client-go/informers/factory.go:134: watch of *v1.PersistentVolumeClaim ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W1102 01:14:18.737802       1 reflector.go:442] vendor/k8s.io/client-go/informers/factory.go:134: watch of *v1.Namespace ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W1102 01:14:18.737829       1 reflector.go:442] vendor/k8s.io/client-go/informers/factory.go:134: watch of *v1.CSIDriver ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W1102 01:14:18.737866       1 reflector.go:442] vendor/k8s.io/client-go/informers/factory.go:134: watch of *v1.StorageClass ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W1102 01:14:18.737890       1 reflector.go:442] vendor/k8s.io/client-go/informers/factory.go:134: watch of *v1.CSIStorageCapacity ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W1102 01:14:18.737910       1 reflector.go:442] vendor/k8s.io/client-go/informers/factory.go:134: watch of *v1.ReplicationController ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W1102 01:14:18.737934       1 reflector.go:442] vendor/k8s.io/client-go/informers/factory.go:134: watch of *v1.Node ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W1102 01:14:18.737955       1 reflector.go:442] vendor/k8s.io/client-go/informers/factory.go:134: watch of *v1.PodDisruptionBudget ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W1102 01:14:18.662674       1 reflector.go:442] vendor/k8s.io/client-go/informers/factory.go:134: watch of *v1.StatefulSet ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W1102 01:14:18.672386       1 reflector.go:442] vendor/k8s.io/client-go/informers/factory.go:134: watch of *v1.CSINode ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W1102 01:40:46.418000       1 reflector.go:442] vendor/k8s.io/client-go/informers/factory.go:134: watch of *v1.StatefulSet ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W1102 01:40:46.418143       1 reflector.go:442] vendor/k8s.io/client-go/informers/factory.go:134: watch of *v1.Node ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W1102 01:40:46.418187       1 reflector.go:442] vendor/k8s.io/client-go/informers/factory.go:134: watch of *v1.CSIDriver ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W1102 01:40:46.418239       1 reflector.go:442] vendor/k8s.io/client-go/informers/factory.go:134: watch of *v1.CSIStorageCapacity ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W1102 01:40:46.418286       1 reflector.go:442] vendor/k8s.io/client-go/informers/factory.go:134: watch of *v1.ReplicationController ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W1102 01:40:46.418331       1 reflector.go:442] vendor/k8s.io/client-go/informers/factory.go:134: watch of *v1.Namespace ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W1102 01:40:46.418378       1 reflector.go:442] vendor/k8s.io/client-go/informers/factory.go:134: watch of *v1.StorageClass ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W1102 01:40:46.418426       1 reflector.go:442] vendor/k8s.io/client-go/informers/factory.go:134: watch of *v1.CSINode ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W1102 01:40:46.418461       1 reflector.go:442] vendor/k8s.io/client-go/informers/factory.go:134: watch of *v1.PersistentVolume ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W1102 01:40:46.419389       1 reflector.go:442] vendor/k8s.io/client-go/informers/factory.go:134: watch of *v1.Pod ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W1102 01:40:46.419485       1 reflector.go:442] vendor/k8s.io/client-go/informers/factory.go:134: watch of *v1.ReplicaSet ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W1102 01:40:46.419605       1 reflector.go:442] vendor/k8s.io/client-go/informers/factory.go:134: watch of *v1.PersistentVolumeClaim ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W1102 01:40:46.419781       1 reflector.go:442] vendor/k8s.io/client-go/informers/factory.go:134: watch of *v1.Service ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
E1102 01:40:46.505630       1 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PodDisruptionBudget: Get "https://192.168.49.2:8443/apis/policy/v1/poddisruptionbudgets?allowWatchBookmarks=true&resourceVersion=329555&timeout=7m56s&timeoutSeconds=476&watch=true": http2: client connection lost

* 
* ==> kube-scheduler [fa7ea1a28f18] <==
* I1028 06:26:36.854251       1 serving.go:348] Generated self-signed cert in-memory
W1028 06:26:40.049334       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W1028 06:26:40.049411       1 authentication.go:346] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W1028 06:26:40.049429       1 authentication.go:347] Continuing without authentication configuration. This may treat all requests as anonymous.
W1028 06:26:40.049436       1 authentication.go:348] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I1028 06:26:40.133622       1 server.go:147] "Starting Kubernetes Scheduler" version="v1.24.3"
I1028 06:26:40.133743       1 server.go:149] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1028 06:26:40.139901       1 secure_serving.go:210] Serving securely on 127.0.0.1:10259
I1028 06:26:40.140338       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1028 06:26:40.140392       1 shared_informer.go:255] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I1028 06:26:40.140506       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
W1028 06:26:40.146721       1 reflector.go:324] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E1028 06:26:40.146804       1 reflector.go:138] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
W1028 06:26:40.147970       1 reflector.go:324] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E1028 06:26:40.148030       1 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
W1028 06:26:40.148123       1 reflector.go:324] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E1028 06:26:40.148170       1 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
W1028 06:26:40.148216       1 reflector.go:324] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E1028 06:26:40.148269       1 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
W1028 06:26:40.152954       1 reflector.go:324] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E1028 06:26:40.153065       1 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
W1028 06:26:40.153280       1 reflector.go:324] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
W1028 06:26:40.153709       1 reflector.go:324] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E1028 06:26:40.153799       1 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
W1028 06:26:40.154770       1 reflector.go:324] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E1028 06:26:40.154893       1 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
W1028 06:26:40.155255       1 reflector.go:324] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E1028 06:26:40.155299       1 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
W1028 06:26:40.155343       1 reflector.go:324] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
W1028 06:26:40.155436       1 reflector.go:324] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E1028 06:26:40.155522       1 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
W1028 06:26:40.155622       1 reflector.go:324] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E1028 06:26:40.155689       1 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
W1028 06:26:40.155787       1 reflector.go:324] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E1028 06:26:40.155835       1 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
W1028 06:26:40.155983       1 reflector.go:324] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E1028 06:26:40.156017       1 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E1028 06:26:40.156254       1 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E1028 06:26:40.153346       1 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
W1028 06:26:40.156497       1 reflector.go:324] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E1028 06:26:40.158545       1 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
W1028 06:26:41.080766       1 reflector.go:324] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E1028 06:26:41.080835       1 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
W1028 06:26:41.160364       1 reflector.go:324] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E1028 06:26:41.160418       1 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
W1028 06:26:41.217523       1 reflector.go:324] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E1028 06:26:41.217588       1 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
W1028 06:26:41.234115       1 reflector.go:324] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E1028 06:26:41.234174       1 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
W1028 06:26:41.402174       1 reflector.go:324] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E1028 06:26:41.402257       1 reflector.go:138] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
I1028 06:26:44.119886       1 shared_informer.go:262] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I1028 12:56:20.161684       1 secure_serving.go:255] Stopped listening on 127.0.0.1:10259
I1028 12:56:20.188793       1 tlsconfig.go:255] "Shutting down DynamicServingCertificateController"
I1028 12:56:20.189952       1 configmap_cafile_content.go:223] "Shutting down controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"

* 
* ==> kubelet <==
* -- Logs begin at Tue 2022-11-01 14:14:22 UTC, end at Thu 2022-11-03 02:24:03 UTC. --
Nov 02 23:19:33 minikube kubelet[1190]: W1102 23:19:33.227250    1190 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Nov 02 23:24:32 minikube kubelet[1190]: W1102 23:24:32.999464    1190 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Nov 02 23:29:32 minikube kubelet[1190]: W1102 23:29:32.775581    1190 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Nov 02 23:34:32 minikube kubelet[1190]: W1102 23:34:32.563528    1190 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Nov 02 23:39:32 minikube kubelet[1190]: W1102 23:39:32.337350    1190 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Nov 02 23:44:32 minikube kubelet[1190]: W1102 23:44:32.120757    1190 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Nov 02 23:49:31 minikube kubelet[1190]: W1102 23:49:31.917285    1190 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Nov 02 23:54:31 minikube kubelet[1190]: W1102 23:54:31.681833    1190 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Nov 02 23:59:31 minikube kubelet[1190]: W1102 23:59:31.452563    1190 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Nov 03 00:04:31 minikube kubelet[1190]: W1103 00:04:31.242570    1190 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Nov 03 00:09:31 minikube kubelet[1190]: W1103 00:09:31.030018    1190 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Nov 03 00:14:30 minikube kubelet[1190]: W1103 00:14:30.797554    1190 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Nov 03 00:19:30 minikube kubelet[1190]: W1103 00:19:30.589960    1190 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Nov 03 00:24:30 minikube kubelet[1190]: W1103 00:24:30.383400    1190 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Nov 03 00:29:30 minikube kubelet[1190]: W1103 00:29:30.113681    1190 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Nov 03 00:34:29 minikube kubelet[1190]: W1103 00:34:29.902033    1190 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Nov 03 00:39:29 minikube kubelet[1190]: W1103 00:39:29.686777    1190 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Nov 03 00:40:47 minikube kubelet[1190]: E1103 00:40:47.218204    1190 remote_runtime.go:680] "ExecSync cmd from runtime service failed" err="rpc error: code = Unknown desc = deadline exceeded (\"DeadlineExceeded\"): context deadline exceeded" containerID="b73bec9ee264fe5b9338c853b950e186abd84f0f3f9f98a5e1184fad437dd7ea" cmd=[/usr/bin/check-status -r]
Nov 03 00:40:49 minikube kubelet[1190]: I1103 00:40:49.394564    1190 scope.go:110] "RemoveContainer" containerID="b68b77200ba1ee96d2dac62cd65ae436d52bd9c0180a1043556fd1e7155678cb"
Nov 03 00:40:49 minikube kubelet[1190]: I1103 00:40:49.397043    1190 scope.go:110] "RemoveContainer" containerID="bbf7ccdfe0751a30564b8a72fcff7ba3cd944aacfb913589265df05e38cc33f0"
Nov 03 00:44:29 minikube kubelet[1190]: W1103 00:44:29.454288    1190 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Nov 03 00:49:29 minikube kubelet[1190]: W1103 00:49:29.248928    1190 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Nov 03 00:54:29 minikube kubelet[1190]: W1103 00:54:29.032670    1190 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Nov 03 00:59:28 minikube kubelet[1190]: W1103 00:59:28.789785    1190 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Nov 03 01:04:28 minikube kubelet[1190]: W1103 01:04:28.572840    1190 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Nov 03 01:09:28 minikube kubelet[1190]: W1103 01:09:28.365694    1190 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Nov 03 01:14:28 minikube kubelet[1190]: W1103 01:14:28.134950    1190 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Nov 03 01:19:27 minikube kubelet[1190]: W1103 01:19:27.936383    1190 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Nov 03 01:24:27 minikube kubelet[1190]: W1103 01:24:27.715101    1190 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Nov 03 01:29:27 minikube kubelet[1190]: W1103 01:29:27.465101    1190 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Nov 03 01:34:27 minikube kubelet[1190]: W1103 01:34:27.271969    1190 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Nov 03 01:39:27 minikube kubelet[1190]: W1103 01:39:27.044820    1190 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Nov 03 01:44:26 minikube kubelet[1190]: W1103 01:44:26.814680    1190 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Nov 03 01:47:07 minikube kubelet[1190]: E1103 01:47:07.730181    1190 remote_runtime.go:680] "ExecSync cmd from runtime service failed" err="rpc error: code = Unknown desc = deadline exceeded (\"DeadlineExceeded\"): context deadline exceeded" containerID="b73bec9ee264fe5b9338c853b950e186abd84f0f3f9f98a5e1184fad437dd7ea" cmd=[/usr/bin/check-status -r]
Nov 03 01:49:26 minikube kubelet[1190]: W1103 01:49:26.609021    1190 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Nov 03 01:54:26 minikube kubelet[1190]: W1103 01:54:26.392605    1190 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Nov 03 01:59:26 minikube kubelet[1190]: W1103 01:59:26.147682    1190 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Nov 03 02:04:25 minikube kubelet[1190]: W1103 02:04:25.938207    1190 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Nov 03 02:09:25 minikube kubelet[1190]: W1103 02:09:25.730667    1190 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Nov 03 02:14:25 minikube kubelet[1190]: W1103 02:14:25.474316    1190 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Nov 03 02:19:25 minikube kubelet[1190]: W1103 02:19:25.261936    1190 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Nov 03 02:21:31 minikube kubelet[1190]: I1103 02:21:31.174056    1190 topology_manager.go:200] "Topology Admit Handler"
Nov 03 02:21:31 minikube kubelet[1190]: I1103 02:21:31.475100    1190 reconciler.go:270] "operationExecutor.VerifyControllerAttachedVolume started for volume \"webhook-cert\" (UniqueName: \"kubernetes.io/secret/11c895d4-7c78-4957-9180-f37cd84d0a2f-webhook-cert\") pod \"ingress-nginx-controller-755dfbfc65-gw44w\" (UID: \"11c895d4-7c78-4957-9180-f37cd84d0a2f\") " pod="ingress-nginx/ingress-nginx-controller-755dfbfc65-gw44w"
Nov 03 02:21:31 minikube kubelet[1190]: I1103 02:21:31.475194    1190 reconciler.go:270] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-jpc9p\" (UniqueName: \"kubernetes.io/projected/11c895d4-7c78-4957-9180-f37cd84d0a2f-kube-api-access-jpc9p\") pod \"ingress-nginx-controller-755dfbfc65-gw44w\" (UID: \"11c895d4-7c78-4957-9180-f37cd84d0a2f\") " pod="ingress-nginx/ingress-nginx-controller-755dfbfc65-gw44w"
Nov 03 02:21:34 minikube kubelet[1190]: I1103 02:21:34.030826    1190 pod_container_deletor.go:79] "Container not found in pod's containers" containerID="e949a2122e351d0eca7cf2dd0d1f24d0d25ef6bc4f23165065d7505985a30e77"
Nov 03 02:21:46 minikube kubelet[1190]: I1103 02:21:46.372670    1190 scope.go:110] "RemoveContainer" containerID="57948df216455e7c1b264a84b9c27299e1158813ce8c8638f7c0635af14b7325"
Nov 03 02:21:46 minikube kubelet[1190]: I1103 02:21:46.447589    1190 scope.go:110] "RemoveContainer" containerID="1fa53bbc31109c17cf7c40f40c05e8990aa048f11a23ee206742034dc20d58bc"
Nov 03 02:21:46 minikube kubelet[1190]: I1103 02:21:46.464973    1190 reconciler.go:192] "operationExecutor.UnmountVolume started for volume \"webhook-cert\" (UniqueName: \"kubernetes.io/secret/69533030-d2e7-4dcb-adc8-c5aa010e9434-webhook-cert\") pod \"69533030-d2e7-4dcb-adc8-c5aa010e9434\" (UID: \"69533030-d2e7-4dcb-adc8-c5aa010e9434\") "
Nov 03 02:21:46 minikube kubelet[1190]: I1103 02:21:46.465096    1190 reconciler.go:192] "operationExecutor.UnmountVolume started for volume \"kube-api-access-xrsxk\" (UniqueName: \"kubernetes.io/projected/69533030-d2e7-4dcb-adc8-c5aa010e9434-kube-api-access-xrsxk\") pod \"69533030-d2e7-4dcb-adc8-c5aa010e9434\" (UID: \"69533030-d2e7-4dcb-adc8-c5aa010e9434\") "
Nov 03 02:21:46 minikube kubelet[1190]: I1103 02:21:46.477250    1190 operation_generator.go:856] UnmountVolume.TearDown succeeded for volume "kubernetes.io/secret/69533030-d2e7-4dcb-adc8-c5aa010e9434-webhook-cert" (OuterVolumeSpecName: "webhook-cert") pod "69533030-d2e7-4dcb-adc8-c5aa010e9434" (UID: "69533030-d2e7-4dcb-adc8-c5aa010e9434"). InnerVolumeSpecName "webhook-cert". PluginName "kubernetes.io/secret", VolumeGidValue ""
Nov 03 02:21:46 minikube kubelet[1190]: I1103 02:21:46.477643    1190 operation_generator.go:856] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/69533030-d2e7-4dcb-adc8-c5aa010e9434-kube-api-access-xrsxk" (OuterVolumeSpecName: "kube-api-access-xrsxk") pod "69533030-d2e7-4dcb-adc8-c5aa010e9434" (UID: "69533030-d2e7-4dcb-adc8-c5aa010e9434"). InnerVolumeSpecName "kube-api-access-xrsxk". PluginName "kubernetes.io/projected", VolumeGidValue ""
Nov 03 02:21:46 minikube kubelet[1190]: I1103 02:21:46.498569    1190 scope.go:110] "RemoveContainer" containerID="57948df216455e7c1b264a84b9c27299e1158813ce8c8638f7c0635af14b7325"
Nov 03 02:21:46 minikube kubelet[1190]: E1103 02:21:46.500555    1190 remote_runtime.go:578] "ContainerStatus from runtime service failed" err="rpc error: code = Unknown desc = Error: No such container: 57948df216455e7c1b264a84b9c27299e1158813ce8c8638f7c0635af14b7325" containerID="57948df216455e7c1b264a84b9c27299e1158813ce8c8638f7c0635af14b7325"
Nov 03 02:21:46 minikube kubelet[1190]: I1103 02:21:46.500642    1190 pod_container_deletor.go:52] "DeleteContainer returned error" containerID={Type:docker ID:57948df216455e7c1b264a84b9c27299e1158813ce8c8638f7c0635af14b7325} err="failed to get container status \"57948df216455e7c1b264a84b9c27299e1158813ce8c8638f7c0635af14b7325\": rpc error: code = Unknown desc = Error: No such container: 57948df216455e7c1b264a84b9c27299e1158813ce8c8638f7c0635af14b7325"
Nov 03 02:21:46 minikube kubelet[1190]: I1103 02:21:46.500664    1190 scope.go:110] "RemoveContainer" containerID="1fa53bbc31109c17cf7c40f40c05e8990aa048f11a23ee206742034dc20d58bc"
Nov 03 02:21:46 minikube kubelet[1190]: E1103 02:21:46.502565    1190 remote_runtime.go:578] "ContainerStatus from runtime service failed" err="rpc error: code = Unknown desc = Error: No such container: 1fa53bbc31109c17cf7c40f40c05e8990aa048f11a23ee206742034dc20d58bc" containerID="1fa53bbc31109c17cf7c40f40c05e8990aa048f11a23ee206742034dc20d58bc"
Nov 03 02:21:46 minikube kubelet[1190]: I1103 02:21:46.502629    1190 pod_container_deletor.go:52] "DeleteContainer returned error" containerID={Type:docker ID:1fa53bbc31109c17cf7c40f40c05e8990aa048f11a23ee206742034dc20d58bc} err="failed to get container status \"1fa53bbc31109c17cf7c40f40c05e8990aa048f11a23ee206742034dc20d58bc\": rpc error: code = Unknown desc = Error: No such container: 1fa53bbc31109c17cf7c40f40c05e8990aa048f11a23ee206742034dc20d58bc"
Nov 03 02:21:46 minikube kubelet[1190]: I1103 02:21:46.566420    1190 reconciler.go:312] "Volume detached for volume \"webhook-cert\" (UniqueName: \"kubernetes.io/secret/69533030-d2e7-4dcb-adc8-c5aa010e9434-webhook-cert\") on node \"minikube\" DevicePath \"\""
Nov 03 02:21:46 minikube kubelet[1190]: I1103 02:21:46.566506    1190 reconciler.go:312] "Volume detached for volume \"kube-api-access-xrsxk\" (UniqueName: \"kubernetes.io/projected/69533030-d2e7-4dcb-adc8-c5aa010e9434-kube-api-access-xrsxk\") on node \"minikube\" DevicePath \"\""
Nov 03 02:21:47 minikube kubelet[1190]: I1103 02:21:47.299569    1190 kubelet_volumes.go:160] "Cleaned up orphaned pod volumes dir" podUID=69533030-d2e7-4dcb-adc8-c5aa010e9434 path="/var/lib/kubelet/pods/69533030-d2e7-4dcb-adc8-c5aa010e9434/volumes"

* 
* ==> storage-provisioner [a67187a329e1] <==
* I1103 00:40:50.311175       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I1103 00:40:50.432976       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I1103 00:40:50.433818       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I1103 00:41:07.921888       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I1103 00:41:07.922564       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_be2dadfd-cfd2-418e-93d8-5256485eb103!
I1103 00:41:07.923337       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"02ed9b1c-25fa-4698-a5bc-2cc15e6dc018", APIVersion:"v1", ResourceVersion:"401375", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_be2dadfd-cfd2-418e-93d8-5256485eb103 became leader
I1103 00:41:08.024092       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_be2dadfd-cfd2-418e-93d8-5256485eb103!

* 
* ==> storage-provisioner [bbf7ccdfe075] <==
* sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).Run.func1.3()
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:881 +0x5c
k8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1(0xc0000464a0)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:155 +0x5f
k8s.io/apimachinery/pkg/util/wait.BackoffUntil(0xc0000464a0, 0x18b3d60, 0xc0005c4b10, 0x1, 0xc0004b6cc0)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:156 +0x9b
k8s.io/apimachinery/pkg/util/wait.JitterUntil(0xc0000464a0, 0x3b9aca00, 0x0, 0x1, 0xc0004b6cc0)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:133 +0x98
k8s.io/apimachinery/pkg/util/wait.Until(0xc0000464a0, 0x3b9aca00, 0xc0004b6cc0)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:90 +0x4d
created by sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).Run.func1
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:881 +0x3d6

goroutine 148 [sync.Cond.Wait, 1114 minutes]:
sync.runtime_notifyListWait(0xc0004a4390, 0x3)
	/usr/local/go/src/runtime/sema.go:513 +0xf8
sync.(*Cond).Wait(0xc0004a4380)
	/usr/local/go/src/sync/cond.go:56 +0x99
k8s.io/client-go/util/workqueue.(*Type).Get(0xc0000762a0, 0x0, 0x0, 0x0)
	/Users/medya/go/pkg/mod/k8s.io/client-go@v0.20.5/util/workqueue/queue.go:145 +0x89
sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).processNextClaimWorkItem(0xc0003dcc80, 0x18e5530, 0xc00044ef00, 0x203000)
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:935 +0x3e
sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).runClaimWorker(...)
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:924
sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).Run.func1.2()
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:880 +0x5c
k8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1(0xc0000464c0)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:155 +0x5f
k8s.io/apimachinery/pkg/util/wait.BackoffUntil(0xc0000464c0, 0x18b3d60, 0xc0005c4b40, 0x1, 0xc0004b6cc0)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:156 +0x9b
k8s.io/apimachinery/pkg/util/wait.JitterUntil(0xc0000464c0, 0x3b9aca00, 0x0, 0x1, 0xc0004b6cc0)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:133 +0x98
k8s.io/apimachinery/pkg/util/wait.Until(0xc0000464c0, 0x3b9aca00, 0xc0004b6cc0)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:90 +0x4d
created by sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).Run.func1
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:880 +0x4af

goroutine 149 [sync.Cond.Wait, 1114 minutes]:
sync.runtime_notifyListWait(0xc0004a4410, 0x3)
	/usr/local/go/src/runtime/sema.go:513 +0xf8
sync.(*Cond).Wait(0xc0004a4400)
	/usr/local/go/src/sync/cond.go:56 +0x99
k8s.io/client-go/util/workqueue.(*Type).Get(0xc000076420, 0x0, 0x0, 0x0)
	/Users/medya/go/pkg/mod/k8s.io/client-go@v0.20.5/util/workqueue/queue.go:145 +0x89
sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).processNextVolumeWorkItem(0xc0003dcc80, 0x18e5530, 0xc00044ef00, 0x203000)
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:990 +0x3e
sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).runVolumeWorker(...)
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:929
sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).Run.func1.3()
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:881 +0x5c
k8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1(0xc0000464e0)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:155 +0x5f
k8s.io/apimachinery/pkg/util/wait.BackoffUntil(0xc0000464e0, 0x18b3d60, 0xc0004c8000, 0x1, 0xc0004b6cc0)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:156 +0x9b
k8s.io/apimachinery/pkg/util/wait.JitterUntil(0xc0000464e0, 0x3b9aca00, 0x0, 0x1, 0xc0004b6cc0)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:133 +0x98
k8s.io/apimachinery/pkg/util/wait.Until(0xc0000464e0, 0x3b9aca00, 0xc0004b6cc0)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:90 +0x4d
created by sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).Run.func1
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:881 +0x3d6

